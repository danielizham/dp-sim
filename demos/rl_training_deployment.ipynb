{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2cfc7f7",
   "metadata": {},
   "source": [
    "# Import all the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b225355f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from enum import IntEnum\n",
    "import time\n",
    "import jsonrpclib\n",
    "import subprocess\n",
    "from subprocess import PIPE, Popen\n",
    "from threading  import Thread\n",
    "import sys\n",
    "import re\n",
    "from collections import OrderedDict\n",
    "\n",
    "import PySimpleGUI as sg\n",
    "\n",
    "from gym import Env, error, spaces, utils\n",
    "from stable_baselines3 import DQN, PPO, A2C, TD3, SAC\n",
    "from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.callbacks import BaseCallback, CallbackList, CheckpointCallback, EvalCallback\n",
    "from stable_baselines3.common import results_plotter\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy, plot_results\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common import results_plotter\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy, plot_results\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import shutil\n",
    "import tempfile\n",
    "import xml.etree.ElementTree as ET\n",
    "from io import StringIO, BytesIO\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from IPython.display import clear_output\n",
    "import gym\n",
    "from cv2 import QRCodeDetector\n",
    "from pyzbar import pyzbar\n",
    "\n",
    "import olympe\n",
    "from olympe.messages.ardrone3.Piloting import TakeOff, Landing, moveBy, PCMD, moveTo\n",
    "from olympe.messages.ardrone3.PilotingState import FlyingStateChanged, PositionChanged, GpsLocationChanged, moveToChanged\n",
    "from olympe.enums.ardrone3.PilotingState import FlyingStateChanged_State as FlyingState\n",
    "from olympe.messages.ardrone3.GPSSettingsState import GPSFixStateChanged, HomeChanged\n",
    "from olympe.messages.gimbal import set_target, attitude\n",
    "from olympe.messages.camera import (\n",
    "    set_camera_mode,\n",
    "    set_photo_mode,\n",
    "    take_photo,\n",
    "    photo_progress,\n",
    ")\n",
    "from olympe.media import (\n",
    "    media_created,\n",
    "    resource_created,\n",
    "    media_removed,\n",
    "    resource_removed,\n",
    "    resource_downloaded,\n",
    "    indexing_state,\n",
    "    delete_media,\n",
    "    download_media,\n",
    "    download_media_thumbnail,\n",
    "    MediaEvent,\n",
    ")\n",
    "\n",
    "from pynput.keyboard import Listener, Key, KeyCode\n",
    "from collections import defaultdict\n",
    "\n",
    "olympe.log.update_config({\n",
    "    \"loggers\": {\n",
    "        \"olympe\": {\n",
    "                \"handlers\": []\n",
    "            }\n",
    "        },\n",
    "        \"ulog\": {\n",
    "            \"level\": \"OFF\",\n",
    "            \"handlers\": [],\n",
    "        }\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0380e41c",
   "metadata": {},
   "source": [
    "# Define the constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee2702da",
   "metadata": {},
   "outputs": [],
   "source": [
    "DRONE_IP = os.environ.get(\"DRONE_IP\", \"10.202.0.1\")\n",
    "DRONE_MEDIA_PORT = os.environ.get(\"DRONE_MEDIA_PORT\", \"80\")\n",
    "\n",
    "ANAFI_URL = \"http://{}/\".format(DRONE_IP)\n",
    "ANAFI_MEDIA_API_URL = ANAFI_URL + \"api/v1/media/medias/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc9719c",
   "metadata": {},
   "source": [
    "# Define the classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f459a37",
   "metadata": {},
   "source": [
    "## Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09ac1cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Action:\n",
    "    def __init__(self, drone):\n",
    "        self.drone = drone\n",
    "        self.home = self.drone.get_state(GpsLocationChanged)\n",
    "        \n",
    "        self.current_cell = self._get_cell(13)\n",
    "        self.invalid_left_cells = [1, 6, 11, 16, 21]\n",
    "        self.invalid_forward_cells = [1, 2, 3, 4, 5]\n",
    "        self.invalid_right_cells = [5, 10, 15, 20, 25]\n",
    "        self.invalid_backward_cells = [21, 22, 23, 24, 25]\n",
    "        \n",
    "        self.Move = IntEnum(\n",
    "            'MOVE',\n",
    "            'FORWARD BACKWARD LEFT RIGHT FORWARD_LEFT FORWARD_RIGHT BACKWARD_LEFT BACKWARD_RIGHT HOVER',\n",
    "            start=0\n",
    "        )\n",
    "        \n",
    "    def take_action(self, action):\n",
    "        next_cell_id = self._get_next_cell_id(action)\n",
    "        next_cell = self._get_cell(next_cell_id)\n",
    "        \n",
    "        old_cell_id, new_cell_id = self.current_cell[\"id\"], next_cell[\"id\"]\n",
    "        if old_cell_id == new_cell_id: \n",
    "            return old_cell_id, new_cell_id, self._get_action_name(action)\n",
    "        \n",
    "        self._move_to_cell(next_cell)\n",
    "        \n",
    "        self.current_cell = next_cell\n",
    "        \n",
    "        return old_cell_id, new_cell_id, self._get_action_name(action)\n",
    "        \n",
    "    def reset(self):\n",
    "        next_cell = self._get_cell(13)\n",
    "        self._move_to_cell(next_cell)\n",
    "        \n",
    "        old_cell_id, new_cell_id = self.current_cell[\"id\"], next_cell[\"id\"]\n",
    "        self.current_cell = next_cell\n",
    "        \n",
    "        return old_cell_id, new_cell_id\n",
    "    \n",
    "    def _get_cell(self, cell_id):\n",
    "        return self._cell_coords[cell_id - 1]\n",
    "    \n",
    "    def _get_action_name(self, action):\n",
    "        direction = str(self.Move(action)).split(\".\")[1]\n",
    "        code = \"\"\n",
    "        for i in direction.split(\"_\"):\n",
    "            if i != \"\":\n",
    "                code += i[0].upper()\n",
    "        \n",
    "        return code\n",
    "#         direction = str(self.Move(action)).split(\".\")[1].capitalize()\n",
    "#         return \"Moving \" + direction if \"hover\" not in direction.lower() else \"Hovering\"\n",
    "    \n",
    "    def _move_to_cell(self, next_cell):        \n",
    "        self.drone(\n",
    "            moveTo(next_cell[\"latitude\"],  next_cell[\"longitude\"], next_cell[\"altitude\"], \"HEADING_DURING\", 90.0)\n",
    "            >> moveToChanged(status=\"DONE\", _timeout=15)\n",
    "        ).wait()\n",
    "    \n",
    "    def _get_next_cell_id(self, action):\n",
    "        if action == self.Move.HOVER:\n",
    "            return self.current_cell[\"id\"]\n",
    "        elif action == self.Move.LEFT:\n",
    "            if self.current_cell[\"id\"] in self.invalid_left_cells:\n",
    "                return self.current_cell[\"id\"]\n",
    "            next_cell_id = self.current_cell[\"id\"] - 1\n",
    "        elif action == self.Move.RIGHT:\n",
    "            if self.current_cell[\"id\"] in self.invalid_right_cells:\n",
    "                return self.current_cell[\"id\"]\n",
    "            next_cell_id = self.current_cell[\"id\"] + 1\n",
    "        elif action == self.Move.FORWARD:\n",
    "            if self.current_cell[\"id\"] in self.invalid_forward_cells:\n",
    "                return self.current_cell[\"id\"]\n",
    "            next_cell_id = self.current_cell[\"id\"] - 5\n",
    "        elif action == self.Move.BACKWARD:\n",
    "            if self.current_cell[\"id\"] in self.invalid_backward_cells:\n",
    "                return self.current_cell[\"id\"]\n",
    "            next_cell_id = self.current_cell[\"id\"] + 5\n",
    "        elif action == self.Move.FORWARD_RIGHT:\n",
    "            if self.current_cell[\"id\"] in self.invalid_forward_cells + self.invalid_right_cells:\n",
    "                return self.current_cell[\"id\"]\n",
    "            next_cell_id = self.current_cell[\"id\"] - 4\n",
    "        elif action == self.Move.FORWARD_LEFT:\n",
    "            if self.current_cell[\"id\"] in self.invalid_forward_cells + self.invalid_left_cells:\n",
    "                return self.current_cell[\"id\"]\n",
    "            next_cell_id = self.current_cell[\"id\"] - 6\n",
    "        elif action == self.Move.BACKWARD_RIGHT:\n",
    "            if self.current_cell[\"id\"] in self.invalid_backward_cells + self.invalid_right_cells:\n",
    "                return self.current_cell[\"id\"]\n",
    "            next_cell_id = self.current_cell[\"id\"] + 6\n",
    "        elif action == self.Move.BACKWARD_LEFT:\n",
    "            if self.current_cell[\"id\"] in self.invalid_backward_cells + self.invalid_left_cells:\n",
    "                return self.current_cell[\"id\"]\n",
    "            next_cell_id = self.current_cell[\"id\"] + 4\n",
    "            \n",
    "        return next_cell_id\n",
    "    \n",
    "    @property\n",
    "    def _cell_coords(self):\n",
    "        altitude = 2.5\n",
    "        dlong = 6.8e-5 * (2/5) # in degrees == 5 meters along x-axis (forward[+]-backward[-])\n",
    "        dlat = 7.2e-5 * (3.2/8) # in degrees == 8 meters along y-axis (left[+]-right[-])\n",
    "        \n",
    "        home_lat = self.home[\"latitude\"]\n",
    "        home_long = self.home[\"longitude\"]\n",
    "        \n",
    "        return [\n",
    "            # cell no. 1\n",
    "            OrderedDict([('id', 1),\n",
    "                         ('latitude', home_lat + 2 * dlat),\n",
    "                         ('longitude', home_long + 2 * dlong),\n",
    "                         ('altitude', altitude)]),\n",
    "            # cell no. 2\n",
    "            OrderedDict([('id', 2),\n",
    "                         ('latitude', home_lat + 1 * dlat),\n",
    "                         ('longitude', home_long + 2 * dlong),\n",
    "                         ('altitude', altitude)]),\n",
    "            # cell no. 3\n",
    "            OrderedDict([('id', 3),\n",
    "                         ('latitude', home_lat + 0 * dlat),\n",
    "                         ('longitude', home_long + 2 * dlong),\n",
    "                         ('altitude', altitude)]),\n",
    "            # cell no. 4\n",
    "            OrderedDict([('id', 4),\n",
    "                         ('latitude', home_lat + -1 * dlat),\n",
    "                         ('longitude', home_long + 2 * dlong),\n",
    "                         ('altitude', altitude)]),\n",
    "            # cell no. 5\n",
    "            OrderedDict([('id', 5),\n",
    "                         ('latitude', home_lat + -2 * dlat),\n",
    "                         ('longitude', home_long + 2 * dlong),\n",
    "                         ('altitude', altitude)]),\n",
    "            # cell no. 6\n",
    "            OrderedDict([('id', 6),\n",
    "                         ('latitude', home_lat + 2 * dlat),\n",
    "                         ('longitude', home_long + 1 * dlong),\n",
    "                         ('altitude', altitude)]),\n",
    "            # cell no. 7\n",
    "            OrderedDict([('id', 7),\n",
    "                         ('latitude', home_lat + 1 * dlat),\n",
    "                         ('longitude', home_long + 1 * dlong),\n",
    "                         ('altitude', altitude)]),\n",
    "            # cell no. 8\n",
    "            OrderedDict([('id', 8),\n",
    "                         ('latitude', home_lat + 0 * dlat),\n",
    "                         ('longitude', home_long + 1 * dlong),\n",
    "                         ('altitude', altitude)]),\n",
    "            # cell no. 9\n",
    "            OrderedDict([('id', 9),\n",
    "                         ('latitude', home_lat + -1 * dlat),\n",
    "                         ('longitude', home_long + 1 * dlong),\n",
    "                         ('altitude', altitude)]),\n",
    "            # cell no. 10\n",
    "            OrderedDict([('id', 10),\n",
    "                         ('latitude', home_lat + -2 * dlat),\n",
    "                         ('longitude', home_long + 1 * dlong),\n",
    "                         ('altitude', altitude)]),\n",
    "            # cell no. 11\n",
    "            OrderedDict([('id', 11),\n",
    "                         ('latitude', home_lat + 2 * dlat),\n",
    "                         ('longitude', home_long + 0 * dlong),\n",
    "                         ('altitude', altitude)]),\n",
    "            # cell no. 12\n",
    "            OrderedDict([('id', 12),\n",
    "                         ('latitude', home_lat + 1 * dlat),\n",
    "                         ('longitude', home_long + 0 * dlong),\n",
    "                         ('altitude', altitude)]),\n",
    "            # cell no. 13\n",
    "            OrderedDict([('id', 13),\n",
    "                         ('latitude', home_lat + 0 * dlat),\n",
    "                         ('longitude', home_long + 0 * dlong),\n",
    "                         ('altitude', altitude)]),\n",
    "            # cell no. 14\n",
    "            OrderedDict([('id', 14),\n",
    "                         ('latitude', home_lat + -1 * dlat),\n",
    "                         ('longitude', home_long + 0 * dlong),\n",
    "                         ('altitude', altitude)]),\n",
    "            # cell no. 15\n",
    "            OrderedDict([('id', 15),\n",
    "                         ('latitude', home_lat + -2 * dlat),\n",
    "                         ('longitude', home_long + 0 * dlong),\n",
    "                         ('altitude', altitude)]),\n",
    "            # cell no. 16\n",
    "            OrderedDict([('id', 16),\n",
    "                         ('latitude', home_lat + 2 * dlat),\n",
    "                         ('longitude', home_long + -1 * dlong),\n",
    "                         ('altitude', altitude)]),\n",
    "            # cell no. 17\n",
    "            OrderedDict([('id', 17),\n",
    "                         ('latitude', home_lat + 1 * dlat),\n",
    "                         ('longitude', home_long + -1 * dlong),\n",
    "                         ('altitude', altitude)]),\n",
    "            # cell no. 18\n",
    "            OrderedDict([('id', 18),\n",
    "                         ('latitude', home_lat + 0 * dlat),\n",
    "                         ('longitude', home_long + -1 * dlong),\n",
    "                         ('altitude', altitude)]),\n",
    "            # cell no. 19\n",
    "            OrderedDict([('id', 19),\n",
    "                         ('latitude', home_lat + -1 * dlat),\n",
    "                         ('longitude', home_long + -1 * dlong),\n",
    "                         ('altitude', altitude)]),\n",
    "            # cell no. 20\n",
    "            OrderedDict([('id', 20),\n",
    "                         ('latitude', home_lat + -2 * dlat),\n",
    "                         ('longitude', home_long + -1 * dlong),\n",
    "                         ('altitude', altitude)]),\n",
    "            # cell no. 21\n",
    "            OrderedDict([('id', 21),\n",
    "                         ('latitude', home_lat + 2 * dlat),\n",
    "                         ('longitude', home_long + -2 * dlong),\n",
    "                         ('altitude', altitude)]),\n",
    "            # cell no. 22\n",
    "            OrderedDict([('id', 22),\n",
    "                         ('latitude', home_lat + 1 * dlat),\n",
    "                         ('longitude', home_long + -2 * dlong),\n",
    "                         ('altitude', altitude)]),\n",
    "            # cell no. 23\n",
    "            OrderedDict([('id', 23),\n",
    "                         ('latitude', home_lat + 0 * dlat),\n",
    "                         ('longitude', home_long + -2 * dlong),\n",
    "                         ('altitude', altitude)]),\n",
    "            # cell no. 24\n",
    "            OrderedDict([('id', 24),\n",
    "                         ('latitude', home_lat + -1 * dlat),\n",
    "                         ('longitude', home_long + -2 * dlong),\n",
    "                         ('altitude', altitude)]),\n",
    "            # cell no. 25\n",
    "            OrderedDict([('id', 25),\n",
    "                         ('latitude', home_lat + -2 * dlat),\n",
    "                         ('longitude', home_long + -2 * dlong),\n",
    "                         ('altitude', altitude)]),\n",
    "            ]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.Move)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c218a6a5",
   "metadata": {},
   "source": [
    "## Drone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "154b880f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Drone:\n",
    "    def __init__(self, drone_ip, num_targets, max_timestep, is_training=False):\n",
    "        self.drone = olympe.Drone(drone_ip)\n",
    "        self.drone.connect()\n",
    "        \n",
    "        self.drone(GPSFixStateChanged(_policy = 'wait'))\n",
    "        self._takeoff()\n",
    "        if not is_training: self._setup_camera()\n",
    "\n",
    "        self.action = Action(self.drone)\n",
    "        \n",
    "        self.is_training = is_training\n",
    "        self.num_targets = num_targets\n",
    "        self.max_timestep = max_timestep\n",
    "        self.timestep = 0\n",
    "        self.visited_targets = np.zeros(self.num_targets, dtype=bool)\n",
    "        self.target_positions = np.zeros(self.num_targets, dtype=np.uint8)\n",
    "    \n",
    "    def take_action(self, action):\n",
    "        old_cell, new_cell, action_name = self.action.take_action(action)\n",
    "        self.timestep += 1\n",
    "        \n",
    "        Simulation.cease_targets()\n",
    "        time.sleep(0.1)\n",
    "        detected_targets = self._detect_targets(new_cell)\n",
    "        \n",
    "        reward = self._get_reward(detected_targets) # !!! _get_reward must come before self.visited_targets is changed in _get_state\n",
    "        state = self._get_state(new_cell, detected_targets) \n",
    "        if self.timestep >= self.max_timestep or np.all(self.visited_targets):\n",
    "            done = True\n",
    "            self.visited_targets[:] = False\n",
    "        else:\n",
    "            done = False\n",
    "        info = {\n",
    "            \"action\": str(action_name), \n",
    "            \"direction\": \"Cell \" + str(old_cell) + \" --> \" + \"Cell \" + str(new_cell),\n",
    "            \"positions\": self.target_positions\n",
    "        }\n",
    "#         print(state)\n",
    "#         print(reward)\n",
    "        return state, reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        old_cell, new_cell = self.action.reset()\n",
    "        detected_targets = self._detect_targets(new_cell)\n",
    "        self.timestep = 0\n",
    "        Simulation.move_targets()\n",
    "        return self._get_state(new_cell, detected_targets)\n",
    "    \n",
    "    def _get_state(self, new_cell, detected_targets):\n",
    "        # {t, cell_id, [I1, I2, I3, ..., In]}\n",
    "        \n",
    "        self.visited_targets[detected_targets] = True\n",
    "        \n",
    "        return np.concatenate(([self.timestep, new_cell], self.visited_targets)).astype(np.uint8)\n",
    "    \n",
    "    def _get_reward(self, detected_targets):\n",
    "        reward_scale = 1.5\n",
    "        num_new_targets = np.count_nonzero(\n",
    "            detected_targets & (detected_targets != self.visited_targets)\n",
    "        )\n",
    "        return reward_scale * num_new_targets if num_new_targets > 0 else -1\n",
    "        \n",
    "    def _detect_targets(self, cell_id):\n",
    "        self.update_moving_target_positions()\n",
    "        return self.target_positions == cell_id\n",
    "        \n",
    "        if self.is_training:\n",
    "            positions = np.genfromtxt('target_positions.csv', delimiter=',', skip_header=1, dtype=np.uint8)\n",
    "            return positions[:,1] == cell_id\n",
    "        \n",
    "        detected_targets = np.zeros(self.num_targets, dtype=bool)\n",
    "\n",
    "        img = self._take_photo()\n",
    "        if img is None:\n",
    "            return detected_targets\n",
    "        \n",
    "        for result in pyzbar.decode(img):\n",
    "            idx = int(result.data) - 1\n",
    "            try:\n",
    "                detected_targets[idx] = True\n",
    "            except (ValueError, IndexError):\n",
    "                pass\n",
    "                \n",
    "        return detected_targets\n",
    "\n",
    "    def update_moving_target_positions(self):\n",
    "        Simulation.update_moving_target_positions(self.target_positions)\n",
    "        \n",
    "    def _setup_camera(self):\n",
    "        assert self.drone.media_autoconnect\n",
    "        self.drone.media.integrity_check = True\n",
    "        is_indexed = False\n",
    "        while not is_indexed:\n",
    "            is_indexed = self.drone.media(\n",
    "                indexing_state(state=\"indexed\")\n",
    "            ).wait(_timeout=5).success()\n",
    "        \n",
    "        self.drone(set_camera_mode(cam_id=0, value=\"photo\")).wait()\n",
    "\n",
    "        assert self.drone(\n",
    "            set_photo_mode(\n",
    "                cam_id=0,\n",
    "                mode=\"single\",\n",
    "                format= \"rectilinear\",\n",
    "                file_format=\"jpeg\",\n",
    "                # the following are ignored in photo single mode\n",
    "                burst=\"burst_14_over_1s\",\n",
    "                bracketing=\"preset_1ev\",\n",
    "                capture_interval=5.0,\n",
    "            )\n",
    "        ).wait().success()\n",
    "\n",
    "        assert self.drone(\n",
    "            set_target(\n",
    "                gimbal_id=0,\n",
    "                control_mode=\"position\",\n",
    "                yaw_frame_of_reference=\"none\",\n",
    "                yaw=0.0,\n",
    "                pitch_frame_of_reference=\"absolute\",\n",
    "                pitch=-90.0,\n",
    "                roll_frame_of_reference=\"none\",\n",
    "                roll=0.0,\n",
    "                )\n",
    "            >> attitude(\n",
    "                pitch_absolute=-90.0, _policy=\"wait\", _float_tol=(1e-3, 1e-1)\n",
    "                )\n",
    "            ).wait(_timeout=20).success()\n",
    "    \n",
    "    def _take_photo(self):\n",
    "        photo_saved = self.drone(photo_progress(result=\"photo_saved\", _policy=\"wait\"))\n",
    "        self.drone(take_photo(cam_id=0)).wait()\n",
    "        \n",
    "        photo_taken = False\n",
    "        tries = 0\n",
    "        while not photo_taken:\n",
    "            tries += 1\n",
    "            if tries > 3:\n",
    "#                 assert False, \"take_photo timedout\"\n",
    "                print(\"take_photo timedout\")\n",
    "                return None\n",
    "            photo_taken = photo_saved.wait(_timeout=5).success()\n",
    "            \n",
    "        # get the bytes of the image\n",
    "        media_id = photo_saved.received_events().last().args[\"media_id\"]\n",
    "        for _ in range(5):\n",
    "            media_info_response = requests.get(ANAFI_MEDIA_API_URL + media_id, timeout=10)\n",
    "            if media_info_response.status_code == 200:\n",
    "                break\n",
    "        try:\n",
    "            media_info_response.raise_for_status()\n",
    "        except requests.exceptions.HTTPError as err:\n",
    "            print(err)\n",
    "            return None\n",
    "        \n",
    "        resource = media_info_response.json()[\"resources\"][0]\n",
    "        image_response = requests.get(ANAFI_URL + resource[\"url\"], stream=True)\n",
    "        image_response.raise_for_status()\n",
    "        \n",
    "        img = Image.open(BytesIO(image_response.content))\n",
    "        \n",
    "        # delete the image stored on the drone\n",
    "        photo_deleted = False\n",
    "        delete_tries = 0\n",
    "        while not photo_deleted:\n",
    "            delete_tries += 1\n",
    "            if delete_tries > 3:\n",
    "#                 assert False, \"Failed to delete media {} {}\".format(media_id, delete.explain())\n",
    "                print(\"Failed to delete media {} {}\".format(media_id, delete.explain()))\n",
    "                break\n",
    "            delete = delete_media(media_id, _timeout=10)\n",
    "            photo_deleted = self.drone.media(delete).wait().success()\n",
    "        \n",
    "        return img\n",
    "    \n",
    "    def _takeoff(self):\n",
    "        takeoff_success = self._success_if_takeoff()\n",
    "        if not takeoff_success:\n",
    "            print(\"Retrying taking off...\")\n",
    "            takeoff_success = self._success_if_takeoff()\n",
    "    \n",
    "    def _success_if_takeoff(self):\n",
    "        return self.drone(\n",
    "                FlyingStateChanged(state=\"hovering\")\n",
    "                | (TakeOff() & FlyingStateChanged(state=\"hovering\"))\n",
    "            ).wait(10).success()\n",
    "        \n",
    "    def _land(self):\n",
    "        self.drone(PCMD(1, 0, 0, 0, 0, 0) >> FlyingStateChanged(state=\"hovering\", _timeout=5)).wait()\n",
    "        assert self.drone(Landing() >> FlyingStateChanged(state=\"landed\")).wait().success()\n",
    "        \n",
    "    def __del__(self):\n",
    "        self._land()\n",
    "        self.drone.disconnect()\n",
    "        del state\n",
    "        del reward\n",
    "        del action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d67d9ba",
   "metadata": {},
   "source": [
    "## Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0b6bde50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simulation:\n",
    "    sphinx = jsonrpclib.Server('http://127.0.0.1:8383')\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    @staticmethod\n",
    "    def disable_battery():\n",
    "        Simulation.sphinx.SetParam(machine='anafi4k',\n",
    "                                   object='lipobattery/lipobattery',\n",
    "                                   parameter='discharge_speed_factor',\n",
    "                                   value='0')\n",
    "    \n",
    "    @staticmethod\n",
    "    def reset_world():\n",
    "        Simulation.sphinx.TriggerAction(machine='world',\n",
    "                                        object='fwman/fwman',\n",
    "                                        action='world_reset_all')\n",
    "    \n",
    "    def find_distance(x, y):\n",
    "        x = x - y[:,np.newaxis]\n",
    "        x = x**2\n",
    "        x = x.sum(axis=2)\n",
    "        x = np.sqrt(x)\n",
    "        return x\n",
    "\n",
    "    # check if the target overlaps with another target or border\n",
    "    @classmethod\n",
    "    def is_overlapping(cls, x, y=None, limit=0.4):\n",
    "        # check with the borders\n",
    "        borders_parallel_y = np.array([5, 3, 1, -1, -3, -5])\n",
    "        x1 = abs(x[:,0] - borders_parallel_y[:,np.newaxis]) < 0.2 # what should this be changed to?\n",
    "\n",
    "        borders_parallel_x = np.array([8, 4.8, 1.6, -1.6, -4.8, -8])\n",
    "        x2 = abs(x[:,1] - borders_parallel_x[:,np.newaxis]) < 0.2 # what should this be changed to?\n",
    "        \n",
    "        # check if outside the boundaries\n",
    "        x3 = (\n",
    "            (x[:, 0] > borders_parallel_y.max()) | (x[:, 0] < borders_parallel_y.min()) | \n",
    "            (x[:, 1] > borders_parallel_x.max()) | (x[:, 1] < borders_parallel_x.min())\n",
    "        )\n",
    "\n",
    "        # check with another target\n",
    "        if y is None: y = x\n",
    "        x = cls.find_distance(x, y) < limit\n",
    "        x = np.triu(x, k=1)\n",
    "\n",
    "        x = np.vstack((x, x1, x2, x3))\n",
    "        x = np.any(x, axis=0)\n",
    "        return x\n",
    "    \n",
    "    def coord_to_cellid(locs):\n",
    "        cell_boundaries = [\n",
    "           (locs[:,0] > 7.5) & (locs[:,1] > 12),\n",
    "           (locs[:,0] > 7.5) & (locs[:,1] > 4),\n",
    "           (locs[:,0] > 7.5) & (locs[:,1] > -4),\n",
    "           (locs[:,0] > 7.5) & (locs[:,1] > -12),\n",
    "           (locs[:,0] > 7.5) & (locs[:,1] > -20),\n",
    "           (locs[:,0] > 2.5) & (locs[:,1] > 12),\n",
    "           (locs[:,0] > 2.5) & (locs[:,1] > 4),\n",
    "           (locs[:,0] > 2.5) & (locs[:,1] > -4),\n",
    "           (locs[:,0] > 2.5) & (locs[:,1] > -12),\n",
    "           (locs[:,0] > 2.5) & (locs[:,1] > -20),\n",
    "           (locs[:,0] > -2.5) & (locs[:,1] > 12),\n",
    "           (locs[:,0] > -2.5) & (locs[:,1] > 4),\n",
    "           (locs[:,0] > -2.5) & (locs[:,1] > -4),\n",
    "           (locs[:,0] > -2.5) & (locs[:,1] > -12),\n",
    "           (locs[:,0] > -2.5) & (locs[:,1] > -20),\n",
    "           (locs[:,0] > -7.5) & (locs[:,1] > 12),\n",
    "           (locs[:,0] > -7.5) & (locs[:,1] > 4),\n",
    "           (locs[:,0] > -7.5) & (locs[:,1] > -4),\n",
    "           (locs[:,0] > -7.5) & (locs[:,1] > -12),\n",
    "           (locs[:,0] > -7.5) & (locs[:,1] > -20),\n",
    "           (locs[:,0] > -12.5) & (locs[:,1] > 12),\n",
    "           (locs[:,0] > -12.5) & (locs[:,1] > 4),\n",
    "           (locs[:,0] > -12.5) & (locs[:,1] > -4),\n",
    "           (locs[:,0] > -12.5) & (locs[:,1] > -12),\n",
    "           (locs[:,0] > -12.5) & (locs[:,1] > -20),\n",
    "        ]\n",
    "        cell_ids = np.arange(25) + 1\n",
    "        return np.select(cell_boundaries, cell_ids)\n",
    "    \n",
    "    @classmethod\n",
    "    def gen_targets_pos(cls, num_targets):\n",
    "\n",
    "        num_mobile = 3\n",
    "        num_fixed = num_targets-num_mobile\n",
    "\n",
    "        mu_x_fixed = 2.2\n",
    "        variance_x_fixed = 1\n",
    "        mu_y_fixed = -4 \n",
    "        variance_y_fixed = 1.5\n",
    "        distribution_fixed = multivariate_normal(\n",
    "            [mu_x_fixed, mu_y_fixed], \n",
    "            [[variance_x_fixed, 0], [0, variance_y_fixed]]\n",
    "        )\n",
    "\n",
    "        mu_x_mobile = -3\n",
    "        variance_x_mobile = 0.5\n",
    "        mu_y_mobile = 2\n",
    "        variance_y_mobile = 4.5\n",
    "        distribution_mobile = multivariate_normal(\n",
    "            [mu_x_mobile, mu_y_mobile],\n",
    "            [[variance_x_mobile, 0], [0, variance_y_mobile]]\n",
    "        )\n",
    "\n",
    "        locs = np.empty((num_targets,2))\n",
    "        locs[:num_fixed] = distribution_fixed.rvs(size=num_fixed)\n",
    "        locs[num_fixed:] = distribution_mobile.rvs(size=num_mobile)\n",
    "\n",
    "        overlapping_targets = Simulation.is_overlapping(locs)\n",
    "        while np.any(overlapping_targets):\n",
    "            num_fixed_overlaps = np.count_nonzero(overlapping_targets[:num_fixed])\n",
    "            num_mobile_overlaps = np.count_nonzero(overlapping_targets[num_fixed:])\n",
    "            locs[:num_fixed][overlapping_targets[:num_fixed]] = distribution_fixed.rvs(size=num_fixed_overlaps)\n",
    "            locs[num_fixed:][overlapping_targets[num_fixed:]] = distribution_mobile.rvs(size=num_mobile_overlaps)\n",
    "            overlapping_targets = Simulation.is_overlapping(locs)\n",
    "        \n",
    "        np.savetxt('../comm/positions.csv', \n",
    "                   np.hstack((np.arange(len(locs))[:,np.newaxis] + 1, locs)),\n",
    "                   fmt='%.4f',\n",
    "                   delimiter=',',\n",
    "                  )\n",
    "        \n",
    "        print(locs)\n",
    "        return cls.coord_to_cellid(locs) # , locs\n",
    "    \n",
    "    @classmethod\n",
    "    def update_moving_target_positions(cls, arr, ids=[8, 9, 10]):\n",
    "        ids = np.array(ids)\n",
    "        for i in ids:\n",
    "            locs = np.loadtxt(\"../comm/position_\" + str(i) + \".csv\", delimiter=',')\n",
    "            arr[i-1] = cls.coord_to_cellid(locs[np.newaxis,:])\n",
    "            \n",
    "        return arr\n",
    "    \n",
    "    @staticmethod\n",
    "    def cease_targets():\n",
    "        f = open(\"../comm/toggle_movement.txt\", \"w\")\n",
    "        f.write(\"0\")\n",
    "        f.close()\n",
    "    \n",
    "    @staticmethod\n",
    "    def move_targets():\n",
    "        f = open(\"../comm/toggle_movement.txt\", \"w\")\n",
    "        f.write(\"1\")\n",
    "        f.close()\n",
    "        \n",
    "    @staticmethod\n",
    "    def reset_targets(release=False):\n",
    "        f = open(\"../comm/reset_position.txt\", \"w\")\n",
    "        if not release:\n",
    "            f.write(\"1\\n\")\n",
    "        else:\n",
    "            f.write(\"0\\n\")\n",
    "        f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddee8aba",
   "metadata": {},
   "source": [
    "# Define a Gym Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "341caa98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnafiEnv(Env):\n",
    "    def __init__(self, num_targets, max_timestep, drone_ip=\"10.202.0.1\", is_training=False):\n",
    "        super(AnafiEnv, self).__init__()\n",
    "        \n",
    "        Simulation.disable_battery()\n",
    "        Simulation.cease_targets()\n",
    "        \n",
    "        self.num_targets = num_targets\n",
    "        self.max_timestep = max_timestep\n",
    "        self.begin(num_targets, max_timestep, is_training, drone_ip)\n",
    "        \n",
    "        self.action_space = spaces.Discrete(len(self.agent.action))\n",
    "        self.observation_space = spaces.Box( # {t, cell_id, [I1, I2, I3, ..., In]}\n",
    "            low=np.array([0, 1] + num_targets*[0]), \n",
    "            high=np.array([max_timestep, 25] + num_targets*[1]), \n",
    "            dtype=np.uint8,\n",
    "        )\n",
    "        \n",
    "        Simulation.move_targets()\n",
    "    \n",
    "    def begin(self, num_targets, max_timestep, is_training, drone_ip=\"10.202.0.1\"):\n",
    "        self.agent = Drone(drone_ip, num_targets, max_timestep, is_training)\n",
    "    \n",
    "    def step(self, action):\n",
    "        obs, reward, done, info = self.agent.take_action(action)\n",
    "        Simulation.move_targets()\n",
    "        return obs, reward, done, info\n",
    "    \n",
    "    def reset(self):\n",
    "        Simulation.cease_targets()\n",
    "        self.agent.target_positions = Simulation.gen_targets_pos(self.num_targets)\n",
    "        Simulation.reset_targets()\n",
    "        time.sleep(0.1)\n",
    "        Simulation.reset_targets(release=True)\n",
    "        return self.agent.reset()\n",
    "    \n",
    "    def render(self, mode='human'):\n",
    "        pass\n",
    "    \n",
    "    def close(self):\n",
    "        Simulation.cease_targets()\n",
    "        del self.agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626d1b39",
   "metadata": {},
   "source": [
    "# Run the simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "dc1dbac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def disp_info(action, observation, reward, done, info):\n",
    "#     clear_output(wait=True)\n",
    "    print(\"Action:\", info[\"action\"] + \",\", info[\"direction\"])\n",
    "    print(\"Current positions:\", info[\"positions\"])\n",
    "    print(\"State:\", observation)\n",
    "    print(\"Reward:\", reward)\n",
    "#     down_scale = 3\n",
    "#     display(img.resize((img.size[0]//down_scale, img.size[1]//down_scale)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a85b8dee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "541 Server Error: Media Not Yet Indexed for url: http://10.202.0.1:80/api/v1/media/medias\n",
      "Media are not yet indexed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.81954714 -4.4504607 ]\n",
      " [ 2.36251794 -5.03519822]\n",
      " [ 2.64674684 -3.39948958]\n",
      " [ 3.71289016 -3.87080832]\n",
      " [ 1.42596756 -4.12369412]\n",
      " [ 2.45235538 -5.69612552]\n",
      " [ 1.62222794 -3.60566696]\n",
      " [-2.59459599  1.13572553]\n",
      " [-2.63361512 -0.39251326]\n",
      " [-1.62348299  6.37305389]]\n",
      "Action: BR, Cell 13 --> Cell 19\n",
      "Current positions: [14 14  8  8 14 14 13 13 13 12]\n",
      "State: [ 1 19  0  0  0  0  0  0  1  0  0  0]\n",
      "Reward: -1\n",
      "Action: R, Cell 19 --> Cell 20\n",
      "Current positions: [14 14  8  8 14 14 13 12 13 12]\n",
      "State: [ 2 20  0  0  0  0  0  0  1  0  0  0]\n",
      "Reward: -1\n",
      "Action: FL, Cell 20 --> Cell 14\n",
      "Current positions: [14 14  8  8 14 14 13  7 12  7]\n",
      "State: [ 3 14  1  1  0  0  1  1  1  0  0  0]\n",
      "Reward: 6.0\n",
      "Action: F, Cell 14 --> Cell 9\n",
      "Current positions: [14 14  8  8 14 14 13  7  7  7]\n",
      "State: [4 9 1 1 0 0 1 1 1 0 0 0]\n",
      "Reward: -1\n",
      "Action: B, Cell 9 --> Cell 14\n",
      "Current positions: [14 14  8  8 14 14 13  7  7  7]\n",
      "State: [ 5 14  1  1  0  0  1  1  1  0  0  0]\n",
      "Reward: -1\n",
      "Action: FL, Cell 14 --> Cell 8\n",
      "Current positions: [14 14  8  8 14 14 13  7  7  7]\n",
      "State: [6 8 1 1 1 1 1 1 1 0 0 0]\n",
      "Reward: 3.0\n",
      "Action: H, Cell 8 --> Cell 8\n",
      "Current positions: [14 14  8  8 14 14 13  7  7  7]\n",
      "State: [7 8 1 1 1 1 1 1 1 0 0 0]\n",
      "Reward: -1\n",
      "Action: BR, Cell 8 --> Cell 14\n",
      "Current positions: [14 14  8  8 14 14 13  7  7  7]\n",
      "State: [ 8 14  1  1  1  1  1  1  1  0  0  0]\n",
      "Reward: -1\n",
      "Action: F, Cell 14 --> Cell 9\n",
      "Current positions: [14 14  8  8 14 14 13  7  7  7]\n",
      "State: [9 9 1 1 1 1 1 1 1 0 0 0]\n",
      "Reward: -1\n",
      "Action: L, Cell 9 --> Cell 8\n",
      "Current positions: [14 14  8  8 14 14 13  7  7  7]\n",
      "State: [10  8  1  1  1  1  1  1  1  0  0  0]\n",
      "Reward: -1\n",
      "Action: BL, Cell 8 --> Cell 12\n",
      "Current positions: [14 14  8  8 14 14 13  7  7  7]\n",
      "State: [11 12  1  1  1  1  1  1  1  0  0  0]\n",
      "Reward: -1\n",
      "Action: R, Cell 12 --> Cell 13\n",
      "Current positions: [14 14  8  8 14 14 13  7  7  7]\n",
      "State: [12 13  1  1  1  1  1  1  1  0  0  0]\n",
      "Reward: -1\n",
      "Action: F, Cell 13 --> Cell 8\n",
      "Current positions: [14 14  8  8 14 14 13  7  7  7]\n",
      "State: [13  8  1  1  1  1  1  1  1  0  0  0]\n",
      "Reward: -1\n",
      "Action: H, Cell 8 --> Cell 8\n",
      "Current positions: [14 14  8  8 14 14 13  7  7  7]\n",
      "State: [14  8  1  1  1  1  1  1  1  0  0  0]\n",
      "Reward: -1\n",
      "Action: BL, Cell 8 --> Cell 12\n",
      "Current positions: [14 14  8  8 14 14 13  7  7  7]\n",
      "State: [15 12  1  1  1  1  1  1  1  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "[[ 2.18589946 -3.0148397 ]\n",
      " [ 2.1377535  -5.46972535]\n",
      " [ 3.56375312 -4.47281551]\n",
      " [ 2.59225367 -5.09881191]\n",
      " [ 1.27310335 -4.55942568]\n",
      " [ 2.74045619 -4.18323094]\n",
      " [ 3.60908868 -5.11135976]\n",
      " [-3.49734844  5.44054953]\n",
      " [-2.54958851 -0.77902906]\n",
      " [-2.60481933  4.19492613]]\n",
      "Action: BL, Cell 8 --> Cell 12\n",
      "Current positions: [14 14  8  8 14 14 13  7  7  7]\n",
      "State: [ 0 13  1  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: L, Cell 13 --> Cell 12\n",
      "Current positions: [13 14  9  9 14  9  9 12 13 12]\n",
      "State: [ 1 12  1  0  0  0  0  0  0  1  0  1]\n",
      "Reward: 3.0\n",
      "Action: H, Cell 12 --> Cell 12\n",
      "Current positions: [13 14  9  9 14  9  9 12 13 12]\n",
      "State: [ 2 12  1  0  0  0  0  0  0  1  0  1]\n",
      "Reward: -1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function Drone.__del__ at 0x7f0fb00aa280>\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_13709/1835863229.py\", line 193, in __del__\n",
      "  File \"/tmp/ipykernel_13709/1835863229.py\", line 190, in _land\n",
      "AssertionError: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [72]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     action \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39msample()\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#     action = actions[i % len(actions)]\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     observation, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     disp_info(action, observation, reward, done, info)\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m done:\n",
      "Input \u001b[0;32mIn [70]\u001b[0m, in \u001b[0;36mAnafiEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m---> 25\u001b[0m     obs, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     Simulation\u001b[38;5;241m.\u001b[39mmove_targets()\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obs, reward, done, info\n",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36mDrone.take_action\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtake_action\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m---> 20\u001b[0m     old_cell, new_cell, action_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimestep \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     23\u001b[0m     Simulation\u001b[38;5;241m.\u001b[39mcease_targets()\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36mAction.take_action\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m old_cell_id \u001b[38;5;241m==\u001b[39m new_cell_id: \n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m old_cell_id, new_cell_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_action_name(action)\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_move_to_cell\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_cell\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_cell \u001b[38;5;241m=\u001b[39m next_cell\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m old_cell_id, new_cell_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_action_name(action)\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36mAction._move_to_cell\u001b[0;34m(self, next_cell)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_move_to_cell\u001b[39m(\u001b[38;5;28mself\u001b[39m, next_cell):        \n\u001b[0;32m---> 56\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrone\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmoveTo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_cell\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlatitude\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mnext_cell\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlongitude\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_cell\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maltitude\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEADING_DURING\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m90.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m>>\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmoveToChanged\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDONE\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/envs/dp-sim/lib/python3.8/site-packages/olympe/expectations.py:94\u001b[0m, in \u001b[0;36mExpectationBase.wait\u001b[0;34m(self, _timeout)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_awaited:\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 94\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_future\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m FutureTimeoutError:\n\u001b[1;32m     96\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_timedout()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/lib/python3.8/concurrent/futures/_base.py:439\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m    437\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m--> 439\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    442\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/lib/python3.8/threading.py:302\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 302\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    303\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "try:\n",
    "    del env\n",
    "except Exception as e:\n",
    "    pass\n",
    "\n",
    "env = AnafiEnv(num_targets=10, max_timestep=15, is_training=True)\n",
    "observation = env.reset()\n",
    "# actions = [7, 4, 7, 3, 1, 2, 8, 4, 1, 5, 6]\n",
    "for i in range(1000):\n",
    "    action = env.action_space.sample()\n",
    "#     action = actions[i % len(actions)]\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    disp_info(action, observation, reward, done, info)\n",
    "\n",
    "    if done:\n",
    "        print(\"The episode has ended. Resetting environment...\")\n",
    "        observation = env.reset()\n",
    "        disp_info(action, observation, 0, done, info)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3a568a",
   "metadata": {},
   "source": [
    "# Train the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19743771",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveOnBestTrainingRewardCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback for saving a model (the check is done every ``check_freq`` steps)\n",
    "    based on the training reward (in practice, we recommend using ``EvalCallback``).\n",
    "\n",
    "    :param check_freq:\n",
    "    :param log_dir: Path to the folder where the model will be saved.\n",
    "      It must contains the file created by the ``Monitor`` wrapper.\n",
    "    :param verbose: Verbosity level.\n",
    "    \"\"\"\n",
    "    def __init__(self, check_freq: int, log_dir: str, verbose: int = 1):\n",
    "        super(SaveOnBestTrainingRewardCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.log_dir = log_dir\n",
    "        self.save_path = os.path.join(log_dir, 'best_model')\n",
    "        self.best_mean_reward = -np.inf\n",
    "        self.progress_dir = os.path.join(log_dir, 'progress')\n",
    "        os.makedirs(self.progress_dir, exist_ok=True)\n",
    "\n",
    "    def _init_callback(self) -> None:\n",
    "        pass\n",
    "#         # Create folder if needed\n",
    "#         if self.save_path is not None:\n",
    "#             os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % 5000 == 0:\n",
    "            self.model.save(os.path.join(progress_dir, 'model_' + str(self.n_calls)))\n",
    "            \n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "\n",
    "            # Retrieve training reward\n",
    "            x, y = ts2xy(load_results(self.log_dir), 'timesteps')\n",
    "            if len(x) > 0:\n",
    "                # Mean training reward over the last 100 episodes\n",
    "                mean_reward = np.mean(y[-100:])\n",
    "                if self.verbose > 0:\n",
    "                    print(f\"Num timesteps: {self.num_timesteps}\")\n",
    "                    print(f\"Best mean reward: {self.best_mean_reward:.2f} - Last mean reward per episode: {mean_reward:.2f}\")\n",
    "\n",
    "                # New best model, you could save the agent here\n",
    "                if mean_reward > self.best_mean_reward:\n",
    "                    self.best_mean_reward = mean_reward\n",
    "                    # Example for saving best model\n",
    "                    if self.verbose > 0:\n",
    "                        print(f\"Saving new best model to {self.save_path}\")\n",
    "                    self.model.save(self.save_path)\n",
    "        \n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "08ce3ab5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "541 Server Error: Media Not Yet Indexed for url: http://10.202.0.1:80/api/v1/media/medias\n",
      "Media are not yet indexed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Logging to logs/PPO_5_0\n",
      "[1 8 0 0 0 0 0 0 0 0 0 0]\n",
      "-1\n",
      "[2 2 0 0 0 0 0 0 0 0 0 0]\n",
      "-1\n",
      "[3 3 0 0 0 0 0 0 0 0 0 0]\n",
      "-1\n",
      "[4 3 0 0 0 0 0 0 0 0 0 0]\n",
      "-1\n",
      "[5 3 0 0 0 0 0 0 0 0 0 0]\n",
      "-1\n",
      "[6 9 1 0 0 0 0 0 1 0 0 0]\n",
      "3.0\n",
      "[7 8 1 0 0 0 0 0 1 0 0 0]\n",
      "-1\n",
      "[8 3 1 0 0 0 0 0 1 0 0 0]\n",
      "-1\n",
      "[9 3 1 0 0 0 0 0 1 0 0 0]\n",
      "-1\n",
      "[10  4  1  0  0  1  1  0  1  0  0  0]\n",
      "3.0\n",
      "[11  4  1  0  0  1  1  0  1  0  0  0]\n",
      "-1\n",
      "[12 10  1  1  0  1  1  1  1  0  0  0]\n",
      "3.0\n",
      "[13 10  1  1  0  1  1  1  1  0  0  0]\n",
      "-1\n",
      "[14 10  1  1  0  1  1  1  1  0  0  0]\n",
      "-1\n",
      "[15  5  1  1  0  1  1  1  1  0  0  0]\n",
      "-1\n",
      "[16 10  1  1  0  1  1  1  1  0  0  0]\n",
      "-1\n",
      "[17 10  1  1  0  1  1  1  1  0  0  0]\n",
      "-1\n",
      "[18  9  1  1  0  1  1  1  1  0  0  0]\n",
      "-1\n",
      "[19 15  1  1  0  1  1  1  1  0  0  0]\n",
      "-1\n",
      "[20 14  1  1  1  1  1  1  1  0  0  0]\n",
      "1.5\n",
      "[ 1 12  0  0  0  0  0  0  0  0  0  0]\n",
      "-1\n",
      "[ 2 13  0  0  0  0  0  0  0  0  0  0]\n",
      "-1\n",
      "[3 8 0 0 0 0 0 0 0 0 0 0]\n",
      "-1\n",
      "[4 8 0 0 0 0 0 0 0 0 0 0]\n",
      "-1\n",
      "[ 5 12  0  0  0  0  0  0  0  1  0  0]\n",
      "1.5\n",
      "[ 6 16  0  0  0  0  0  0  0  1  0  0]\n",
      "-1\n",
      "[ 7 16  0  0  0  0  0  0  0  1  0  0]\n",
      "-1\n",
      "[ 8 22  0  0  0  0  0  0  0  1  0  0]\n",
      "-1\n",
      "[ 9 18  0  0  0  0  0  0  0  1  0  0]\n",
      "-1\n",
      "[10 14  0  0  0  0  0  0  1  1  0  0]\n",
      "1.5\n",
      "[11  8  0  0  0  0  0  0  1  1  0  0]\n",
      "-1\n",
      "[12  8  0  0  0  0  0  0  1  1  0  0]\n",
      "-1\n",
      "[13  2  0  0  0  0  0  0  1  1  0  0]\n",
      "-1\n",
      "[14  2  0  0  0  0  0  0  1  1  0  0]\n",
      "-1\n",
      "[15  2  0  0  0  0  0  0  1  1  0  0]\n",
      "-1\n",
      "[16  2  0  0  0  0  0  0  1  1  0  0]\n",
      "-1\n",
      "[17  8  0  0  0  0  0  0  1  1  0  0]\n",
      "-1\n",
      "[18 14  0  0  0  0  0  0  1  1  0  0]\n",
      "-1\n",
      "[19 18  0  0  0  0  0  0  1  1  0  0]\n",
      "-1\n",
      "[20 12  0  0  0  0  0  0  1  1  0  0]\n",
      "-1\n",
      "[ 1 17  0  0  0  0  0  0  0  0  0  1]\n",
      "1.5\n",
      "[ 2 18  0  0  0  0  0  0  0  0  1  1]\n",
      "1.5\n",
      "[ 3 23  0  0  0  0  0  0  0  0  1  1]\n",
      "-1\n",
      "[ 4 22  0  0  0  0  0  0  0  0  1  1]\n",
      "-1\n",
      "[ 5 22  0  0  0  0  0  0  0  0  1  1]\n",
      "-1\n",
      "[ 6 22  0  0  0  0  0  0  0  0  1  1]\n",
      "-1\n",
      "[ 7 17  0  0  0  0  0  0  0  0  1  1]\n",
      "-1\n",
      "[ 8 23  0  0  0  0  0  0  0  0  1  1]\n",
      "-1\n",
      "[ 9 18  0  0  0  0  0  0  0  0  1  1]\n",
      "-1\n",
      "[10 12  0  0  0  0  0  0  0  0  1  1]\n",
      "-1\n",
      "[11  7  0  0  0  0  0  0  0  0  1  1]\n",
      "-1\n",
      "[12 12  0  0  0  0  0  0  0  0  1  1]\n",
      "-1\n",
      "[13 11  0  0  0  0  0  0  0  0  1  1]\n",
      "-1\n",
      "[14 16  0  0  0  0  0  0  0  0  1  1]\n",
      "-1\n",
      "[15 21  0  0  0  0  0  0  0  0  1  1]\n",
      "-1\n",
      "[16 21  0  0  0  0  0  0  0  0  1  1]\n",
      "-1\n",
      "[17 21  0  0  0  0  0  0  0  0  1  1]\n",
      "-1\n",
      "[18 21  0  0  0  0  0  0  0  0  1  1]\n",
      "-1\n",
      "[19 21  0  0  0  0  0  0  0  0  1  1]\n",
      "-1\n",
      "[20 21  0  0  0  0  0  0  0  0  1  1]\n",
      "-1\n",
      "[ 1 18  0  0  0  0  0  0  0  0  0  0]\n",
      "-1\n",
      "[ 2 19  0  0  0  0  0  0  0  0  0  0]\n",
      "-1\n",
      "[ 3 25  0  0  0  0  0  0  0  0  0  0]\n",
      "-1\n",
      "[ 4 25  0  0  0  0  0  0  0  0  0  0]\n",
      "-1\n",
      "[ 5 25  0  0  0  0  0  0  0  0  0  0]\n",
      "-1\n",
      "[ 6 25  0  0  0  0  0  0  0  0  0  0]\n",
      "-1\n",
      "[ 7 19  0  0  0  0  0  0  0  0  0  0]\n",
      "-1\n",
      "[ 8 23  0  0  0  0  0  0  0  0  0  0]\n",
      "-1\n",
      "[ 9 23  0  0  0  0  0  0  0  0  0  0]\n",
      "-1\n",
      "[10 19  0  0  0  0  0  0  0  0  0  0]\n",
      "-1\n",
      "[11 24  0  0  0  0  0  0  0  0  0  0]\n",
      "-1\n",
      "[12 20  0  0  0  0  0  0  0  0  0  0]\n",
      "-1\n",
      "[13 20  0  0  0  0  0  0  0  0  0  0]\n",
      "-1\n",
      "[14 20  0  0  0  0  0  0  0  0  0  0]\n",
      "-1\n",
      "[15 20  0  0  0  0  0  0  0  0  0  0]\n",
      "-1\n",
      "[16 20  0  0  0  0  0  0  0  0  0  0]\n",
      "-1\n",
      "[17 24  0  0  0  0  0  0  0  0  0  0]\n",
      "-1\n",
      "[18 20  0  0  0  0  0  0  0  0  0  0]\n",
      "-1\n",
      "[19 24  0  0  0  0  0  0  0  0  0  0]\n",
      "-1\n",
      "[20 25  0  0  0  0  0  0  0  0  0  0]\n",
      "-1\n",
      "[ 1 12  0  0  0  0  0  0  0  0  0  0]\n",
      "-1\n",
      "[ 2 18  0  0  0  0  0  0  0  0  0  0]\n",
      "-1\n",
      "[ 3 22  0  0  0  0  0  0  0  0  0  0]\n",
      "-1\n",
      "[ 4 22  0  0  0  0  0  0  0  0  0  0]\n",
      "-1\n",
      "[ 5 22  0  0  0  0  0  0  0  0  0  0]\n",
      "-1\n",
      "[ 6 22  0  0  0  0  0  0  0  0  0  0]\n",
      "-1\n",
      "[ 7 23  0  0  0  0  0  0  0  0  0  0]\n",
      "-1\n",
      "[ 8 23  0  0  0  0  0  0  0  0  0  0]\n",
      "-1\n",
      "[ 9 22  0  0  0  0  0  0  0  0  0  0]\n",
      "-1\n",
      "[10 22  0  0  0  0  0  0  0  0  0  0]\n",
      "-1\n",
      "[11 23  0  0  0  0  0  0  0  0  0  0]\n",
      "-1\n",
      "[12 23  0  0  0  0  0  0  0  0  0  0]\n",
      "-1\n",
      "[13 23  0  0  0  0  0  0  0  0  0  0]\n",
      "-1\n",
      "[14 19  0  0  0  0  0  0  0  0  0  0]\n",
      "-1\n",
      "[15 15  0  0  0  0  0  0  0  0  0  0]\n",
      "-1\n",
      "[16 19  0  0  0  0  0  0  0  0  0  0]\n",
      "-1\n",
      "[17 23  0  0  0  0  0  0  0  0  0  0]\n",
      "-1\n",
      "[18 17  0  0  0  0  0  0  0  0  0  0]\n",
      "-1\n",
      "[19 18  0  0  0  0  0  0  0  0  0  0]\n",
      "-1\n",
      "[20 14  0  0  0  0  0  0  0  0  0  0]\n",
      "-1\n",
      "[ 1 12  0  0  0  0  0  0  0  0  0  0]\n",
      "-1\n",
      "[ 2 16  0  0  0  0  0  0  0  1  0  0]\n",
      "1.5\n",
      "[ 3 16  0  0  0  0  0  0  0  1  0  0]\n",
      "-1\n",
      "[ 4 17  0  0  0  0  0  0  0  1  0  0]\n",
      "-1\n",
      "[ 5 11  0  0  0  0  0  0  0  1  0  0]\n",
      "-1\n",
      "[ 6 12  0  0  0  0  0  0  0  1  0  1]\n",
      "1.5\n",
      "[ 7 12  0  0  0  0  0  0  0  1  0  1]\n",
      "-1\n",
      "[ 8 11  0  0  0  0  0  0  0  1  0  1]\n",
      "-1\n",
      "[ 9 12  0  0  0  0  0  0  0  1  0  1]\n",
      "-1\n",
      "[10 12  0  0  0  0  0  0  0  1  0  1]\n",
      "-1\n",
      "[11 16  0  0  0  0  0  0  0  1  0  1]\n",
      "-1\n",
      "[12 21  0  0  0  0  0  0  0  1  0  1]\n",
      "-1\n",
      "[13 21  0  0  0  0  0  0  0  1  0  1]\n",
      "-1\n",
      "[14 16  0  0  0  0  0  0  0  1  0  1]\n",
      "-1\n",
      "[15 16  0  0  0  0  0  0  0  1  0  1]\n",
      "-1\n",
      "[16 16  0  0  0  0  0  0  0  1  0  1]\n",
      "-1\n",
      "[17 16  0  0  0  0  0  0  0  1  0  1]\n",
      "-1\n",
      "[18 22  0  0  0  0  0  0  0  1  0  1]\n",
      "-1\n",
      "[19 21  0  0  0  0  0  0  0  1  0  1]\n",
      "-1\n",
      "[20 17  0  0  0  0  0  0  0  1  0  1]\n",
      "-1\n",
      "[1 8 0 0 0 0 0 0 0 0 0 0]\n",
      "-1\n",
      "[2 8 0 0 0 0 0 0 0 0 0 0]\n",
      "-1\n",
      "[ 3 13  0  0  0  0  0  0  0  0  1  0]\n",
      "1.5\n",
      "[4 8 0 0 0 0 0 0 0 0 1 0]\n",
      "-1\n",
      "[5 2 0 0 0 0 0 0 0 0 1 0]\n",
      "-1\n",
      "[6 2 0 0 0 0 0 0 0 0 1 0]\n",
      "-1\n",
      "[7 2 0 0 0 0 0 0 0 0 1 0]\n",
      "-1\n",
      "[8 2 0 0 0 0 0 0 0 0 1 0]\n",
      "-1\n",
      "[9 7 0 0 0 0 0 0 0 0 1 0]\n",
      "-1\n",
      "[10  6  0  0  0  0  0  0  0  0  1  1]\n",
      "1.5\n",
      "[11  6  0  0  0  0  0  0  0  0  1  1]\n",
      "-1\n",
      "[12  6  0  0  0  0  0  0  0  0  1  1]\n",
      "-1\n",
      "[13  6  0  0  0  0  0  0  0  0  1  1]\n",
      "-1\n",
      "[14 12  0  0  0  0  0  0  0  0  1  1]\n",
      "-1\n",
      "[15 12  0  0  0  0  0  0  0  0  1  1]\n",
      "-1\n",
      "[16 16  0  0  0  0  0  0  0  0  1  1]\n",
      "-1\n",
      "[17 16  0  0  0  0  0  0  0  0  1  1]\n",
      "-1\n",
      "[18 16  0  0  0  0  0  0  0  0  1  1]\n",
      "-1\n",
      "[19 17  0  0  0  0  0  0  0  0  1  1]\n",
      "-1\n",
      "[20 12  0  0  0  0  0  0  0  0  1  1]\n",
      "-1\n",
      "[1 7 0 0 0 0 0 0 0 0 0 0]\n",
      "-1\n",
      "[ 2 13  0  0  0  0  0  0  0  0  0  0]\n",
      "-1\n",
      "[3 7 0 0 0 0 0 0 0 0 0 0]\n",
      "-1\n",
      "[4 3 0 0 0 1 0 0 0 0 0 0]\n",
      "1.5\n",
      "[5 9 1 1 0 1 0 1 0 0 0 0]\n",
      "4.5\n",
      "[6 4 1 1 0 1 0 1 0 0 0 0]\n",
      "-1\n",
      "[7 4 1 1 0 1 0 1 0 0 0 0]\n",
      "-1\n",
      "[8 5 1 1 0 1 0 1 0 0 0 0]\n",
      "-1\n",
      "[9 9 1 1 0 1 0 1 0 0 0 0]\n",
      "-1\n",
      "[10 14  1  1  0  1  0  1  0  0  0  0]\n",
      "-1\n",
      "[11 18  1  1  0  1  0  1  0  0  0  0]\n",
      "-1\n",
      "[12 17  1  1  0  1  0  1  0  0  0  0]\n",
      "-1\n",
      "[13 23  1  1  0  1  0  1  0  0  0  0]\n",
      "-1\n",
      "[14 23  1  1  0  1  0  1  0  0  0  0]\n",
      "-1\n",
      "[15 23  1  1  0  1  0  1  0  0  0  0]\n",
      "-1\n",
      "[16 18  1  1  0  1  0  1  0  0  0  0]\n",
      "-1\n",
      "[17 14  1  1  0  1  0  1  0  0  0  0]\n",
      "-1\n",
      "[18  8  1  1  1  1  0  1  0  0  0  0]\n",
      "1.5\n",
      "[19  9  1  1  1  1  0  1  0  0  0  0]\n",
      "-1\n",
      "[20  9  1  1  1  1  0  1  0  0  0  0]\n",
      "-1\n",
      "[ 1 14  0  0  0  0  0  0  0  0  0  0]\n",
      "-1\n",
      "[ 2 15  0  0  0  0  0  0  0  0  0  0]\n",
      "-1\n",
      "[3 9 0 1 1 0 1 1 1 0 0 0]\n",
      "7.5\n",
      "[4 3 0 1 1 0 1 1 1 0 0 0]\n",
      "-1\n",
      "[5 3 0 1 1 0 1 1 1 0 0 0]\n",
      "-1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-19 17:51:39,048 \u001b[31m[ERROR] \u001b[0m\tulog - pomp - epoll_ctl(fd=154) err=9(Bad file descriptor)\u001b[0m\n",
      "2022-03-19 17:51:39,049 \u001b[31m[ERROR] \u001b[0m\tulog - pomp - epoll_ctl op=2 cb=0x7fdf751a7240 userdata=0x7fde10327210\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 2 0 1 1 0 1 1 1 0 0 0]\n",
      "-1\n",
      "[7 2 0 1 1 0 1 1 1 0 0 0]\n",
      "-1\n",
      "[8 2 0 1 1 0 1 1 1 0 0 0]\n",
      "-1\n",
      "[9 1 0 1 1 0 1 1 1 0 0 0]\n",
      "-1\n",
      "[10  1  0  1  1  0  1  1  1  0  0  0]\n",
      "-1\n",
      "[11  1  0  1  1  0  1  1  1  0  0  0]\n",
      "-1\n",
      "[12  1  0  1  1  0  1  1  1  0  0  0]\n",
      "-1\n",
      "[13  1  0  1  1  0  1  1  1  0  0  0]\n",
      "-1\n",
      "[14  1  0  1  1  0  1  1  1  0  0  0]\n",
      "-1\n",
      "[15  1  0  1  1  0  1  1  1  0  0  0]\n",
      "-1\n",
      "[16  1  0  1  1  0  1  1  1  0  0  0]\n",
      "-1\n",
      "[17  1  0  1  1  0  1  1  1  0  0  0]\n",
      "-1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [21]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m model \u001b[38;5;241m=\u001b[39m PPO(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMlpPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m, env, n_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2048\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, tensorboard_log\u001b[38;5;241m=\u001b[39mlog_dir)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# model = PPO.load(os.path.join(log_dir, str(run-1) + \"_run\"), env)\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m524_288\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPPO_\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m model\u001b[38;5;241m.\u001b[39msave(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(log_dir, \u001b[38;5;28mstr\u001b[39m(run) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_run\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/envs/dp-sim/lib/python3.8/site-packages/stable_baselines3/ppo/ppo.py:299\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    288\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    296\u001b[0m     reset_num_timesteps: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    297\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPPO\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mPPO\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_eval_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_eval_episodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_log_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_log_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/envs/dp-sim/lib/python3.8/site-packages/stable_baselines3/common/on_policy_algorithm.py:250\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[1;32m    246\u001b[0m callback\u001b[38;5;241m.\u001b[39mon_training_start(\u001b[38;5;28mlocals\u001b[39m(), \u001b[38;5;28mglobals\u001b[39m())\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 250\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m continue_training \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    253\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/envs/dp-sim/lib/python3.8/site-packages/stable_baselines3/common/on_policy_algorithm.py:178\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space, gym\u001b[38;5;241m.\u001b[39mspaces\u001b[38;5;241m.\u001b[39mBox):\n\u001b[1;32m    176\u001b[0m     clipped_actions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(actions, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mlow, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mhigh)\n\u001b[0;32m--> 178\u001b[0m new_obs, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclipped_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mnum_envs\n\u001b[1;32m    182\u001b[0m \u001b[38;5;66;03m# Give access to local variables\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/envs/dp-sim/lib/python3.8/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:162\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \n\u001b[1;32m    158\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[0;32m--> 162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/envs/dp-sim/lib/python3.8/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:43\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs):\n\u001b[0;32m---> 43\u001b[0m         obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_rews[env_idx], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx]:\n\u001b[1;32m     47\u001b[0m             \u001b[38;5;66;03m# save final observation where user can get it, then reset\u001b[39;00m\n\u001b[1;32m     48\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mterminal_observation\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m obs\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/envs/dp-sim/lib/python3.8/site-packages/stable_baselines3/common/monitor.py:90\u001b[0m, in \u001b[0;36mMonitor.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneeds_reset:\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTried to step environment that needs reset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 90\u001b[0m observation, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrewards\u001b[38;5;241m.\u001b[39mappend(reward)\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m done:\n",
      "Input \u001b[0;32mIn [19]\u001b[0m, in \u001b[0;36mAnafiEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m---> 25\u001b[0m     obs, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     Simulation\u001b[38;5;241m.\u001b[39mmove_targets()\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obs, reward, done, info\n",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36mDrone.take_action\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimestep \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     23\u001b[0m Simulation\u001b[38;5;241m.\u001b[39mcease_targets()\n\u001b[0;32m---> 24\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m detected_targets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_targets(new_cell)\n\u001b[1;32m     27\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_reward(detected_targets) \u001b[38;5;66;03m# !!! _get_reward must come before self.visited_targets is changed in _get_state\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "try:\n",
    "    del env\n",
    "    del model\n",
    "except Exception as e:\n",
    "    pass\n",
    "\n",
    "# Create log dir\n",
    "log_dir = \"logs/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "run = len([filename for filename in os.listdir(log_dir) if filename.endswith(\"monitor.csv\")]) + 1\n",
    "monitor_file = os.path.join(log_dir, str(run))\n",
    "\n",
    "env = AnafiEnv(num_targets=10, max_timestep=20, is_training=True)\n",
    "env = Monitor(env, monitor_file)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "\n",
    "callback = SaveOnBestTrainingRewardCallback(check_freq=1024, log_dir=log_dir)\n",
    "\n",
    "model = PPO(\"MlpPolicy\", env, n_steps=2048, verbose=1, tensorboard_log=log_dir)\n",
    "# model = PPO.load(os.path.join(log_dir, str(run-1) + \"_run\"), env)\n",
    "model.learn(total_timesteps=524_288, callback=callback, tb_log_name=\"PPO_\" + str(run), reset_num_timesteps=False)\n",
    "model.save(os.path.join(log_dir, str(run) + \"_run\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b27d7fa",
   "metadata": {},
   "source": [
    "# Test the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fd568b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_target_pos():\n",
    "    Simulation.gen_targets_pos(10)\n",
    "    Simulation.reset_targets()\n",
    "    time.sleep(0.1)\n",
    "    Simulation.reset_targets(release=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1be2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del env\n",
    "    del model\n",
    "except Exception as e:\n",
    "    pass\n",
    "\n",
    "env = AnafiEnv(num_targets=10, max_timestep=15)\n",
    "model = PPO.load(\"logs/4_run\", env, verbose=1)\n",
    "\n",
    "# header = [\"timestep\", \"action\"]\n",
    "\n",
    "# episode = 9\n",
    "# timestep = 0\n",
    "# file = open('run_' + str(episode+1) + '.csv', 'w', encoding='UTF8', newline='')\n",
    "# writer = csv.writer(file)\n",
    "# writer.writerow(header)\n",
    "\n",
    "obs = env.reset()\n",
    "reset_target_pos()\n",
    "\n",
    "for i in range(1000):\n",
    "    action, _state = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "#     timestep += 1\n",
    "    \n",
    "#     writer.writerow([str(timestep), info[\"action\"]])\n",
    "\n",
    "    if done:\n",
    "        obs = env.reset()\n",
    "        reset_target_pos()\n",
    "        \n",
    "#         if episode >= 10:\n",
    "#             file.close()\n",
    "#             break\n",
    "#         episode += 1\n",
    "#         timestep = 0\n",
    "\n",
    "#         file = open('run_' + str(episode+1) + '.csv', 'w', encoding='UTF8', newline='')\n",
    "#         writer = csv.writer(file)\n",
    "#         writer.writerow(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacef254",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
