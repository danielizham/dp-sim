{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2cfc7f7",
   "metadata": {},
   "source": [
    "# Import all the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b225355f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import IntEnum\n",
    "import time\n",
    "import jsonrpclib\n",
    "import subprocess\n",
    "from subprocess import PIPE, Popen\n",
    "from threading  import Thread\n",
    "import sys\n",
    "import re\n",
    "from collections import OrderedDict\n",
    "\n",
    "import PySimpleGUI as sg\n",
    "\n",
    "from gym import Env, error, spaces, utils\n",
    "from stable_baselines3 import DQN, PPO, A2C, TD3, SAC\n",
    "from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.callbacks import BaseCallback, CallbackList, CheckpointCallback, EvalCallback\n",
    "from stable_baselines3.common import results_plotter\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy, plot_results\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common import results_plotter\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy, plot_results\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import shutil\n",
    "import tempfile\n",
    "import xml.etree.ElementTree as ET\n",
    "from io import StringIO, BytesIO\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from IPython.display import clear_output\n",
    "import gym\n",
    "from cv2 import QRCodeDetector\n",
    "from pyzbar import pyzbar\n",
    "\n",
    "import olympe\n",
    "from olympe.messages.ardrone3.Piloting import TakeOff, Landing, moveBy, PCMD, moveTo\n",
    "from olympe.messages.ardrone3.PilotingState import FlyingStateChanged, PositionChanged, GpsLocationChanged, moveToChanged\n",
    "from olympe.enums.ardrone3.PilotingState import FlyingStateChanged_State as FlyingState\n",
    "from olympe.messages.ardrone3.GPSSettingsState import GPSFixStateChanged, HomeChanged\n",
    "from olympe.messages.gimbal import set_target, attitude\n",
    "from olympe.messages.camera import (\n",
    "    set_camera_mode,\n",
    "    set_photo_mode,\n",
    "    take_photo,\n",
    "    photo_progress,\n",
    ")\n",
    "from olympe.media import (\n",
    "    media_created,\n",
    "    resource_created,\n",
    "    media_removed,\n",
    "    resource_removed,\n",
    "    resource_downloaded,\n",
    "    indexing_state,\n",
    "    delete_media,\n",
    "    download_media,\n",
    "    download_media_thumbnail,\n",
    "    MediaEvent,\n",
    ")\n",
    "\n",
    "from pynput.keyboard import Listener, Key, KeyCode\n",
    "from collections import defaultdict\n",
    "\n",
    "olympe.log.update_config({\n",
    "    \"loggers\": {\n",
    "        \"olympe\": {\n",
    "                \"handlers\": []\n",
    "            }\n",
    "        },\n",
    "        \"ulog\": {\n",
    "            \"level\": \"OFF\",\n",
    "            \"handlers\": [],\n",
    "        }\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0380e41c",
   "metadata": {},
   "source": [
    "# Define the constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee2702da",
   "metadata": {},
   "outputs": [],
   "source": [
    "DRONE_IP = os.environ.get(\"DRONE_IP\", \"10.202.0.1\")\n",
    "DRONE_MEDIA_PORT = os.environ.get(\"DRONE_MEDIA_PORT\", \"80\")\n",
    "\n",
    "ANAFI_URL = \"http://{}/\".format(DRONE_IP)\n",
    "ANAFI_MEDIA_API_URL = ANAFI_URL + \"api/v1/media/medias/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc9719c",
   "metadata": {},
   "source": [
    "# Define the classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f459a37",
   "metadata": {},
   "source": [
    "## Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09ac1cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Action:\n",
    "    def __init__(self, drone):\n",
    "        self.drone = drone\n",
    "        self.home = self.drone.get_state(GpsLocationChanged)\n",
    "        \n",
    "        self.current_cell = self._get_cell(13)\n",
    "        self.invalid_left_cells = [1, 6, 11, 16, 21]\n",
    "        self.invalid_forward_cells = [1, 2, 3, 4, 5]\n",
    "        self.invalid_right_cells = [5, 10, 15, 20, 25]\n",
    "        self.invalid_backward_cells = [21, 22, 23, 24, 25]\n",
    "        \n",
    "        self.Move = IntEnum(\n",
    "            'MOVE',\n",
    "            'FORWARD BACKWARD LEFT RIGHT FORWARD_LEFT FORWARD_RIGHT BACKWARD_LEFT BACKWARD_RIGHT HOVER',\n",
    "            start=0\n",
    "        )\n",
    "        \n",
    "    def take_action(self, action):\n",
    "        next_cell_id = self._get_next_cell_id(action)\n",
    "        next_cell = self._get_cell(next_cell_id)\n",
    "        \n",
    "        old_cell_id, new_cell_id = self.current_cell[\"id\"], next_cell[\"id\"]\n",
    "        if old_cell_id == new_cell_id: \n",
    "            return old_cell_id, new_cell_id, self._get_action_name(action)\n",
    "        \n",
    "        self._move_to_cell(next_cell)\n",
    "        \n",
    "        self.current_cell = next_cell\n",
    "        \n",
    "        return old_cell_id, new_cell_id, self._get_action_name(action)\n",
    "        \n",
    "    def reset(self):\n",
    "        next_cell = self._get_cell(13)\n",
    "        self._move_to_cell(next_cell)\n",
    "        \n",
    "        old_cell_id, new_cell_id = self.current_cell[\"id\"], next_cell[\"id\"]\n",
    "        self.current_cell = next_cell\n",
    "        \n",
    "        return old_cell_id, new_cell_id\n",
    "    \n",
    "    def _get_cell(self, cell_id):\n",
    "        return self._cell_coords[cell_id - 1]\n",
    "    \n",
    "    def _get_action_name(self, action):\n",
    "        direction = str(self.Move(action)).split(\".\")[1].capitalize()\n",
    "        return \"Moving \" + direction if \"hover\" not in direction.lower() else \"Hovering\"\n",
    "    \n",
    "    def _move_to_cell(self, next_cell):        \n",
    "        self.drone(\n",
    "            moveTo(next_cell[\"latitude\"],  next_cell[\"longitude\"], next_cell[\"altitude\"], \"HEADING_DURING\", 90.0)\n",
    "            >> moveToChanged(status=\"DONE\", _timeout=15)\n",
    "        ).wait()\n",
    "    \n",
    "    def _get_next_cell_id(self, action):\n",
    "        if action == self.Move.HOVER:\n",
    "            return self.current_cell[\"id\"]\n",
    "        elif action == self.Move.LEFT:\n",
    "            if self.current_cell[\"id\"] in self.invalid_left_cells:\n",
    "                return self.current_cell[\"id\"]\n",
    "            next_cell_id = self.current_cell[\"id\"] - 1\n",
    "        elif action == self.Move.RIGHT:\n",
    "            if self.current_cell[\"id\"] in self.invalid_right_cells:\n",
    "                return self.current_cell[\"id\"]\n",
    "            next_cell_id = self.current_cell[\"id\"] + 1\n",
    "        elif action == self.Move.FORWARD:\n",
    "            if self.current_cell[\"id\"] in self.invalid_forward_cells:\n",
    "                return self.current_cell[\"id\"]\n",
    "            next_cell_id = self.current_cell[\"id\"] - 5\n",
    "        elif action == self.Move.BACKWARD:\n",
    "            if self.current_cell[\"id\"] in self.invalid_backward_cells:\n",
    "                return self.current_cell[\"id\"]\n",
    "            next_cell_id = self.current_cell[\"id\"] + 5\n",
    "        elif action == self.Move.FORWARD_RIGHT:\n",
    "            if self.current_cell[\"id\"] in self.invalid_forward_cells + self.invalid_right_cells:\n",
    "                return self.current_cell[\"id\"]\n",
    "            next_cell_id = self.current_cell[\"id\"] - 4\n",
    "        elif action == self.Move.FORWARD_LEFT:\n",
    "            if self.current_cell[\"id\"] in self.invalid_forward_cells + self.invalid_left_cells:\n",
    "                return self.current_cell[\"id\"]\n",
    "            next_cell_id = self.current_cell[\"id\"] - 6\n",
    "        elif action == self.Move.BACKWARD_RIGHT:\n",
    "            if self.current_cell[\"id\"] in self.invalid_backward_cells + self.invalid_right_cells:\n",
    "                return self.current_cell[\"id\"]\n",
    "            next_cell_id = self.current_cell[\"id\"] + 6\n",
    "        elif action == self.Move.BACKWARD_LEFT:\n",
    "            if self.current_cell[\"id\"] in self.invalid_backward_cells + self.invalid_left_cells:\n",
    "                return self.current_cell[\"id\"]\n",
    "            next_cell_id = self.current_cell[\"id\"] + 4\n",
    "            \n",
    "        return next_cell_id\n",
    "    \n",
    "    @property\n",
    "    def _cell_coords(self):\n",
    "        altitude = 6.0\n",
    "        dlong = 6.8e-5 # in degrees == 5 meters along x-axis (forward[+]-backward[-])\n",
    "        dlat = 7.2e-5 # in degrees == 8 meters along y-axis (left[+]-right[-])\n",
    "        \n",
    "        home_lat = self.home[\"latitude\"]\n",
    "        home_long = self.home[\"longitude\"]\n",
    "        \n",
    "        return [\n",
    "            # cell no. 1\n",
    "            OrderedDict([('id', 1),\n",
    "                         ('latitude', home_lat + 2 * dlat),\n",
    "                         ('longitude', home_long + 2 * dlong),\n",
    "                         ('altitude', altitude)]),\n",
    "            # cell no. 2\n",
    "            OrderedDict([('id', 2),\n",
    "                         ('latitude', home_lat + 1 * dlat),\n",
    "                         ('longitude', home_long + 2 * dlong),\n",
    "                         ('altitude', altitude)]),\n",
    "            # cell no. 3\n",
    "            OrderedDict([('id', 3),\n",
    "                         ('latitude', home_lat + 0 * dlat),\n",
    "                         ('longitude', home_long + 2 * dlong),\n",
    "                         ('altitude', altitude)]),\n",
    "            # cell no. 4\n",
    "            OrderedDict([('id', 4),\n",
    "                         ('latitude', home_lat + -1 * dlat),\n",
    "                         ('longitude', home_long + 2 * dlong),\n",
    "                         ('altitude', altitude)]),\n",
    "            # cell no. 5\n",
    "            OrderedDict([('id', 5),\n",
    "                         ('latitude', home_lat + -2 * dlat),\n",
    "                         ('longitude', home_long + 2 * dlong),\n",
    "                         ('altitude', altitude)]),\n",
    "            # cell no. 6\n",
    "            OrderedDict([('id', 6),\n",
    "                         ('latitude', home_lat + 2 * dlat),\n",
    "                         ('longitude', home_long + 1 * dlong),\n",
    "                         ('altitude', altitude)]),\n",
    "            # cell no. 7\n",
    "            OrderedDict([('id', 7),\n",
    "                         ('latitude', home_lat + 1 * dlat),\n",
    "                         ('longitude', home_long + 1 * dlong),\n",
    "                         ('altitude', altitude)]),\n",
    "            # cell no. 8\n",
    "            OrderedDict([('id', 8),\n",
    "                         ('latitude', home_lat + 0 * dlat),\n",
    "                         ('longitude', home_long + 1 * dlong),\n",
    "                         ('altitude', altitude)]),\n",
    "            # cell no. 9\n",
    "            OrderedDict([('id', 9),\n",
    "                         ('latitude', home_lat + -1 * dlat),\n",
    "                         ('longitude', home_long + 1 * dlong),\n",
    "                         ('altitude', altitude)]),\n",
    "            # cell no. 10\n",
    "            OrderedDict([('id', 10),\n",
    "                         ('latitude', home_lat + -2 * dlat),\n",
    "                         ('longitude', home_long + 1 * dlong),\n",
    "                         ('altitude', altitude)]),\n",
    "            # cell no. 11\n",
    "            OrderedDict([('id', 11),\n",
    "                         ('latitude', home_lat + 2 * dlat),\n",
    "                         ('longitude', home_long + 0 * dlong),\n",
    "                         ('altitude', altitude)]),\n",
    "            # cell no. 12\n",
    "            OrderedDict([('id', 12),\n",
    "                         ('latitude', home_lat + 1 * dlat),\n",
    "                         ('longitude', home_long + 0 * dlong),\n",
    "                         ('altitude', altitude)]),\n",
    "            # cell no. 13\n",
    "            OrderedDict([('id', 13),\n",
    "                         ('latitude', home_lat + 0 * dlat),\n",
    "                         ('longitude', home_long + 0 * dlong),\n",
    "                         ('altitude', altitude)]),\n",
    "            # cell no. 14\n",
    "            OrderedDict([('id', 14),\n",
    "                         ('latitude', home_lat + -1 * dlat),\n",
    "                         ('longitude', home_long + 0 * dlong),\n",
    "                         ('altitude', altitude)]),\n",
    "            # cell no. 15\n",
    "            OrderedDict([('id', 15),\n",
    "                         ('latitude', home_lat + -2 * dlat),\n",
    "                         ('longitude', home_long + 0 * dlong),\n",
    "                         ('altitude', altitude)]),\n",
    "            # cell no. 16\n",
    "            OrderedDict([('id', 16),\n",
    "                         ('latitude', home_lat + 2 * dlat),\n",
    "                         ('longitude', home_long + -1 * dlong),\n",
    "                         ('altitude', altitude)]),\n",
    "            # cell no. 17\n",
    "            OrderedDict([('id', 17),\n",
    "                         ('latitude', home_lat + 1 * dlat),\n",
    "                         ('longitude', home_long + -1 * dlong),\n",
    "                         ('altitude', altitude)]),\n",
    "            # cell no. 18\n",
    "            OrderedDict([('id', 18),\n",
    "                         ('latitude', home_lat + 0 * dlat),\n",
    "                         ('longitude', home_long + -1 * dlong),\n",
    "                         ('altitude', altitude)]),\n",
    "            # cell no. 19\n",
    "            OrderedDict([('id', 19),\n",
    "                         ('latitude', home_lat + -1 * dlat),\n",
    "                         ('longitude', home_long + -1 * dlong),\n",
    "                         ('altitude', altitude)]),\n",
    "            # cell no. 20\n",
    "            OrderedDict([('id', 20),\n",
    "                         ('latitude', home_lat + -2 * dlat),\n",
    "                         ('longitude', home_long + -1 * dlong),\n",
    "                         ('altitude', altitude)]),\n",
    "            # cell no. 21\n",
    "            OrderedDict([('id', 21),\n",
    "                         ('latitude', home_lat + 2 * dlat),\n",
    "                         ('longitude', home_long + -2 * dlong),\n",
    "                         ('altitude', altitude)]),\n",
    "            # cell no. 22\n",
    "            OrderedDict([('id', 22),\n",
    "                         ('latitude', home_lat + 1 * dlat),\n",
    "                         ('longitude', home_long + -2 * dlong),\n",
    "                         ('altitude', altitude)]),\n",
    "            # cell no. 23\n",
    "            OrderedDict([('id', 23),\n",
    "                         ('latitude', home_lat + 0 * dlat),\n",
    "                         ('longitude', home_long + -2 * dlong),\n",
    "                         ('altitude', altitude)]),\n",
    "            # cell no. 24\n",
    "            OrderedDict([('id', 24),\n",
    "                         ('latitude', home_lat + -1 * dlat),\n",
    "                         ('longitude', home_long + -2 * dlong),\n",
    "                         ('altitude', altitude)]),\n",
    "            # cell no. 25\n",
    "            OrderedDict([('id', 25),\n",
    "                         ('latitude', home_lat + -2 * dlat),\n",
    "                         ('longitude', home_long + -2 * dlong),\n",
    "                         ('altitude', altitude)]),\n",
    "            ]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.Move)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c218a6a5",
   "metadata": {},
   "source": [
    "## Drone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "154b880f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Drone:\n",
    "    def __init__(self, drone_ip, num_targets, max_timestep, is_training=False):\n",
    "        self.drone = olympe.Drone(drone_ip)\n",
    "        self.drone.connect()\n",
    "        \n",
    "        self.drone(GPSFixStateChanged(_policy = 'wait'))\n",
    "        self._takeoff()\n",
    "        if not is_training: self._setup_camera()\n",
    "\n",
    "        self.action = Action(self.drone)\n",
    "        \n",
    "        self.is_training = is_training\n",
    "        self.num_targets = num_targets\n",
    "        self.max_timestep = max_timestep\n",
    "        self.timestep = 0\n",
    "        self.visited_targets = np.zeros(self.num_targets, dtype=bool)\n",
    "        self.target_positions = np.zeros(self.num_targets, dtype=np.uint8)\n",
    "    \n",
    "    def take_action(self, action):\n",
    "        old_cell, new_cell, action_name = self.action.take_action(action)\n",
    "        self.timestep += 1\n",
    "        detected_targets = self._detect_targets(new_cell)\n",
    "        \n",
    "        reward = self._get_reward(detected_targets) # !!! _get_reward must come before self.visited_targets is changed in _get_state\n",
    "        state = self._get_state(new_cell, detected_targets) \n",
    "        if self.timestep >= self.max_timestep or np.all(self.visited_targets):\n",
    "            done = True\n",
    "            self.visited_targets[:] = False\n",
    "        else:\n",
    "            done = False\n",
    "        info = {\n",
    "            \"action\": str(action_name), \n",
    "            \"direction\": \"Cell \" + str(old_cell) + \" --> \" + \"Cell \" + str(new_cell)\n",
    "        }\n",
    "        \n",
    "        return state, reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        old_cell, new_cell = self.action.reset()\n",
    "        detected_targets = self._detect_targets(new_cell)\n",
    "        self.timestep = 0\n",
    "        return self._get_state(new_cell, detected_targets)\n",
    "    \n",
    "    def _get_state(self, new_cell, detected_targets):\n",
    "        # {t, cell_id, [I1, I2, I3, ..., In]}\n",
    "        \n",
    "        self.visited_targets[detected_targets] = True\n",
    "        \n",
    "        return np.concatenate(([self.timestep, new_cell], self.visited_targets)).astype(np.uint8)\n",
    "    \n",
    "    def _get_reward(self, detected_targets):\n",
    "        reward_scale = 1.5\n",
    "        num_new_targets = np.count_nonzero(\n",
    "            detected_targets & (detected_targets != self.visited_targets)\n",
    "        )\n",
    "        return reward_scale * num_new_targets if num_new_targets > 0 else -1\n",
    "        \n",
    "    def _detect_targets(self, cell_id):\n",
    "        return self.target_positions == cell_id\n",
    "        \n",
    "        if self.is_training:\n",
    "            positions = np.genfromtxt('target_positions.csv', delimiter=',', skip_header=1, dtype=np.uint8)\n",
    "            return positions[:,1] == cell_id\n",
    "        \n",
    "        detected_targets = np.zeros(self.num_targets, dtype=bool)\n",
    "\n",
    "        img = self._take_photo()\n",
    "        if img is None:\n",
    "            return detected_targets\n",
    "        \n",
    "        for result in pyzbar.decode(img):\n",
    "            idx = int(result.data) - 1\n",
    "            try:\n",
    "                detected_targets[idx] = True\n",
    "            except (ValueError, IndexError):\n",
    "                pass\n",
    "                \n",
    "        return detected_targets\n",
    "\n",
    "    def _setup_camera(self):\n",
    "        assert self.drone.media_autoconnect\n",
    "        self.drone.media.integrity_check = True\n",
    "        is_indexed = False\n",
    "        while not is_indexed:\n",
    "            is_indexed = self.drone.media(\n",
    "                indexing_state(state=\"indexed\")\n",
    "            ).wait(_timeout=5).success()\n",
    "        \n",
    "        self.drone(set_camera_mode(cam_id=0, value=\"photo\")).wait()\n",
    "\n",
    "        assert self.drone(\n",
    "            set_photo_mode(\n",
    "                cam_id=0,\n",
    "                mode=\"single\",\n",
    "                format= \"rectilinear\",\n",
    "                file_format=\"jpeg\",\n",
    "                # the following are ignored in photo single mode\n",
    "                burst=\"burst_14_over_1s\",\n",
    "                bracketing=\"preset_1ev\",\n",
    "                capture_interval=5.0,\n",
    "            )\n",
    "        ).wait().success()\n",
    "\n",
    "        assert self.drone(\n",
    "            set_target(\n",
    "                gimbal_id=0,\n",
    "                control_mode=\"position\",\n",
    "                yaw_frame_of_reference=\"none\",\n",
    "                yaw=0.0,\n",
    "                pitch_frame_of_reference=\"absolute\",\n",
    "                pitch=-90.0,\n",
    "                roll_frame_of_reference=\"none\",\n",
    "                roll=0.0,\n",
    "                )\n",
    "            >> attitude(\n",
    "                pitch_absolute=-90.0, _policy=\"wait\", _float_tol=(1e-3, 1e-1)\n",
    "                )\n",
    "            ).wait(_timeout=20).success()\n",
    "    \n",
    "    def _take_photo(self):\n",
    "        photo_saved = self.drone(photo_progress(result=\"photo_saved\", _policy=\"wait\"))\n",
    "        self.drone(take_photo(cam_id=0)).wait()\n",
    "        \n",
    "        photo_taken = False\n",
    "        tries = 0\n",
    "        while not photo_taken:\n",
    "            tries += 1\n",
    "            if tries > 3:\n",
    "#                 assert False, \"take_photo timedout\"\n",
    "                print(\"take_photo timedout\")\n",
    "                return None\n",
    "            photo_taken = photo_saved.wait(_timeout=5).success()\n",
    "            \n",
    "        # get the bytes of the image\n",
    "        media_id = photo_saved.received_events().last().args[\"media_id\"]\n",
    "        for _ in range(5):\n",
    "            media_info_response = requests.get(ANAFI_MEDIA_API_URL + media_id, timeout=10)\n",
    "            if media_info_response.status_code == 200:\n",
    "                break\n",
    "        try:\n",
    "            media_info_response.raise_for_status()\n",
    "        except requests.exceptions.HTTPError as err:\n",
    "            print(err)\n",
    "            return None\n",
    "        \n",
    "        resource = media_info_response.json()[\"resources\"][0]\n",
    "        image_response = requests.get(ANAFI_URL + resource[\"url\"], stream=True)\n",
    "        image_response.raise_for_status()\n",
    "        \n",
    "        img = Image.open(BytesIO(image_response.content))\n",
    "        \n",
    "        # delete the image stored on the drone\n",
    "        photo_deleted = False\n",
    "        delete_tries = 0\n",
    "        while not photo_deleted:\n",
    "            delete_tries += 1\n",
    "            if delete_tries > 3:\n",
    "#                 assert False, \"Failed to delete media {} {}\".format(media_id, delete.explain())\n",
    "                print(\"Failed to delete media {} {}\".format(media_id, delete.explain()))\n",
    "                break\n",
    "            delete = delete_media(media_id, _timeout=10)\n",
    "            photo_deleted = self.drone.media(delete).wait().success()\n",
    "        \n",
    "        return img\n",
    "    \n",
    "    def _takeoff(self):\n",
    "        takeoff_success = self._success_if_takeoff()\n",
    "        if not takeoff_success:\n",
    "            print(\"Retrying taking off...\")\n",
    "            takeoff_success = self._success_if_takeoff()\n",
    "    \n",
    "    def _success_if_takeoff(self):\n",
    "        return self.drone(\n",
    "                FlyingStateChanged(state=\"hovering\")\n",
    "                | (TakeOff() & FlyingStateChanged(state=\"hovering\"))\n",
    "            ).wait(10).success()\n",
    "        \n",
    "    def _land(self):\n",
    "        self.drone(PCMD(1, 0, 0, 0, 0, 0) >> FlyingStateChanged(state=\"hovering\", _timeout=5)).wait()\n",
    "        assert self.drone(Landing() >> FlyingStateChanged(state=\"landed\")).wait().success()\n",
    "        \n",
    "    def __del__(self):\n",
    "        self._land()\n",
    "        self.drone.disconnect()\n",
    "        del state\n",
    "        del reward\n",
    "        del action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d67d9ba",
   "metadata": {},
   "source": [
    "## Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b6bde50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simulation:\n",
    "    sphinx = jsonrpclib.Server('http://127.0.0.1:8383')\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    @staticmethod\n",
    "    def disable_battery():\n",
    "        Simulation.sphinx.SetParam(machine='anafi4k',\n",
    "                                   object='lipobattery/lipobattery',\n",
    "                                   parameter='discharge_speed_factor',\n",
    "                                   value='0')\n",
    "    \n",
    "    @staticmethod\n",
    "    def reset_world():\n",
    "        Simulation.sphinx.TriggerAction(machine='world',\n",
    "                                        object='fwman/fwman',\n",
    "                                        action='world_reset_all')\n",
    "    \n",
    "    @staticmethod\n",
    "    def gen_targets_pos(num_targets, mu_x=-7.5, variance_x=3, mu_y=-11, variance_y=5):\n",
    "        locs = multivariate_normal([mu_x, mu_y], [[variance_x, 0], [0, variance_y]]).rvs(size=num_targets)\n",
    "        \n",
    "        cell_boundaries = [\n",
    "           (locs[:,0] > 7.5) & (locs[:,1] > 12),\n",
    "           (locs[:,0] > 7.5) & (locs[:,1] > 4),\n",
    "           (locs[:,0] > 7.5) & (locs[:,1] > -4),\n",
    "           (locs[:,0] > 7.5) & (locs[:,1] > -12),\n",
    "           (locs[:,0] > 7.5) & (locs[:,1] > -20),\n",
    "           (locs[:,0] > 2.5) & (locs[:,1] > 12),\n",
    "           (locs[:,0] > 2.5) & (locs[:,1] > 4),\n",
    "           (locs[:,0] > 2.5) & (locs[:,1] > -4),\n",
    "           (locs[:,0] > 2.5) & (locs[:,1] > -12),\n",
    "           (locs[:,0] > 2.5) & (locs[:,1] > -20),\n",
    "           (locs[:,0] > -2.5) & (locs[:,1] > 12),\n",
    "           (locs[:,0] > -2.5) & (locs[:,1] > 4),\n",
    "           (locs[:,0] > -2.5) & (locs[:,1] > -4),\n",
    "           (locs[:,0] > -2.5) & (locs[:,1] > -12),\n",
    "           (locs[:,0] > -2.5) & (locs[:,1] > -20),\n",
    "           (locs[:,0] > -7.5) & (locs[:,1] > 12),\n",
    "           (locs[:,0] > -7.5) & (locs[:,1] > 4),\n",
    "           (locs[:,0] > -7.5) & (locs[:,1] > -4),\n",
    "           (locs[:,0] > -7.5) & (locs[:,1] > -12),\n",
    "           (locs[:,0] > -7.5) & (locs[:,1] > -20),\n",
    "           (locs[:,0] > -12.5) & (locs[:,1] > 12),\n",
    "           (locs[:,0] > -12.5) & (locs[:,1] > 4),\n",
    "           (locs[:,0] > -12.5) & (locs[:,1] > -4),\n",
    "           (locs[:,0] > -12.5) & (locs[:,1] > -12),\n",
    "           (locs[:,0] > -12.5) & (locs[:,1] > -20),\n",
    "        ]\n",
    "        cell_ids = np.arange(25) + 1\n",
    "        \n",
    "        return np.select(cell_boundaries, cell_ids)\n",
    "    \n",
    "    @staticmethod\n",
    "    def cease_targets():\n",
    "        f = open(\"../plugins/moving_target/toggle_movement.txt\", \"w\")\n",
    "        f.write(\"0\")\n",
    "        f.close()\n",
    "    \n",
    "    @staticmethod\n",
    "    def move_targets():\n",
    "        f = open(\"../plugins/moving_target/toggle_movement.txt\", \"w\")\n",
    "        f.write(\"1\")\n",
    "        f.close()\n",
    "        \n",
    "    @staticmethod\n",
    "    def reset_targets(release=False):\n",
    "        f = open(\"../plugins/moving_target/reset_position.txt\", \"w\")\n",
    "        if not release:\n",
    "            f.write(\"1\")\n",
    "        else:\n",
    "            f.write(\"0\")\n",
    "        f.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad643179",
   "metadata": {},
   "source": [
    "# Helper classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3dd8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvManager():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def store_state(self, state):\n",
    "        pass\n",
    "    \n",
    "    def store_reward(self, state):\n",
    "        pass\n",
    "    \n",
    "    def get_last_state(self):\n",
    "        pass\n",
    "\n",
    "class FlightListener(olympe.EventListener):\n",
    "\n",
    "    default_queue_size = 100\n",
    "\n",
    "    def __init__(self, *args, **kwds):\n",
    "        super().__init__(*args, **kwds)\n",
    "    \n",
    "    @olympe.listen_event(media_created())\n",
    "    def onMediaCreated(self, event, scheduler):\n",
    "        resource_url = list(event.media.resources.values())[0].url\n",
    "        image_response = requests.get(ANAFI_URL + resource_url, stream=True)\n",
    "        image_response.raise_for_status()\n",
    "        img = Image.open(BytesIO(image_response.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddee8aba",
   "metadata": {},
   "source": [
    "# Define a Gym Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "341caa98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnafiEnv(Env):\n",
    "    def __init__(self, num_targets, max_timestep, drone_ip=\"10.202.0.1\", is_training=False):\n",
    "        super(AnafiEnv, self).__init__()\n",
    "        \n",
    "        Simulation.disable_battery()\n",
    "#         Simulation.cease_targets()\n",
    "        \n",
    "        self.num_targets = num_targets\n",
    "        self.max_timestep = max_timestep\n",
    "        self.begin(num_targets, max_timestep, is_training, drone_ip)\n",
    "        \n",
    "        self.action_space = spaces.Discrete(len(self.agent.action))\n",
    "        self.observation_space = spaces.Box( # {t, cell_id, [I1, I2, I3, ..., In]}\n",
    "            low=np.array([0, 1] + num_targets*[0]), \n",
    "            high=np.array([max_timestep, 25] + num_targets*[1]), \n",
    "            dtype=np.uint8,\n",
    "        )\n",
    "        \n",
    "#         Simulation.move_targets()\n",
    "    \n",
    "    def begin(self, num_targets, max_timestep, is_training, drone_ip=\"10.202.0.1\"):\n",
    "        self.agent = Drone(drone_ip, num_targets, max_timestep, is_training)\n",
    "    \n",
    "    def step(self, action):\n",
    "        obs, reward, done, info = self.agent.take_action(action)\n",
    "        \n",
    "        return obs, reward, done, info\n",
    "    \n",
    "    def reset(self):\n",
    "#         Simulation.reset_targets()\n",
    "        self.agent.target_positions = Simulation.gen_targets_pos(self.num_targets)\n",
    "        return self.agent.reset()\n",
    "    \n",
    "    def render(self, mode='human'):\n",
    "        pass\n",
    "    \n",
    "    def close(self):\n",
    "#         Simulation.cease_targets()\n",
    "        del self.agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626d1b39",
   "metadata": {},
   "source": [
    "# Run the simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc1dbac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def disp_info(action, observation, reward, done, info):\n",
    "#     clear_output(wait=True)\n",
    "    print(\"Action:\", info[\"action\"] + \",\", info[\"direction\"])\n",
    "    print(\"State:\", observation)\n",
    "    print(\"Reward:\", reward)\n",
    "#     down_scale = 3\n",
    "#     display(img.resize((img.size[0]//down_scale, img.size[1]//down_scale)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85b8dee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    del env\n",
    "except Exception as e:\n",
    "    pass\n",
    "\n",
    "env = AnafiEnv(num_targets=10, max_timestep=15, is_training=True)\n",
    "observation = env.reset()\n",
    "actions = [7, 4, 7, 3, 1, 2, 8, 4, 1, 5, 6]\n",
    "for i in range(1000):\n",
    "#     action = env.action_space.sample()\n",
    "    action = actions[i % len(actions)]\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    disp_info(action, observation, reward, done, info)\n",
    "\n",
    "    if done:\n",
    "        print(\"The episode has ended. Resetting environment...\")\n",
    "        observation = env.reset()\n",
    "        disp_info(action, observation, 0, done, info)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df100bc3",
   "metadata": {},
   "source": [
    "# Check the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2800f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = AnafiEnv(num_targets=10, max_timestep=8)\n",
    "check_env(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3a568a",
   "metadata": {},
   "source": [
    "# Train the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19743771",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveOnBestTrainingRewardCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback for saving a model (the check is done every ``check_freq`` steps)\n",
    "    based on the training reward (in practice, we recommend using ``EvalCallback``).\n",
    "\n",
    "    :param check_freq:\n",
    "    :param log_dir: Path to the folder where the model will be saved.\n",
    "      It must contains the file created by the ``Monitor`` wrapper.\n",
    "    :param verbose: Verbosity level.\n",
    "    \"\"\"\n",
    "    def __init__(self, check_freq: int, log_dir: str, verbose: int = 1):\n",
    "        super(SaveOnBestTrainingRewardCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.log_dir = log_dir\n",
    "        self.save_path = os.path.join(log_dir, 'best_model')\n",
    "        self.best_mean_reward = -np.inf\n",
    "\n",
    "    def _init_callback(self) -> None:\n",
    "        pass\n",
    "#         # Create folder if needed\n",
    "#         if self.save_path is not None:\n",
    "#             os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "\n",
    "            # Retrieve training reward\n",
    "            x, y = ts2xy(load_results(self.log_dir), 'timesteps')\n",
    "            if len(x) > 0:\n",
    "                # Mean training reward over the last 100 episodes\n",
    "                mean_reward = np.mean(y[-100:])\n",
    "                if self.verbose > 0:\n",
    "                    print(f\"Num timesteps: {self.num_timesteps}\")\n",
    "                    print(f\"Best mean reward: {self.best_mean_reward:.2f} - Last mean reward per episode: {mean_reward:.2f}\")\n",
    "\n",
    "                # New best model, you could save the agent here\n",
    "                if mean_reward > self.best_mean_reward:\n",
    "                    self.best_mean_reward = mean_reward\n",
    "                    # Example for saving best model\n",
    "                    if self.verbose > 0:\n",
    "                        print(f\"Saving new best model to {self.save_path}\")\n",
    "                    self.model.save(self.save_path)\n",
    "        \n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08ce3ab5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Logging to logs/PPO_1_0\n",
      "Num timesteps: 512\n",
      "Best mean reward: -inf - Last mean reward per episode: -8.97\n",
      "Saving new best model to logs/best_model\n",
      "Num timesteps: 1024\n",
      "Best mean reward: -8.97 - Last mean reward per episode: -10.12\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 14.8     |\n",
      "|    ep_rew_mean     | -10.1    |\n",
      "| time/              |          |\n",
      "|    fps             | 0        |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 2544     |\n",
      "|    total_timesteps | 1024     |\n",
      "---------------------------------\n",
      "Num timesteps: 1536\n",
      "Best mean reward: -8.97 - Last mean reward per episode: -9.88\n",
      "Num timesteps: 2048\n",
      "Best mean reward: -8.97 - Last mean reward per episode: -8.79\n",
      "Saving new best model to logs/best_model\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 14.6        |\n",
      "|    ep_rew_mean          | -8.79       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 0           |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 4916        |\n",
      "|    total_timesteps      | 2048        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013572121 |\n",
      "|    clip_fraction        | 0.165       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.19       |\n",
      "|    explained_variance   | -0.0205     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.38        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0269     |\n",
      "|    value_loss           | 17.7        |\n",
      "-----------------------------------------\n",
      "Num timesteps: 2560\n",
      "Best mean reward: -8.79 - Last mean reward per episode: -8.30\n",
      "Saving new best model to logs/best_model\n",
      "Num timesteps: 3072\n",
      "Best mean reward: -8.30 - Last mean reward per episode: -6.66\n",
      "Saving new best model to logs/best_model\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 14.4       |\n",
      "|    ep_rew_mean          | -6.66      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 0          |\n",
      "|    iterations           | 3          |\n",
      "|    time_elapsed         | 7174       |\n",
      "|    total_timesteps      | 3072       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01568637 |\n",
      "|    clip_fraction        | 0.238      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.15      |\n",
      "|    explained_variance   | 0.0728     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 5.82       |\n",
      "|    n_updates            | 20         |\n",
      "|    policy_gradient_loss | -0.0355    |\n",
      "|    value_loss           | 17         |\n",
      "----------------------------------------\n",
      "Num timesteps: 3584\n",
      "Best mean reward: -6.66 - Last mean reward per episode: -4.57\n",
      "Saving new best model to logs/best_model\n",
      "Num timesteps: 4096\n",
      "Best mean reward: -4.57 - Last mean reward per episode: -2.52\n",
      "Saving new best model to logs/best_model\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 13.9        |\n",
      "|    ep_rew_mean          | -2.52       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 0           |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 9282        |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017624496 |\n",
      "|    clip_fraction        | 0.278       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.08       |\n",
      "|    explained_variance   | 0.132       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.43        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0326     |\n",
      "|    value_loss           | 16.5        |\n",
      "-----------------------------------------\n",
      "Num timesteps: 4608\n",
      "Best mean reward: -2.52 - Last mean reward per episode: -1.32\n",
      "Saving new best model to logs/best_model\n",
      "Num timesteps: 5120\n",
      "Best mean reward: -1.32 - Last mean reward per episode: -1.19\n",
      "Saving new best model to logs/best_model\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 13.7        |\n",
      "|    ep_rew_mean          | -1.19       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 0           |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 11306       |\n",
      "|    total_timesteps      | 5120        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014039763 |\n",
      "|    clip_fraction        | 0.148       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.02       |\n",
      "|    explained_variance   | 0.0166      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 13.7        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0201     |\n",
      "|    value_loss           | 20.1        |\n",
      "-----------------------------------------\n",
      "Num timesteps: 5632\n",
      "Best mean reward: -1.19 - Last mean reward per episode: -0.96\n",
      "Saving new best model to logs/best_model\n",
      "Num timesteps: 6144\n",
      "Best mean reward: -0.96 - Last mean reward per episode: -0.04\n",
      "Saving new best model to logs/best_model\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 13.3        |\n",
      "|    ep_rew_mean          | -0.035      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 0           |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 13179       |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009780471 |\n",
      "|    clip_fraction        | 0.134       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.96       |\n",
      "|    explained_variance   | 0.0413      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.69        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0206     |\n",
      "|    value_loss           | 17          |\n",
      "-----------------------------------------\n",
      "Num timesteps: 6656\n",
      "Best mean reward: -0.04 - Last mean reward per episode: -0.24\n",
      "Num timesteps: 7168\n",
      "Best mean reward: -0.04 - Last mean reward per episode: -0.41\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 13.6        |\n",
      "|    ep_rew_mean          | -0.405      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 0           |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 14998       |\n",
      "|    total_timesteps      | 7168        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013349159 |\n",
      "|    clip_fraction        | 0.1         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.92       |\n",
      "|    explained_variance   | 0.0938      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.88        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0146     |\n",
      "|    value_loss           | 18.3        |\n",
      "-----------------------------------------\n",
      "Num timesteps: 7680\n",
      "Best mean reward: -0.04 - Last mean reward per episode: 0.17\n",
      "Saving new best model to logs/best_model\n",
      "Num timesteps: 8192\n",
      "Best mean reward: 0.17 - Last mean reward per episode: 0.21\n",
      "Saving new best model to logs/best_model\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 13.2       |\n",
      "|    ep_rew_mean          | 0.21       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 0          |\n",
      "|    iterations           | 8          |\n",
      "|    time_elapsed         | 16891      |\n",
      "|    total_timesteps      | 8192       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00819304 |\n",
      "|    clip_fraction        | 0.0652     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.92      |\n",
      "|    explained_variance   | 0.0911     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 8.39       |\n",
      "|    n_updates            | 70         |\n",
      "|    policy_gradient_loss | -0.0147    |\n",
      "|    value_loss           | 16.6       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 8704\n",
      "Best mean reward: 0.21 - Last mean reward per episode: 1.45\n",
      "Saving new best model to logs/best_model\n",
      "Num timesteps: 9216\n",
      "Best mean reward: 1.45 - Last mean reward per episode: 2.67\n",
      "Saving new best model to logs/best_model\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 12.4        |\n",
      "|    ep_rew_mean          | 2.67        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 0           |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 18938       |\n",
      "|    total_timesteps      | 9216        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013835983 |\n",
      "|    clip_fraction        | 0.143       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.89       |\n",
      "|    explained_variance   | 0.0847      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.59        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0186     |\n",
      "|    value_loss           | 17.3        |\n",
      "-----------------------------------------\n",
      "Num timesteps: 9728\n",
      "Best mean reward: 2.67 - Last mean reward per episode: 3.80\n",
      "Saving new best model to logs/best_model\n",
      "Num timesteps: 10240\n",
      "Best mean reward: 3.80 - Last mean reward per episode: 2.96\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 12.6      |\n",
      "|    ep_rew_mean          | 2.96      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 0         |\n",
      "|    iterations           | 10        |\n",
      "|    time_elapsed         | 20778     |\n",
      "|    total_timesteps      | 10240     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0098313 |\n",
      "|    clip_fraction        | 0.12      |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.85     |\n",
      "|    explained_variance   | 0.149     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 11.3      |\n",
      "|    n_updates            | 90        |\n",
      "|    policy_gradient_loss | -0.0149   |\n",
      "|    value_loss           | 19.7      |\n",
      "---------------------------------------\n",
      "Num timesteps: 10752\n",
      "Best mean reward: 3.80 - Last mean reward per episode: 2.65\n",
      "Num timesteps: 11264\n",
      "Best mean reward: 3.80 - Last mean reward per episode: 2.57\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 12.6        |\n",
      "|    ep_rew_mean          | 2.57        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 0           |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 22644       |\n",
      "|    total_timesteps      | 11264       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012602635 |\n",
      "|    clip_fraction        | 0.098       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.83       |\n",
      "|    explained_variance   | 0.238       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.24        |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0135     |\n",
      "|    value_loss           | 15          |\n",
      "-----------------------------------------\n",
      "Num timesteps: 11776\n",
      "Best mean reward: 3.80 - Last mean reward per episode: 3.40\n",
      "Num timesteps: 12288\n",
      "Best mean reward: 3.80 - Last mean reward per episode: 4.89\n",
      "Saving new best model to logs/best_model\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 11.5        |\n",
      "|    ep_rew_mean          | 4.89        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 0           |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 24594       |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008847302 |\n",
      "|    clip_fraction        | 0.113       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.81       |\n",
      "|    explained_variance   | 0.164       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.83        |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0133     |\n",
      "|    value_loss           | 17.9        |\n",
      "-----------------------------------------\n",
      "Num timesteps: 12800\n",
      "Best mean reward: 4.89 - Last mean reward per episode: 4.21\n",
      "Num timesteps: 13312\n",
      "Best mean reward: 4.89 - Last mean reward per episode: 3.50\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 12.4        |\n",
      "|    ep_rew_mean          | 3.5         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 0           |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 26705       |\n",
      "|    total_timesteps      | 13312       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010817323 |\n",
      "|    clip_fraction        | 0.14        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.8        |\n",
      "|    explained_variance   | 0.23        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.73        |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0168     |\n",
      "|    value_loss           | 17.4        |\n",
      "-----------------------------------------\n",
      "Num timesteps: 13824\n",
      "Best mean reward: 4.89 - Last mean reward per episode: 3.08\n",
      "Num timesteps: 14336\n",
      "Best mean reward: 4.89 - Last mean reward per episode: 3.18\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 12.3        |\n",
      "|    ep_rew_mean          | 3.18        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 0           |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 28731       |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008599465 |\n",
      "|    clip_fraction        | 0.0594      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.79       |\n",
      "|    explained_variance   | 0.243       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.36        |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.00891    |\n",
      "|    value_loss           | 15.1        |\n",
      "-----------------------------------------\n",
      "Num timesteps: 14848\n",
      "Best mean reward: 4.89 - Last mean reward per episode: 3.77\n",
      "Num timesteps: 15360\n",
      "Best mean reward: 4.89 - Last mean reward per episode: 4.78\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 11.8        |\n",
      "|    ep_rew_mean          | 4.78        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 0           |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 30911       |\n",
      "|    total_timesteps      | 15360       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010305632 |\n",
      "|    clip_fraction        | 0.0761      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.76       |\n",
      "|    explained_variance   | 0.179       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.49        |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0113     |\n",
      "|    value_loss           | 17          |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-07 15:08:03,012 \u001b[31m[ERROR] \u001b[0m\tulog - pomp - epoll_ctl(fd=73) err=9(Bad file descriptor)\u001b[0m\n",
      "2022-03-07 15:08:03,016 \u001b[31m[ERROR] \u001b[0m\tulog - pomp - epoll_ctl op=2 cb=0x7fc467e194e0 userdata=0x7fc3037edf90\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    del env\n",
    "    del model\n",
    "except Exception as e:\n",
    "    pass\n",
    "\n",
    "# Create log dir\n",
    "run = 2\n",
    "log_dir = \"logs/\"\n",
    "monitor_file = os.path.join(log_dir, str(run))\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "env = AnafiEnv(num_targets=10, max_timestep=15, is_training=True)\n",
    "env = Monitor(env, monitor_file)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "\n",
    "callback = SaveOnBestTrainingRewardCallback(check_freq=512, log_dir=log_dir)\n",
    "\n",
    "# model = PPO(\"MlpPolicy\", env, n_steps=1024, verbose=1, tensorboard_log=log_dir)\n",
    "model = PPO.load(os.path.join(log_dir, str(run-1) + \"_run\"), env)\n",
    "model.learn(total_timesteps=15_000, callback=callback, tb_log_name=\"PPO_\" + str(run), reset_num_timesteps=False)\n",
    "model.save(os.path.join(log_dir, str(run) + \"_run\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1c5ed4",
   "metadata": {},
   "source": [
    "# Continue training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f692ba89",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    del env\n",
    "    del model\n",
    "except Exception as e:\n",
    "    pass\n",
    "\n",
    "# Create log dir\n",
    "log_dir = \"tmp/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "num_targets=10\n",
    "max_timestep=15\n",
    "\n",
    "env = AnafiEnv(num_targets=num_targets, max_timestep=max_timestep)\n",
    "env = Monitor(env, log_dir)\n",
    "\n",
    "callback = SaveOnBestTrainingRewardCallback(check_freq=25, log_dir=log_dir, num_targets=num_targets, max_timestep=max_timestep)\n",
    "\n",
    "checkpoint_callback = CheckpointCallback(save_freq=10, save_path='./logs/')\n",
    "\n",
    "model = PPO.load(\"logs/rl_model_180_steps.zip\", env, verbose=1)\n",
    "model.learn(total_timesteps=150, callback=[callback, checkpoint_callback])\n",
    "model.save(\"best_model.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b27d7fa",
   "metadata": {},
   "source": [
    "# Test the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1be2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del env\n",
    "    del model\n",
    "except Exception as e:\n",
    "    pass\n",
    "\n",
    "env = AnafiEnv(num_targets=10, max_timestep=15)\n",
    "model = PPO.load(\"third_run\", env, verbose=1)\n",
    "\n",
    "obs = env.reset()\n",
    "for i in range(1000):\n",
    "    action, _state = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "\n",
    "    if done:\n",
    "        obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b3d549",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
