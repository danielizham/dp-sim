{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2cfc7f7",
   "metadata": {},
   "source": [
    "# Import all the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b225355f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from enum import IntEnum\n",
    "import time\n",
    "import jsonrpclib\n",
    "import subprocess\n",
    "from subprocess import PIPE, Popen\n",
    "from threading  import Thread\n",
    "import sys\n",
    "import re\n",
    "from collections import OrderedDict\n",
    "\n",
    "import PySimpleGUI as sg\n",
    "\n",
    "from gym import Env, error, spaces, utils\n",
    "from stable_baselines3 import DQN, PPO, A2C, TD3, SAC\n",
    "from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.callbacks import BaseCallback, CallbackList, CheckpointCallback, EvalCallback\n",
    "from stable_baselines3.common import results_plotter\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy, plot_results\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common import results_plotter\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy, plot_results\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import shutil\n",
    "import tempfile\n",
    "import xml.etree.ElementTree as ET\n",
    "from io import StringIO, BytesIO\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from IPython.display import clear_output\n",
    "import gym\n",
    "from cv2 import QRCodeDetector\n",
    "from pyzbar import pyzbar\n",
    "\n",
    "import olympe\n",
    "from olympe.messages.ardrone3.Piloting import TakeOff, Landing, moveBy, PCMD, moveTo\n",
    "from olympe.messages.ardrone3.PilotingState import FlyingStateChanged, PositionChanged, GpsLocationChanged, moveToChanged\n",
    "from olympe.enums.ardrone3.PilotingState import FlyingStateChanged_State as FlyingState\n",
    "from olympe.messages.ardrone3.GPSSettingsState import GPSFixStateChanged, HomeChanged\n",
    "from olympe.messages.gimbal import set_target, attitude\n",
    "from olympe.messages.camera import (\n",
    "    set_camera_mode,\n",
    "    set_photo_mode,\n",
    "    take_photo,\n",
    "    photo_progress,\n",
    ")\n",
    "from olympe.media import (\n",
    "    media_created,\n",
    "    resource_created,\n",
    "    media_removed,\n",
    "    resource_removed,\n",
    "    resource_downloaded,\n",
    "    indexing_state,\n",
    "    delete_media,\n",
    "    download_media,\n",
    "    download_media_thumbnail,\n",
    "    MediaEvent,\n",
    ")\n",
    "\n",
    "from pynput.keyboard import Listener, Key, KeyCode\n",
    "from collections import defaultdict\n",
    "\n",
    "olympe.log.update_config({\n",
    "    \"loggers\": {\n",
    "        \"olympe\": {\n",
    "                \"handlers\": []\n",
    "            }\n",
    "        },\n",
    "        \"ulog\": {\n",
    "            \"level\": \"OFF\",\n",
    "            \"handlers\": [],\n",
    "        }\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0380e41c",
   "metadata": {},
   "source": [
    "# Define the constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee2702da",
   "metadata": {},
   "outputs": [],
   "source": [
    "DRONE_IP = os.environ.get(\"DRONE_IP\", \"10.202.0.1\")\n",
    "DRONE_MEDIA_PORT = os.environ.get(\"DRONE_MEDIA_PORT\", \"80\")\n",
    "\n",
    "ANAFI_URL = \"http://{}/\".format(DRONE_IP)\n",
    "ANAFI_MEDIA_API_URL = ANAFI_URL + \"api/v1/media/medias/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc9719c",
   "metadata": {},
   "source": [
    "# Define the classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f459a37",
   "metadata": {},
   "source": [
    "## Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09ac1cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Action:\n",
    "    def __init__(self, drone):\n",
    "        self.drone = drone\n",
    "        self.home = self.drone.get_state(GpsLocationChanged)\n",
    "        \n",
    "        self.current_cell = self._get_cell(13)\n",
    "        self.invalid_left_cells = [1, 6, 11, 16, 21]\n",
    "        self.invalid_forward_cells = [1, 2, 3, 4, 5]\n",
    "        self.invalid_right_cells = [5, 10, 15, 20, 25]\n",
    "        self.invalid_backward_cells = [21, 22, 23, 24, 25]\n",
    "        \n",
    "        self.Move = IntEnum(\n",
    "            'MOVE',\n",
    "            'FORWARD BACKWARD LEFT RIGHT FORWARD_LEFT FORWARD_RIGHT BACKWARD_LEFT BACKWARD_RIGHT HOVER',\n",
    "            start=0\n",
    "        )\n",
    "        \n",
    "    def take_action(self, action):\n",
    "        next_cell_id = self._get_next_cell_id(action)\n",
    "        next_cell = self._get_cell(next_cell_id)\n",
    "        \n",
    "        old_cell_id, new_cell_id = self.current_cell[\"id\"], next_cell[\"id\"]\n",
    "        if old_cell_id == new_cell_id: \n",
    "            return old_cell_id, new_cell_id, self._get_action_name(action)\n",
    "        \n",
    "        self._move_to_cell(next_cell)\n",
    "        \n",
    "        self.current_cell = next_cell\n",
    "        \n",
    "        return old_cell_id, new_cell_id, self._get_action_name(action)\n",
    "        \n",
    "    def reset(self):\n",
    "        next_cell = self._get_cell(13)\n",
    "        self._move_to_cell(next_cell)\n",
    "        \n",
    "        old_cell_id, new_cell_id = self.current_cell[\"id\"], next_cell[\"id\"]\n",
    "        self.current_cell = next_cell\n",
    "        \n",
    "        return old_cell_id, new_cell_id\n",
    "    \n",
    "    def _get_cell(self, cell_id):\n",
    "        return self._cell_coords[cell_id - 1]\n",
    "    \n",
    "    def _get_action_name(self, action):\n",
    "        direction = str(self.Move(action)).split(\".\")[1]\n",
    "        code = \"\"\n",
    "        for i in direction.split(\"_\"):\n",
    "            if i != \"\":\n",
    "                code += i[0].upper()\n",
    "        \n",
    "        return code\n",
    "#         direction = str(self.Move(action)).split(\".\")[1].capitalize()\n",
    "#         return \"Moving \" + direction if \"hover\" not in direction.lower() else \"Hovering\"\n",
    "    \n",
    "    def _move_to_cell(self, next_cell):        \n",
    "        self.drone(\n",
    "            moveTo(next_cell[\"latitude\"],  next_cell[\"longitude\"], next_cell[\"altitude\"], \"HEADING_DURING\", 90.0)\n",
    "            >> moveToChanged(status=\"DONE\", _timeout=15)\n",
    "        ).wait()\n",
    "    \n",
    "    def _get_next_cell_id(self, action):\n",
    "        if action == self.Move.HOVER:\n",
    "            return self.current_cell[\"id\"]\n",
    "        elif action == self.Move.LEFT:\n",
    "            if self.current_cell[\"id\"] in self.invalid_left_cells:\n",
    "                return self.current_cell[\"id\"]\n",
    "            next_cell_id = self.current_cell[\"id\"] - 1\n",
    "        elif action == self.Move.RIGHT:\n",
    "            if self.current_cell[\"id\"] in self.invalid_right_cells:\n",
    "                return self.current_cell[\"id\"]\n",
    "            next_cell_id = self.current_cell[\"id\"] + 1\n",
    "        elif action == self.Move.FORWARD:\n",
    "            if self.current_cell[\"id\"] in self.invalid_forward_cells:\n",
    "                return self.current_cell[\"id\"]\n",
    "            next_cell_id = self.current_cell[\"id\"] - 5\n",
    "        elif action == self.Move.BACKWARD:\n",
    "            if self.current_cell[\"id\"] in self.invalid_backward_cells:\n",
    "                return self.current_cell[\"id\"]\n",
    "            next_cell_id = self.current_cell[\"id\"] + 5\n",
    "        elif action == self.Move.FORWARD_RIGHT:\n",
    "            if self.current_cell[\"id\"] in self.invalid_forward_cells + self.invalid_right_cells:\n",
    "                return self.current_cell[\"id\"]\n",
    "            next_cell_id = self.current_cell[\"id\"] - 4\n",
    "        elif action == self.Move.FORWARD_LEFT:\n",
    "            if self.current_cell[\"id\"] in self.invalid_forward_cells + self.invalid_left_cells:\n",
    "                return self.current_cell[\"id\"]\n",
    "            next_cell_id = self.current_cell[\"id\"] - 6\n",
    "        elif action == self.Move.BACKWARD_RIGHT:\n",
    "            if self.current_cell[\"id\"] in self.invalid_backward_cells + self.invalid_right_cells:\n",
    "                return self.current_cell[\"id\"]\n",
    "            next_cell_id = self.current_cell[\"id\"] + 6\n",
    "        elif action == self.Move.BACKWARD_LEFT:\n",
    "            if self.current_cell[\"id\"] in self.invalid_backward_cells + self.invalid_left_cells:\n",
    "                return self.current_cell[\"id\"]\n",
    "            next_cell_id = self.current_cell[\"id\"] + 4\n",
    "            \n",
    "        return next_cell_id\n",
    "    \n",
    "    @property\n",
    "    def _cell_coords(self):\n",
    "        altitude = 6.0\n",
    "        dlong = 6.8e-5 # in degrees == 5 meters along x-axis (forward[+]-backward[-])\n",
    "        dlat = 7.2e-5 # in degrees == 8 meters along y-axis (left[+]-right[-])\n",
    "        \n",
    "        home_lat = self.home[\"latitude\"]\n",
    "        home_long = self.home[\"longitude\"]\n",
    "        \n",
    "        return [\n",
    "            # cell no. 1\n",
    "            OrderedDict([('id', 1),\n",
    "                         ('latitude', home_lat + 2 * dlat),\n",
    "                         ('longitude', home_long + 2 * dlong),\n",
    "                         ('altitude', altitude)]),\n",
    "            # cell no. 2\n",
    "            OrderedDict([('id', 2),\n",
    "                         ('latitude', home_lat + 1 * dlat),\n",
    "                         ('longitude', home_long + 2 * dlong),\n",
    "                         ('altitude', altitude)]),\n",
    "            # cell no. 3\n",
    "            OrderedDict([('id', 3),\n",
    "                         ('latitude', home_lat + 0 * dlat),\n",
    "                         ('longitude', home_long + 2 * dlong),\n",
    "                         ('altitude', altitude)]),\n",
    "            # cell no. 4\n",
    "            OrderedDict([('id', 4),\n",
    "                         ('latitude', home_lat + -1 * dlat),\n",
    "                         ('longitude', home_long + 2 * dlong),\n",
    "                         ('altitude', altitude)]),\n",
    "            # cell no. 5\n",
    "            OrderedDict([('id', 5),\n",
    "                         ('latitude', home_lat + -2 * dlat),\n",
    "                         ('longitude', home_long + 2 * dlong),\n",
    "                         ('altitude', altitude)]),\n",
    "            # cell no. 6\n",
    "            OrderedDict([('id', 6),\n",
    "                         ('latitude', home_lat + 2 * dlat),\n",
    "                         ('longitude', home_long + 1 * dlong),\n",
    "                         ('altitude', altitude)]),\n",
    "            # cell no. 7\n",
    "            OrderedDict([('id', 7),\n",
    "                         ('latitude', home_lat + 1 * dlat),\n",
    "                         ('longitude', home_long + 1 * dlong),\n",
    "                         ('altitude', altitude)]),\n",
    "            # cell no. 8\n",
    "            OrderedDict([('id', 8),\n",
    "                         ('latitude', home_lat + 0 * dlat),\n",
    "                         ('longitude', home_long + 1 * dlong),\n",
    "                         ('altitude', altitude)]),\n",
    "            # cell no. 9\n",
    "            OrderedDict([('id', 9),\n",
    "                         ('latitude', home_lat + -1 * dlat),\n",
    "                         ('longitude', home_long + 1 * dlong),\n",
    "                         ('altitude', altitude)]),\n",
    "            # cell no. 10\n",
    "            OrderedDict([('id', 10),\n",
    "                         ('latitude', home_lat + -2 * dlat),\n",
    "                         ('longitude', home_long + 1 * dlong),\n",
    "                         ('altitude', altitude)]),\n",
    "            # cell no. 11\n",
    "            OrderedDict([('id', 11),\n",
    "                         ('latitude', home_lat + 2 * dlat),\n",
    "                         ('longitude', home_long + 0 * dlong),\n",
    "                         ('altitude', altitude)]),\n",
    "            # cell no. 12\n",
    "            OrderedDict([('id', 12),\n",
    "                         ('latitude', home_lat + 1 * dlat),\n",
    "                         ('longitude', home_long + 0 * dlong),\n",
    "                         ('altitude', altitude)]),\n",
    "            # cell no. 13\n",
    "            OrderedDict([('id', 13),\n",
    "                         ('latitude', home_lat + 0 * dlat),\n",
    "                         ('longitude', home_long + 0 * dlong),\n",
    "                         ('altitude', altitude)]),\n",
    "            # cell no. 14\n",
    "            OrderedDict([('id', 14),\n",
    "                         ('latitude', home_lat + -1 * dlat),\n",
    "                         ('longitude', home_long + 0 * dlong),\n",
    "                         ('altitude', altitude)]),\n",
    "            # cell no. 15\n",
    "            OrderedDict([('id', 15),\n",
    "                         ('latitude', home_lat + -2 * dlat),\n",
    "                         ('longitude', home_long + 0 * dlong),\n",
    "                         ('altitude', altitude)]),\n",
    "            # cell no. 16\n",
    "            OrderedDict([('id', 16),\n",
    "                         ('latitude', home_lat + 2 * dlat),\n",
    "                         ('longitude', home_long + -1 * dlong),\n",
    "                         ('altitude', altitude)]),\n",
    "            # cell no. 17\n",
    "            OrderedDict([('id', 17),\n",
    "                         ('latitude', home_lat + 1 * dlat),\n",
    "                         ('longitude', home_long + -1 * dlong),\n",
    "                         ('altitude', altitude)]),\n",
    "            # cell no. 18\n",
    "            OrderedDict([('id', 18),\n",
    "                         ('latitude', home_lat + 0 * dlat),\n",
    "                         ('longitude', home_long + -1 * dlong),\n",
    "                         ('altitude', altitude)]),\n",
    "            # cell no. 19\n",
    "            OrderedDict([('id', 19),\n",
    "                         ('latitude', home_lat + -1 * dlat),\n",
    "                         ('longitude', home_long + -1 * dlong),\n",
    "                         ('altitude', altitude)]),\n",
    "            # cell no. 20\n",
    "            OrderedDict([('id', 20),\n",
    "                         ('latitude', home_lat + -2 * dlat),\n",
    "                         ('longitude', home_long + -1 * dlong),\n",
    "                         ('altitude', altitude)]),\n",
    "            # cell no. 21\n",
    "            OrderedDict([('id', 21),\n",
    "                         ('latitude', home_lat + 2 * dlat),\n",
    "                         ('longitude', home_long + -2 * dlong),\n",
    "                         ('altitude', altitude)]),\n",
    "            # cell no. 22\n",
    "            OrderedDict([('id', 22),\n",
    "                         ('latitude', home_lat + 1 * dlat),\n",
    "                         ('longitude', home_long + -2 * dlong),\n",
    "                         ('altitude', altitude)]),\n",
    "            # cell no. 23\n",
    "            OrderedDict([('id', 23),\n",
    "                         ('latitude', home_lat + 0 * dlat),\n",
    "                         ('longitude', home_long + -2 * dlong),\n",
    "                         ('altitude', altitude)]),\n",
    "            # cell no. 24\n",
    "            OrderedDict([('id', 24),\n",
    "                         ('latitude', home_lat + -1 * dlat),\n",
    "                         ('longitude', home_long + -2 * dlong),\n",
    "                         ('altitude', altitude)]),\n",
    "            # cell no. 25\n",
    "            OrderedDict([('id', 25),\n",
    "                         ('latitude', home_lat + -2 * dlat),\n",
    "                         ('longitude', home_long + -2 * dlong),\n",
    "                         ('altitude', altitude)]),\n",
    "            ]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.Move)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c218a6a5",
   "metadata": {},
   "source": [
    "## Drone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "154b880f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Drone:\n",
    "    def __init__(self, drone_ip, num_targets, max_timestep, is_training=False):\n",
    "        self.drone = olympe.Drone(drone_ip)\n",
    "        self.drone.connect()\n",
    "        \n",
    "        self.drone(GPSFixStateChanged(_policy = 'wait'))\n",
    "        self._takeoff()\n",
    "        if not is_training: self._setup_camera()\n",
    "\n",
    "        self.action = Action(self.drone)\n",
    "        \n",
    "        self.is_training = is_training\n",
    "        self.num_targets = num_targets\n",
    "        self.max_timestep = max_timestep\n",
    "        self.timestep = 0\n",
    "        self.visited_targets = np.zeros(self.num_targets, dtype=bool)\n",
    "        self.target_positions = np.zeros(self.num_targets, dtype=np.uint8)\n",
    "    \n",
    "    def take_action(self, action):\n",
    "        old_cell, new_cell, action_name = self.action.take_action(action)\n",
    "        self.timestep += 1\n",
    "        detected_targets = self._detect_targets(new_cell)\n",
    "        \n",
    "        reward = self._get_reward(detected_targets) # !!! _get_reward must come before self.visited_targets is changed in _get_state\n",
    "        state = self._get_state(new_cell, detected_targets) \n",
    "        if self.timestep >= self.max_timestep or np.all(self.visited_targets):\n",
    "            done = True\n",
    "            self.visited_targets[:] = False\n",
    "        else:\n",
    "            done = False\n",
    "        info = {\n",
    "            \"action\": str(action_name), \n",
    "            \"direction\": \"Cell \" + str(old_cell) + \" --> \" + \"Cell \" + str(new_cell)\n",
    "        }\n",
    "        \n",
    "        return state, reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        old_cell, new_cell = self.action.reset()\n",
    "        detected_targets = self._detect_targets(new_cell)\n",
    "        self.timestep = 0\n",
    "        return self._get_state(new_cell, detected_targets)\n",
    "    \n",
    "    def _get_state(self, new_cell, detected_targets):\n",
    "        # {t, cell_id, [I1, I2, I3, ..., In]}\n",
    "        \n",
    "        self.visited_targets[detected_targets] = True\n",
    "        \n",
    "        return np.concatenate(([self.timestep, new_cell], self.visited_targets)).astype(np.uint8)\n",
    "    \n",
    "    def _get_reward(self, detected_targets):\n",
    "        reward_scale = 1.5\n",
    "        num_new_targets = np.count_nonzero(\n",
    "            detected_targets & (detected_targets != self.visited_targets)\n",
    "        )\n",
    "        return reward_scale * num_new_targets if num_new_targets > 0 else -1\n",
    "        \n",
    "    def _detect_targets(self, cell_id):\n",
    "        return self.target_positions == cell_id\n",
    "        \n",
    "        if self.is_training:\n",
    "            positions = np.genfromtxt('target_positions.csv', delimiter=',', skip_header=1, dtype=np.uint8)\n",
    "            return positions[:,1] == cell_id\n",
    "        \n",
    "        detected_targets = np.zeros(self.num_targets, dtype=bool)\n",
    "\n",
    "        img = self._take_photo()\n",
    "        if img is None:\n",
    "            return detected_targets\n",
    "        \n",
    "        for result in pyzbar.decode(img):\n",
    "            idx = int(result.data) - 1\n",
    "            try:\n",
    "                detected_targets[idx] = True\n",
    "            except (ValueError, IndexError):\n",
    "                pass\n",
    "                \n",
    "        return detected_targets\n",
    "\n",
    "    def _setup_camera(self):\n",
    "        assert self.drone.media_autoconnect\n",
    "        self.drone.media.integrity_check = True\n",
    "        is_indexed = False\n",
    "        while not is_indexed:\n",
    "            is_indexed = self.drone.media(\n",
    "                indexing_state(state=\"indexed\")\n",
    "            ).wait(_timeout=5).success()\n",
    "        \n",
    "        self.drone(set_camera_mode(cam_id=0, value=\"photo\")).wait()\n",
    "\n",
    "        assert self.drone(\n",
    "            set_photo_mode(\n",
    "                cam_id=0,\n",
    "                mode=\"single\",\n",
    "                format= \"rectilinear\",\n",
    "                file_format=\"jpeg\",\n",
    "                # the following are ignored in photo single mode\n",
    "                burst=\"burst_14_over_1s\",\n",
    "                bracketing=\"preset_1ev\",\n",
    "                capture_interval=5.0,\n",
    "            )\n",
    "        ).wait().success()\n",
    "\n",
    "        assert self.drone(\n",
    "            set_target(\n",
    "                gimbal_id=0,\n",
    "                control_mode=\"position\",\n",
    "                yaw_frame_of_reference=\"none\",\n",
    "                yaw=0.0,\n",
    "                pitch_frame_of_reference=\"absolute\",\n",
    "                pitch=-90.0,\n",
    "                roll_frame_of_reference=\"none\",\n",
    "                roll=0.0,\n",
    "                )\n",
    "            >> attitude(\n",
    "                pitch_absolute=-90.0, _policy=\"wait\", _float_tol=(1e-3, 1e-1)\n",
    "                )\n",
    "            ).wait(_timeout=20).success()\n",
    "    \n",
    "    def _take_photo(self):\n",
    "        photo_saved = self.drone(photo_progress(result=\"photo_saved\", _policy=\"wait\"))\n",
    "        self.drone(take_photo(cam_id=0)).wait()\n",
    "        \n",
    "        photo_taken = False\n",
    "        tries = 0\n",
    "        while not photo_taken:\n",
    "            tries += 1\n",
    "            if tries > 3:\n",
    "#                 assert False, \"take_photo timedout\"\n",
    "                print(\"take_photo timedout\")\n",
    "                return None\n",
    "            photo_taken = photo_saved.wait(_timeout=5).success()\n",
    "            \n",
    "        # get the bytes of the image\n",
    "        media_id = photo_saved.received_events().last().args[\"media_id\"]\n",
    "        for _ in range(5):\n",
    "            media_info_response = requests.get(ANAFI_MEDIA_API_URL + media_id, timeout=10)\n",
    "            if media_info_response.status_code == 200:\n",
    "                break\n",
    "        try:\n",
    "            media_info_response.raise_for_status()\n",
    "        except requests.exceptions.HTTPError as err:\n",
    "            print(err)\n",
    "            return None\n",
    "        \n",
    "        resource = media_info_response.json()[\"resources\"][0]\n",
    "        image_response = requests.get(ANAFI_URL + resource[\"url\"], stream=True)\n",
    "        image_response.raise_for_status()\n",
    "        \n",
    "        img = Image.open(BytesIO(image_response.content))\n",
    "        \n",
    "        # delete the image stored on the drone\n",
    "        photo_deleted = False\n",
    "        delete_tries = 0\n",
    "        while not photo_deleted:\n",
    "            delete_tries += 1\n",
    "            if delete_tries > 3:\n",
    "#                 assert False, \"Failed to delete media {} {}\".format(media_id, delete.explain())\n",
    "                print(\"Failed to delete media {} {}\".format(media_id, delete.explain()))\n",
    "                break\n",
    "            delete = delete_media(media_id, _timeout=10)\n",
    "            photo_deleted = self.drone.media(delete).wait().success()\n",
    "        \n",
    "        return img\n",
    "    \n",
    "    def _takeoff(self):\n",
    "        takeoff_success = self._success_if_takeoff()\n",
    "        if not takeoff_success:\n",
    "            print(\"Retrying taking off...\")\n",
    "            takeoff_success = self._success_if_takeoff()\n",
    "    \n",
    "    def _success_if_takeoff(self):\n",
    "        return self.drone(\n",
    "                FlyingStateChanged(state=\"hovering\")\n",
    "                | (TakeOff() & FlyingStateChanged(state=\"hovering\"))\n",
    "            ).wait(10).success()\n",
    "        \n",
    "    def _land(self):\n",
    "        self.drone(PCMD(1, 0, 0, 0, 0, 0) >> FlyingStateChanged(state=\"hovering\", _timeout=5)).wait()\n",
    "        assert self.drone(Landing() >> FlyingStateChanged(state=\"landed\")).wait().success()\n",
    "        \n",
    "    def __del__(self):\n",
    "        self._land()\n",
    "        self.drone.disconnect()\n",
    "        del state\n",
    "        del reward\n",
    "        del action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d67d9ba",
   "metadata": {},
   "source": [
    "## Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf60baab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_distance(x, y):\n",
    "    x = x - y[:,np.newaxis]\n",
    "    x = x**2\n",
    "    x = x.sum(axis=2)\n",
    "    x = np.sqrt(x)\n",
    "    return x\n",
    "\n",
    "# check if the target overlaps with \n",
    "# another target or border\n",
    "def is_overlapping(x, y=None, limit=0.5):\n",
    "    # check with another target\n",
    "    if y is None: y = x\n",
    "    x = find_distance(x, y) < limit\n",
    "    x = np.triu(x, k=1)\n",
    "    \n",
    "    # check with the borders\n",
    "    borders_parallel_y = np.array([12.5, 7.5, 2.5, -2.5, -7.5, -12.5])\n",
    "    x1 = abs(x[:,0] - borders_parallel_y[:,np.newaxis]) < 0.4\n",
    "\n",
    "    borders_parallel_x = np.array([20, 12, 4, -4, -12, -20])\n",
    "    x2 = abs(x[:,1] - borders_parallel_x[:,np.newaxis]) < 0.4\n",
    "    \n",
    "    x = np.vstack((x, x1, x2))\n",
    "    x = np.any(x, axis=0)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92ec48a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_targets=10\n",
    "mu_x=-7.5 \n",
    "variance_x=3\n",
    "mu_y=-11\n",
    "variance_y=5\n",
    "\n",
    "distribution = multivariate_normal([mu_x, mu_y], [[variance_x, 0], [0, variance_y]])\n",
    "locs = distribution.rvs(size=num_targets)\n",
    "\n",
    "overlapping_targets = is_overlapping(locs)\n",
    "while np.any(overlapping_targets):\n",
    "    num_overlaps = overlapping_targets[overlapping_targets].size\n",
    "    locs[overlapping_targets] = distribution.rvs(size=num_overlaps)\n",
    "    overlapping_targets = is_overlapping(locs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "b02bbbec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_overlaps = overlapping_targets[overlapping_targets].size\n",
    "num_overlaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "ca30d7ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -8.89833982,  -9.2297941 ],\n",
       "       [ -6.14338843,  -7.05054842],\n",
       "       [ -9.39296298, -10.29139529],\n",
       "       [ -2.49084933, -12.58398566],\n",
       "       [ -7.80841139, -11.54652836],\n",
       "       [ -6.34247422, -13.69616478],\n",
       "       [ -9.47697081, -13.13073038],\n",
       "       [ -6.87047567,  -6.11557153],\n",
       "       [ -8.49369027, -16.85767925],\n",
       "       [-10.40676275, -16.74391154]])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "341ba28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "locs[overlapping_targets] = distribution.rvs(size=num_overlaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "f6b0bef5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -8.89833982,  -9.2297941 ],\n",
       "       [ -6.14338843,  -7.05054842],\n",
       "       [ -9.42833748, -16.50998974],\n",
       "       [ -2.49084933, -12.58398566],\n",
       "       [ -5.75448539, -11.38370156],\n",
       "       [ -6.34247422, -13.69616478],\n",
       "       [ -6.54538078,  -9.91008558],\n",
       "       [ -6.26700056,  -7.63354395],\n",
       "       [ -8.49369027, -16.85767925],\n",
       "       [-10.40676275, -16.74391154]])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "3182cb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "overlapping_targets = is_overlapping(locs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d3731303",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False,  True, False,  True, False,  True,  True, False,\n",
       "       False])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overlapping_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b6bde50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simulation:\n",
    "    sphinx = jsonrpclib.Server('http://127.0.0.1:8383')\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    @staticmethod\n",
    "    def disable_battery():\n",
    "        Simulation.sphinx.SetParam(machine='anafi4k',\n",
    "                                   object='lipobattery/lipobattery',\n",
    "                                   parameter='discharge_speed_factor',\n",
    "                                   value='0')\n",
    "    \n",
    "    @staticmethod\n",
    "    def reset_world():\n",
    "        Simulation.sphinx.TriggerAction(machine='world',\n",
    "                                        object='fwman/fwman',\n",
    "                                        action='world_reset_all')\n",
    "    \n",
    "    def find_distance(x, y):\n",
    "        x = x - y[:,np.newaxis]\n",
    "        x = x**2\n",
    "        x = x.sum(axis=2)\n",
    "        x = np.sqrt(x)\n",
    "        return x\n",
    "\n",
    "    # check if the target overlaps with another target or border\n",
    "    @classmethod\n",
    "    def is_overlapping(cls, x, y=None, limit=0.8):\n",
    "        # check with the borders\n",
    "        borders_parallel_y = np.array([12.5, 7.5, 2.5, -2.5, -7.5, -12.5])\n",
    "        x1 = abs(x[:,0] - borders_parallel_y[:,np.newaxis]) < 0.4\n",
    "\n",
    "        borders_parallel_x = np.array([20, 12, 4, -4, -12, -20])\n",
    "        x2 = abs(x[:,1] - borders_parallel_x[:,np.newaxis]) < 0.4\n",
    "        \n",
    "        # check with another target\n",
    "        if y is None: y = x\n",
    "        x = cls.find_distance(x, y) < limit\n",
    "        x = np.triu(x, k=1)\n",
    "\n",
    "        x = np.vstack((x, x1, x2))\n",
    "        x = np.any(x, axis=0)\n",
    "        return x\n",
    "    \n",
    "    @classmethod\n",
    "    def gen_targets_pos(cls, num_targets, mu_x=-7.5, variance_x=3, mu_y=-11, variance_y=5):\n",
    "#         locs = multivariate_normal([mu_x, mu_y], [[variance_x, 0], [0, variance_y]]).rvs(size=num_targets)\n",
    "        \n",
    "        distribution = multivariate_normal([mu_x, mu_y], [[variance_x, 0], [0, variance_y]])\n",
    "        locs = distribution.rvs(size=num_targets)\n",
    "\n",
    "        overlapping_targets = cls.is_overlapping(locs)\n",
    "        while np.any(overlapping_targets):\n",
    "            num_overlaps = overlapping_targets[overlapping_targets].size\n",
    "            locs[overlapping_targets] = distribution.rvs(size=num_overlaps)\n",
    "            overlapping_targets = cls.is_overlapping(locs)\n",
    "        \n",
    "        np.savetxt('../comm/positions.csv', \n",
    "                   np.hstack((np.arange(len(locs))[:,np.newaxis] + 1, locs)),\n",
    "                   fmt='%.4f',\n",
    "                   delimiter=',',\n",
    "                  )\n",
    "        \n",
    "        cell_boundaries = [\n",
    "           (locs[:,0] > 7.5) & (locs[:,1] > 12),\n",
    "           (locs[:,0] > 7.5) & (locs[:,1] > 4),\n",
    "           (locs[:,0] > 7.5) & (locs[:,1] > -4),\n",
    "           (locs[:,0] > 7.5) & (locs[:,1] > -12),\n",
    "           (locs[:,0] > 7.5) & (locs[:,1] > -20),\n",
    "           (locs[:,0] > 2.5) & (locs[:,1] > 12),\n",
    "           (locs[:,0] > 2.5) & (locs[:,1] > 4),\n",
    "           (locs[:,0] > 2.5) & (locs[:,1] > -4),\n",
    "           (locs[:,0] > 2.5) & (locs[:,1] > -12),\n",
    "           (locs[:,0] > 2.5) & (locs[:,1] > -20),\n",
    "           (locs[:,0] > -2.5) & (locs[:,1] > 12),\n",
    "           (locs[:,0] > -2.5) & (locs[:,1] > 4),\n",
    "           (locs[:,0] > -2.5) & (locs[:,1] > -4),\n",
    "           (locs[:,0] > -2.5) & (locs[:,1] > -12),\n",
    "           (locs[:,0] > -2.5) & (locs[:,1] > -20),\n",
    "           (locs[:,0] > -7.5) & (locs[:,1] > 12),\n",
    "           (locs[:,0] > -7.5) & (locs[:,1] > 4),\n",
    "           (locs[:,0] > -7.5) & (locs[:,1] > -4),\n",
    "           (locs[:,0] > -7.5) & (locs[:,1] > -12),\n",
    "           (locs[:,0] > -7.5) & (locs[:,1] > -20),\n",
    "           (locs[:,0] > -12.5) & (locs[:,1] > 12),\n",
    "           (locs[:,0] > -12.5) & (locs[:,1] > 4),\n",
    "           (locs[:,0] > -12.5) & (locs[:,1] > -4),\n",
    "           (locs[:,0] > -12.5) & (locs[:,1] > -12),\n",
    "           (locs[:,0] > -12.5) & (locs[:,1] > -20),\n",
    "        ]\n",
    "        cell_ids = np.arange(25) + 1\n",
    "        \n",
    "        return np.select(cell_boundaries, cell_ids), locs\n",
    "#         return locs\n",
    "    \n",
    "    @staticmethod\n",
    "    def cease_targets():\n",
    "        f = open(\"../comm/toggle_movement.txt\", \"w\")\n",
    "        f.write(\"0\")\n",
    "        f.close()\n",
    "    \n",
    "    @staticmethod\n",
    "    def move_targets():\n",
    "        f = open(\"../comm/toggle_movement.txt\", \"w\")\n",
    "        f.write(\"1\")\n",
    "        f.close()\n",
    "        \n",
    "    @staticmethod\n",
    "    def reset_targets(release=False):\n",
    "        fmobile = open(\"../comm/reset_position.txt\", \"w\")\n",
    "#         fstatic = open(\"../plugins/static_target/reset_position.txt\", \"w\")\n",
    "        if not release:\n",
    "            fmobile.write(\"1\\n\")\n",
    "#             fstatic.write(\"1\\n\")\n",
    "        else:\n",
    "            fmobile.write(\"0\\n\")\n",
    "#             fstatic.write(\"0\\n\")\n",
    "        fmobile.close()\n",
    "#         fstatic.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c5b42f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-10.46458845, -10.01633378],\n",
       "       [ -6.24402812, -10.54155127],\n",
       "       [ -6.08411325,  -6.51080688],\n",
       "       [ -8.804801  , -11.15770231],\n",
       "       [ -8.34083635,  -9.8827977 ],\n",
       "       [ -5.9115347 ,  -9.73495238],\n",
       "       [ -6.67194236, -14.80068425],\n",
       "       [-11.51665774, -12.71224588],\n",
       "       [ -6.25879009,  -8.87102503],\n",
       "       [ -9.48866335, -12.54896677]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Simulation.gen_targets_pos(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "549f170b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Simulation.reset_targets(release=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "095c84a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Simulation.reset_targets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "10342aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "Simulation.move_targets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "779876bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Simulation.cease_targets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "abdb293b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for _ in range(5):\n",
    "cells, locs = Simulation.gen_targets_pos(10)\n",
    "Simulation.reset_targets()\n",
    "time.sleep(0.1)\n",
    "Simulation.reset_targets(release=True)\n",
    "#     time.sleep(1)\n",
    "# for _ in range(10):\n",
    "#     print(sorted(Simulation.gen_targets_pos(10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "3488a448",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3211650600000002"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "7.5-7.17883494"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "55584d93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -6.53392003,  -9.38427206],\n",
       "       [ -8.08757585, -12.59822305],\n",
       "       [ -9.04958566, -12.05622549],\n",
       "       [ -6.71018167,  -9.60039297],\n",
       "       [ -7.8884485 ,  -9.59102998],\n",
       "       [ -9.29949368, -13.57839219],\n",
       "       [ -7.84492359, -13.40426512],\n",
       "       [ -7.17883494, -12.22309897],\n",
       "       [ -8.51418777, -10.8921918 ],\n",
       "       [ -7.07184698, -11.1778088 ]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37572f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_A = np.array([\n",
    "    [ -6.53392003,  -9.38427206],\n",
    "    [ -8.08757585, -12.59822305],\n",
    "    [ -9.04958566, -12.05622549],\n",
    "    [ -6.71018167,  -9.60039297],\n",
    "    [ -7.8884485 ,  -9.59102998],\n",
    "    [ -9.29949368, -13.57839219],\n",
    "    [ -7.84492359, -13.40426512],\n",
    "    [ -7.17883494, -12.22309897],\n",
    "    [ -8.51418777, -10.8921918 ],\n",
    "    [ -7.07184698, -11.1778088 ]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5aada06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e08d1a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = np.linalg.norm\n",
    "\n",
    "p1 = np.array([-7.5, 4])\n",
    "p2 = np.array([-7.5, -20])\n",
    "\n",
    "p3 = np.array([-7.8884485, -9.59102998])\n",
    "d = np.abs(norm(np.cross(p2-p1, p1-p3)))/norm(p2-p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0da97cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_parallel_borders = np.array(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "828438b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_parallel_borders = np.array([\n",
    "    [12.5, 7.5, 2.5, -2.5, -7.5, -12.5]\n",
    "])\n",
    "\n",
    "x_parallel_borders = np.array([\n",
    "    [20, 12, 4, -4, -12, -20]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "20c85cff",
   "metadata": {},
   "outputs": [
    {
     "ename": "AxisError",
     "evalue": "axis 2 is out of bounds for array of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [33]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mfind_distance\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_parallel_borders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [31]\u001b[0m, in \u001b[0;36mfind_distance\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m      2\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m-\u001b[39m y[:,np\u001b[38;5;241m.\u001b[39mnewaxis]\n\u001b[1;32m      3\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[0;32m----> 4\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(x)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(x)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/envs/dp-sim/lib/python3.8/site-packages/numpy/core/_methods.py:48\u001b[0m, in \u001b[0;36m_sum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sum\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     47\u001b[0m          initial\u001b[38;5;241m=\u001b[39m_NoValue, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m---> 48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mumr_sum\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mAxisError\u001b[0m: axis 2 is out of bounds for array of dimension 2"
     ]
    }
   ],
   "source": [
    "find_distance(y_parallel_borders, test[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9ac8853b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 12.5,   7.5,   2.5,  -2.5,  -7.5, -12.5])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_parallel_borders.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0413248b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[19.03392003, 20.58757585, 21.54958566, 19.21018167, 20.3884485 ,\n",
       "        21.79949368, 20.34492359, 19.67883494, 21.01418777, 19.57184698],\n",
       "       [14.03392003, 15.58757585, 16.54958566, 14.21018167, 15.3884485 ,\n",
       "        16.79949368, 15.34492359, 14.67883494, 16.01418777, 14.57184698],\n",
       "       [ 9.03392003, 10.58757585, 11.54958566,  9.21018167, 10.3884485 ,\n",
       "        11.79949368, 10.34492359,  9.67883494, 11.01418777,  9.57184698],\n",
       "       [ 4.03392003,  5.58757585,  6.54958566,  4.21018167,  5.3884485 ,\n",
       "         6.79949368,  5.34492359,  4.67883494,  6.01418777,  4.57184698],\n",
       "       [ 0.96607997,  0.58757585,  1.54958566,  0.78981833,  0.3884485 ,\n",
       "         1.79949368,  0.34492359,  0.32116506,  1.01418777,  0.42815302],\n",
       "       [ 5.96607997,  4.41242415,  3.45041434,  5.78981833,  4.6115515 ,\n",
       "         3.20050632,  4.65507641,  5.32116506,  3.98581223,  5.42815302]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs(test[:,0] - y_parallel_borders.ravel()[:,np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0638afa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False,  True, False,  True,  True, False,\n",
       "       False])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.any(abs(test[:,0] - y_parallel_borders.ravel()[:,np.newaxis]) < 0.4, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6603f058",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False, False, False, False, False, False],\n",
       "       [False, False, False, False, False, False],\n",
       "       [False, False, False, False, False, False],\n",
       "       [False, False, False, False, False, False],\n",
       "       [False, False, False, False,  True, False],\n",
       "       [False, False, False, False, False, False],\n",
       "       [False, False, False, False,  True, False],\n",
       "       [False, False, False, False,  True, False],\n",
       "       [False, False, False, False, False, False],\n",
       "       [False, False, False, False,  True, False]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs(test[:,0][:,np.newaxis] - y_parallel_borders) < 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ebd72f23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[19.03392003, 14.03392003,  9.03392003,  4.03392003,  0.96607997,\n",
       "         5.96607997],\n",
       "       [20.58757585, 15.58757585, 10.58757585,  5.58757585,  0.58757585,\n",
       "         4.41242415],\n",
       "       [21.54958566, 16.54958566, 11.54958566,  6.54958566,  1.54958566,\n",
       "         3.45041434],\n",
       "       [19.21018167, 14.21018167,  9.21018167,  4.21018167,  0.78981833,\n",
       "         5.78981833],\n",
       "       [20.3884485 , 15.3884485 , 10.3884485 ,  5.3884485 ,  0.3884485 ,\n",
       "         4.6115515 ],\n",
       "       [21.79949368, 16.79949368, 11.79949368,  6.79949368,  1.79949368,\n",
       "         3.20050632],\n",
       "       [20.34492359, 15.34492359, 10.34492359,  5.34492359,  0.34492359,\n",
       "         4.65507641],\n",
       "       [19.67883494, 14.67883494,  9.67883494,  4.67883494,  0.32116506,\n",
       "         5.32116506],\n",
       "       [21.01418777, 16.01418777, 11.01418777,  6.01418777,  1.01418777,\n",
       "         3.98581223],\n",
       "       [19.57184698, 14.57184698,  9.57184698,  4.57184698,  0.42815302,\n",
       "         5.42815302]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs(test[:,0][:,np.newaxis] - y_parallel_borders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dd49d3d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-6.53392003,  1.        ],\n",
       "       [-8.08757585,  2.        ],\n",
       "       [-9.04958566,  3.        ],\n",
       "       [-6.71018167,  4.        ],\n",
       "       [-7.8884485 ,  5.        ],\n",
       "       [-9.29949368,  6.        ],\n",
       "       [-7.84492359,  7.        ],\n",
       "       [-7.17883494,  8.        ],\n",
       "       [-8.51418777,  9.        ],\n",
       "       [-7.07184698, 10.        ]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_parallel_borders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840897f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "           (locs[:,0] > 7.5) & (locs[:,1] > 12),\n",
    "           (locs[:,0] > 7.5) & (locs[:,1] > 4),\n",
    "           (locs[:,0] > 7.5) & (locs[:,1] > -4),\n",
    "           (locs[:,0] > 7.5) & (locs[:,1] > -12),\n",
    "           (locs[:,0] > 7.5) & (locs[:,1] > -20),\n",
    "           (locs[:,0] > 2.5) & (locs[:,1] > 12),\n",
    "           (locs[:,0] > 2.5) & (locs[:,1] > 4),\n",
    "           (locs[:,0] > 2.5) & (locs[:,1] > -4),\n",
    "           (locs[:,0] > 2.5) & (locs[:,1] > -12),\n",
    "           (locs[:,0] > 2.5) & (locs[:,1] > -20),\n",
    "           (locs[:,0] > -2.5) & (locs[:,1] > 12),\n",
    "           (locs[:,0] > -2.5) & (locs[:,1] > 4),\n",
    "           (locs[:,0] > -2.5) & (locs[:,1] > -4),\n",
    "           (locs[:,0] > -2.5) & (locs[:,1] > -12),\n",
    "           (locs[:,0] > -2.5) & (locs[:,1] > -20),\n",
    "           (locs[:,0] > -7.5) & (locs[:,1] > 12),\n",
    "           (locs[:,0] > -7.5) & (locs[:,1] > 4),\n",
    "           (locs[:,0] > -7.5) & (locs[:,1] > -4),\n",
    "           (locs[:,0] > -7.5) & (locs[:,1] > -12),\n",
    "           (locs[:,0] > -7.5) & (locs[:,1] > -20),\n",
    "           (locs[:,0] > -12.5) & (locs[:,1] > 12),\n",
    "           (locs[:,0] > -12.5) & (locs[:,1] > 4),\n",
    "           (locs[:,0] > -12.5) & (locs[:,1] > -4),\n",
    "           (locs[:,0] > -12.5) & (locs[:,1] > -12),\n",
    "           (locs[:,0] > -12.5) & (locs[:,1] > -20),\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "025a1e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[1,2], [3,4]])\n",
    "b = np.array([[5,6], [7,8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7154da29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [3, 4],\n",
       "       [5, 6],\n",
       "       [7, 8]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.vstack((a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5adc3a16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c852bb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3884485"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4296451c",
   "metadata": {},
   "outputs": [],
   "source": [
    "borders_parallel_x = np.array([20, 12, 4, -4, -12, -20])\n",
    "    \n",
    "x2 = abs(test[:,1] - borders_parallel_x[:,np.newaxis]) < 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "25c95aa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False, False, False, False, False, False, False, False, False,\n",
       "        False],\n",
       "       [False, False, False, False, False, False, False, False, False,\n",
       "        False],\n",
       "       [False, False, False, False, False, False, False, False, False,\n",
       "        False],\n",
       "       [False, False, False, False, False, False, False, False, False,\n",
       "        False],\n",
       "       [False, False,  True, False, False, False, False,  True, False,\n",
       "        False],\n",
       "       [False, False, False, False, False, False, False, False, False,\n",
       "        False]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7f4b7de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         3.56977974 3.66986494 0.27888423 1.37021758 5.02384726\n",
      "  4.22836547 2.91116016 2.4890324  1.87246881]\n",
      " [3.56977974 0.         1.10418487 3.29912108 3.01377867 1.55867776\n",
      "  0.84177428 0.98312162 1.75856201 1.74621928]\n",
      " [3.66986494 1.10418487 0.         3.39174356 2.72496393 1.54254513\n",
      "  1.80787764 1.87817864 1.28125928 2.16403932]\n",
      " [0.27888423 3.29912108 3.39174356 0.         1.17830403 4.7464739\n",
      "  3.96951918 2.66424898 2.21882451 1.6183456 ]\n",
      " [1.37021758 3.01377867 2.72496393 1.17830403 0.         4.22966971\n",
      "  3.81348353 2.72604816 1.4438046  1.7845742 ]\n",
      " [5.02384726 1.55867776 1.54254513 4.7464739  4.22966971 0.\n",
      "  1.46495542 2.51674655 2.79863858 3.27493671]\n",
      " [4.22836547 0.84177428 1.80787764 3.96951918 3.81348353 1.46495542\n",
      "  0.         1.35603376 2.59969746 2.35685281]\n",
      " [2.91116016 0.98312162 1.87817864 2.66424898 2.72604816 2.51674655\n",
      "  1.35603376 0.         1.88533315 1.05075114]\n",
      " [2.4890324  1.75856201 1.28125928 2.21882451 1.4438046  2.79863858\n",
      "  2.59969746 1.88533315 0.         1.47034827]\n",
      " [1.87246881 1.74621928 2.16403932 1.6183456  1.7845742  3.27493671\n",
      "  2.35685281 1.05075114 1.47034827 0.        ]]\n",
      "[[False False False  True False False False False False False]\n",
      " [False False False False False False False False False False]\n",
      " [False False False False False False False False False False]\n",
      " [False False False False False False False False False False]\n",
      " [False False False False False False False False False False]\n",
      " [False False False False False False False False False False]\n",
      " [False False False False False False False False False False]\n",
      " [False False False False False False False False False False]\n",
      " [False False False False False False False False False False]\n",
      " [False False False False False False False False False False]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([False, False, False,  True, False, False, False, False, False,\n",
       "       False])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_overlapping(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "12807d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[2, 3], [5, 10]])\n",
    "B = np.array([3, 10, 11, 12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "4441b571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6.95534788  9.83610616 10.36631163  7.1964284   8.3030751  11.3910415\n",
      "  10.12706462  8.87786202  9.36165319  8.21371451]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 6.95534788,  9.83610616, 10.36631163,  7.1964284 ,  8.3030751 ,\n",
       "        11.3910415 , 10.12706462,  8.87786202,  9.36165319,  8.21371451]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_distance(test, np.array([[0, -7]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9f51f86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_distance(x, y):\n",
    "    x = x - y[:,np.newaxis]\n",
    "    x = x**2\n",
    "    x = x.sum(axis=2)\n",
    "    x = np.sqrt(x)\n",
    "#     print(x)\n",
    "    return x\n",
    "\n",
    "# check if the target overlaps with \n",
    "# another target or border\n",
    "def is_overlapping(x, y=None, limit=0.5):\n",
    "    # check with another target\n",
    "    if y is None: y = x\n",
    "    x = find_distance(x, y) < limit\n",
    "    x = np.triu(x, k=1)\n",
    "    \n",
    "    # check with the borders\n",
    "    borders_parallel_y = np.array([12.5, 7.5, 2.5, -2.5, -7.5, -12.5])\n",
    "    x1 = abs(test[:,0] - borders_parallel_y[:,np.newaxis]) < 0.4\n",
    "\n",
    "    borders_parallel_x = np.array([20, 12, 4, -4, -12, -20])\n",
    "    x2 = abs(test[:,1] - borders_parallel_x[:,np.newaxis]) < 0.4\n",
    "    \n",
    "#     print(x)\n",
    "    \n",
    "    x = np.vstack((x, x1, x2))\n",
    "    x = np.any(x, axis=0)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e386c8f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.any(is_overlapping(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3f4f92d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0,  0],\n",
       "        [ 9, 49]],\n",
       "\n",
       "       [[ 9, 49],\n",
       "        [ 0,  0]]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E = D**2\n",
    "E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "620427f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0, 58],\n",
       "       [58,  0]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F = E.sum(axis=2)\n",
    "F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "715b65b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 7.61577311],\n",
       "       [7.61577311, 0.        ]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G = np.sqrt(F)\n",
    "G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "34ed42fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49497474683058323"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt((0.35**2) + (0.35**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f424d06a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False,  True, False, False, False, False, False,\n",
       "       False])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.any(np.triu(find_distance(test) < 0.5, k=1), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0a00be51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  4,   9],\n",
       "       [ 25, 100]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "edaa597d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 13, 125])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(A**2).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b8937d5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.60555128, 11.18033989])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt((A**2).sum(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "452df65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = A == B[:,np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bffbf119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False,  True, False, False],\n",
       "       [False, False, False,  True],\n",
       "       [False, False, False, False],\n",
       "       [False, False, False, False]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cc028534",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False,  True, False,  True])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.any(C, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f09f74eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2],\n",
       "       [3],\n",
       "       [5]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A[:,np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddee8aba",
   "metadata": {},
   "source": [
    "# Define a Gym Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "341caa98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnafiEnv(Env):\n",
    "    def __init__(self, num_targets, max_timestep, drone_ip=\"10.202.0.1\", is_training=False):\n",
    "        super(AnafiEnv, self).__init__()\n",
    "        \n",
    "        Simulation.disable_battery()\n",
    "#         Simulation.cease_targets()\n",
    "        \n",
    "        self.num_targets = num_targets\n",
    "        self.max_timestep = max_timestep\n",
    "        self.begin(num_targets, max_timestep, is_training, drone_ip)\n",
    "        \n",
    "        self.action_space = spaces.Discrete(len(self.agent.action))\n",
    "        self.observation_space = spaces.Box( # {t, cell_id, [I1, I2, I3, ..., In]}\n",
    "            low=np.array([0, 1] + num_targets*[0]), \n",
    "            high=np.array([max_timestep, 25] + num_targets*[1]), \n",
    "            dtype=np.uint8,\n",
    "        )\n",
    "        \n",
    "#         Simulation.move_targets()\n",
    "    \n",
    "    def begin(self, num_targets, max_timestep, is_training, drone_ip=\"10.202.0.1\"):\n",
    "        self.agent = Drone(drone_ip, num_targets, max_timestep, is_training)\n",
    "    \n",
    "    def step(self, action):\n",
    "        obs, reward, done, info = self.agent.take_action(action)\n",
    "        \n",
    "        return obs, reward, done, info\n",
    "    \n",
    "    def reset(self):\n",
    "#         Simulation.reset_targets()\n",
    "        self.agent.target_positions = Simulation.gen_targets_pos(self.num_targets)\n",
    "        return self.agent.reset()\n",
    "    \n",
    "    def render(self, mode='human'):\n",
    "        pass\n",
    "    \n",
    "    def close(self):\n",
    "#         Simulation.cease_targets()\n",
    "        del self.agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626d1b39",
   "metadata": {},
   "source": [
    "# Run the simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc1dbac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def disp_info(action, observation, reward, done, info):\n",
    "#     clear_output(wait=True)\n",
    "    print(\"Action:\", info[\"action\"] + \",\", info[\"direction\"])\n",
    "    print(\"State:\", observation)\n",
    "    print(\"Reward:\", reward)\n",
    "#     down_scale = 3\n",
    "#     display(img.resize((img.size[0]//down_scale, img.size[1]//down_scale)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a85b8dee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: BR, Cell 13 --> Cell 19\n",
      "State: [ 1 19  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: F, Cell 19 --> Cell 14\n",
      "State: [ 2 14  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BR, Cell 14 --> Cell 20\n",
      "State: [ 3 20  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BR, Cell 20 --> Cell 20\n",
      "State: [ 4 20  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: R, Cell 20 --> Cell 20\n",
      "State: [ 5 20  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: R, Cell 20 --> Cell 20\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: BL, Cell 13 --> Cell 17\n",
      "State: [ 1 17  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BL, Cell 17 --> Cell 21\n",
      "State: [ 2 21  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: B, Cell 21 --> Cell 21\n",
      "State: [ 3 21  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: F, Cell 21 --> Cell 16\n",
      "State: [ 4 16  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: B, Cell 16 --> Cell 21\n",
      "State: [ 5 21  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: B, Cell 16 --> Cell 21\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: BL, Cell 13 --> Cell 17\n",
      "State: [ 1 17  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: H, Cell 17 --> Cell 17\n",
      "State: [ 2 17  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BL, Cell 17 --> Cell 21\n",
      "State: [ 3 21  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: L, Cell 21 --> Cell 21\n",
      "State: [ 4 21  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: B, Cell 21 --> Cell 21\n",
      "State: [ 5 21  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: B, Cell 21 --> Cell 21\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: FL, Cell 13 --> Cell 7\n",
      "State: [1 7 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: H, Cell 7 --> Cell 7\n",
      "State: [2 7 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-24 05:43:26,323 \u001b[31m[ERROR] \u001b[0m\tulog - pomp - epoll_ctl(fd=105) err=9(Bad file descriptor)\u001b[0m\n",
      "2022-03-24 05:43:26,324 \u001b[31m[ERROR] \u001b[0m\tulog - pomp - epoll_ctl op=2 cb=0x7ff731c633c8 userdata=0x7ff5cc414f10\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: L, Cell 7 --> Cell 6\n",
      "State: [3 6 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: F, Cell 6 --> Cell 1\n",
      "State: [4 1 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: FL, Cell 1 --> Cell 1\n",
      "State: [5 1 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: FL, Cell 1 --> Cell 1\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: L, Cell 13 --> Cell 12\n",
      "State: [ 1 12  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BL, Cell 12 --> Cell 16\n",
      "State: [ 2 16  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: R, Cell 16 --> Cell 17\n",
      "State: [ 3 17  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: H, Cell 17 --> Cell 17\n",
      "State: [ 4 17  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: H, Cell 17 --> Cell 17\n",
      "State: [ 5 17  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: H, Cell 17 --> Cell 17\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: F, Cell 13 --> Cell 8\n",
      "State: [1 8 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: L, Cell 8 --> Cell 7\n",
      "State: [2 7 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: L, Cell 7 --> Cell 6\n",
      "State: [3 6 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: L, Cell 6 --> Cell 6\n",
      "State: [4 6 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: FR, Cell 6 --> Cell 2\n",
      "State: [5 2 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: FR, Cell 6 --> Cell 2\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: BL, Cell 13 --> Cell 17\n",
      "State: [ 1 17  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: L, Cell 17 --> Cell 16\n",
      "State: [ 2 16  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: B, Cell 16 --> Cell 21\n",
      "State: [ 3 21  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BL, Cell 21 --> Cell 21\n",
      "State: [ 4 21  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FL, Cell 21 --> Cell 21\n",
      "State: [ 5 21  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: FL, Cell 21 --> Cell 21\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: BL, Cell 13 --> Cell 17\n",
      "State: [ 1 17  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BR, Cell 17 --> Cell 23\n",
      "State: [ 2 23  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: L, Cell 23 --> Cell 22\n",
      "State: [ 3 22  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: H, Cell 22 --> Cell 22\n",
      "State: [ 4 22  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BL, Cell 22 --> Cell 22\n",
      "State: [ 5 22  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: BL, Cell 22 --> Cell 22\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: FR, Cell 13 --> Cell 9\n",
      "State: [1 9 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: F, Cell 9 --> Cell 4\n",
      "State: [2 4 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: FL, Cell 4 --> Cell 4\n",
      "State: [3 4 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: L, Cell 4 --> Cell 3\n",
      "State: [4 3 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: R, Cell 3 --> Cell 4\n",
      "State: [5 4 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: R, Cell 3 --> Cell 4\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: BR, Cell 13 --> Cell 19\n",
      "State: [ 1 19  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: R, Cell 19 --> Cell 20\n",
      "State: [ 2 20  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: L, Cell 20 --> Cell 19\n",
      "State: [ 3 19  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: F, Cell 19 --> Cell 14\n",
      "State: [ 4 14  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: F, Cell 14 --> Cell 9\n",
      "State: [5 9 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: F, Cell 14 --> Cell 9\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: B, Cell 13 --> Cell 18\n",
      "State: [ 1 18  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FL, Cell 18 --> Cell 12\n",
      "State: [ 2 12  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: B, Cell 12 --> Cell 17\n",
      "State: [ 3 17  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: F, Cell 17 --> Cell 12\n",
      "State: [ 4 12  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: H, Cell 12 --> Cell 12\n",
      "State: [ 5 12  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: H, Cell 12 --> Cell 12\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: R, Cell 13 --> Cell 14\n",
      "State: [ 1 14  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FR, Cell 14 --> Cell 10\n",
      "State: [ 2 10  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: R, Cell 10 --> Cell 10\n",
      "State: [ 3 10  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BL, Cell 10 --> Cell 14\n",
      "State: [ 4 14  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: R, Cell 14 --> Cell 15\n",
      "State: [ 5 15  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: R, Cell 14 --> Cell 15\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: BL, Cell 13 --> Cell 17\n",
      "State: [ 1 17  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: L, Cell 17 --> Cell 16\n",
      "State: [ 2 16  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: F, Cell 16 --> Cell 11\n",
      "State: [ 3 11  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BL, Cell 11 --> Cell 11\n",
      "State: [ 4 11  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FR, Cell 11 --> Cell 7\n",
      "State: [5 7 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: FR, Cell 11 --> Cell 7\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: H, Cell 13 --> Cell 13\n",
      "State: [ 1 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BR, Cell 13 --> Cell 19\n",
      "State: [ 2 19  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: F, Cell 19 --> Cell 14\n",
      "State: [ 3 14  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: B, Cell 14 --> Cell 19\n",
      "State: [ 4 19  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BR, Cell 19 --> Cell 25\n",
      "State: [ 5 25  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: BR, Cell 19 --> Cell 25\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: FL, Cell 13 --> Cell 7\n",
      "State: [1 7 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: BL, Cell 7 --> Cell 11\n",
      "State: [ 2 11  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: L, Cell 11 --> Cell 11\n",
      "State: [ 3 11  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: H, Cell 11 --> Cell 11\n",
      "State: [ 4 11  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: R, Cell 11 --> Cell 12\n",
      "State: [ 5 12  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: R, Cell 11 --> Cell 12\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: R, Cell 13 --> Cell 14\n",
      "State: [ 1 14  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BL, Cell 14 --> Cell 18\n",
      "State: [ 2 18  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: F, Cell 18 --> Cell 13\n",
      "State: [ 3 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BL, Cell 13 --> Cell 17\n",
      "State: [ 4 17  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FL, Cell 17 --> Cell 11\n",
      "State: [ 5 11  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: FL, Cell 17 --> Cell 11\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: BR, Cell 13 --> Cell 19\n",
      "State: [ 1 19  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BR, Cell 19 --> Cell 25\n",
      "State: [ 2 25  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FL, Cell 25 --> Cell 19\n",
      "State: [ 3 19  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FL, Cell 19 --> Cell 13\n",
      "State: [ 4 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FL, Cell 13 --> Cell 7\n",
      "State: [5 7 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: FL, Cell 13 --> Cell 7\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: R, Cell 13 --> Cell 14\n",
      "State: [ 1 14  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BL, Cell 14 --> Cell 18\n",
      "State: [ 2 18  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: B, Cell 18 --> Cell 23\n",
      "State: [ 3 23  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: H, Cell 23 --> Cell 23\n",
      "State: [ 4 23  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: H, Cell 23 --> Cell 23\n",
      "State: [ 5 23  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: H, Cell 23 --> Cell 23\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: BR, Cell 13 --> Cell 19\n",
      "State: [ 1 19  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BL, Cell 19 --> Cell 23\n",
      "State: [ 2 23  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BR, Cell 23 --> Cell 23\n",
      "State: [ 3 23  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: B, Cell 23 --> Cell 23\n",
      "State: [ 4 23  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: L, Cell 23 --> Cell 22\n",
      "State: [ 5 22  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: L, Cell 23 --> Cell 22\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: FL, Cell 13 --> Cell 7\n",
      "State: [1 7 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: H, Cell 7 --> Cell 7\n",
      "State: [2 7 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: B, Cell 7 --> Cell 12\n",
      "State: [ 3 12  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: R, Cell 12 --> Cell 13\n",
      "State: [ 4 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: F, Cell 13 --> Cell 8\n",
      "State: [5 8 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: F, Cell 13 --> Cell 8\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: BR, Cell 13 --> Cell 19\n",
      "State: [ 1 19  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FL, Cell 19 --> Cell 13\n",
      "State: [ 2 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: L, Cell 13 --> Cell 12\n",
      "State: [ 3 12  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FR, Cell 12 --> Cell 8\n",
      "State: [4 8 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: L, Cell 8 --> Cell 7\n",
      "State: [5 7 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: L, Cell 8 --> Cell 7\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: BL, Cell 13 --> Cell 17\n",
      "State: [ 1 17  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: F, Cell 17 --> Cell 12\n",
      "State: [ 2 12  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: H, Cell 12 --> Cell 12\n",
      "State: [ 3 12  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: R, Cell 12 --> Cell 13\n",
      "State: [ 4 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FR, Cell 13 --> Cell 9\n",
      "State: [5 9 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: FR, Cell 13 --> Cell 9\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: BL, Cell 13 --> Cell 17\n",
      "State: [ 1 17  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: H, Cell 17 --> Cell 17\n",
      "State: [ 2 17  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: R, Cell 17 --> Cell 18\n",
      "State: [ 3 18  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BL, Cell 18 --> Cell 22\n",
      "State: [ 4 22  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: R, Cell 22 --> Cell 23\n",
      "State: [ 5 23  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: R, Cell 22 --> Cell 23\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: L, Cell 13 --> Cell 12\n",
      "State: [ 1 12  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BL, Cell 12 --> Cell 16\n",
      "State: [ 2 16  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BR, Cell 16 --> Cell 22\n",
      "State: [ 3 22  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FL, Cell 22 --> Cell 16\n",
      "State: [ 4 16  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: B, Cell 16 --> Cell 21\n",
      "State: [ 5 21  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: B, Cell 16 --> Cell 21\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: L, Cell 13 --> Cell 12\n",
      "State: [ 1 12  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: B, Cell 12 --> Cell 17\n",
      "State: [ 2 17  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FR, Cell 17 --> Cell 13\n",
      "State: [ 3 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FL, Cell 13 --> Cell 7\n",
      "State: [4 7 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: FR, Cell 7 --> Cell 3\n",
      "State: [5 3 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: FR, Cell 7 --> Cell 3\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: R, Cell 13 --> Cell 14\n",
      "State: [ 1 14  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: H, Cell 14 --> Cell 14\n",
      "State: [ 2 14  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BR, Cell 14 --> Cell 20\n",
      "State: [ 3 20  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BL, Cell 20 --> Cell 24\n",
      "State: [ 4 24  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: B, Cell 24 --> Cell 24\n",
      "State: [ 5 24  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: B, Cell 24 --> Cell 24\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: B, Cell 13 --> Cell 18\n",
      "State: [ 1 18  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: R, Cell 18 --> Cell 19\n",
      "State: [ 2 19  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: F, Cell 19 --> Cell 14\n",
      "State: [ 3 14  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: L, Cell 14 --> Cell 13\n",
      "State: [ 4 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BR, Cell 13 --> Cell 19\n",
      "State: [ 5 19  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: BR, Cell 13 --> Cell 19\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: B, Cell 13 --> Cell 18\n",
      "State: [ 1 18  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BR, Cell 18 --> Cell 24\n",
      "State: [ 2 24  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FR, Cell 24 --> Cell 20\n",
      "State: [ 3 20  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: F, Cell 20 --> Cell 15\n",
      "State: [ 4 15  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: BR, Cell 15 --> Cell 15\n",
      "State: [ 5 15  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: BR, Cell 15 --> Cell 15\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: L, Cell 13 --> Cell 12\n",
      "State: [ 1 12  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BR, Cell 12 --> Cell 18\n",
      "State: [ 2 18  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: B, Cell 18 --> Cell 23\n",
      "State: [ 3 23  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FL, Cell 23 --> Cell 17\n",
      "State: [ 4 17  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BL, Cell 17 --> Cell 21\n",
      "State: [ 5 21  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: BL, Cell 17 --> Cell 21\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: FL, Cell 13 --> Cell 7\n",
      "State: [1 7 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: R, Cell 7 --> Cell 8\n",
      "State: [2 8 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: BL, Cell 8 --> Cell 12\n",
      "State: [ 3 12  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: F, Cell 12 --> Cell 7\n",
      "State: [4 7 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: L, Cell 7 --> Cell 6\n",
      "State: [5 6 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: L, Cell 7 --> Cell 6\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: BR, Cell 13 --> Cell 19\n",
      "State: [ 1 19  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: R, Cell 19 --> Cell 20\n",
      "State: [ 2 20  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: F, Cell 20 --> Cell 15\n",
      "State: [ 3 15  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BL, Cell 15 --> Cell 19\n",
      "State: [ 4 19  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BR, Cell 19 --> Cell 25\n",
      "State: [ 5 25  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: BR, Cell 19 --> Cell 25\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: BL, Cell 13 --> Cell 17\n",
      "State: [ 1 17  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: B, Cell 17 --> Cell 22\n",
      "State: [ 2 22  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: L, Cell 22 --> Cell 21\n",
      "State: [ 3 21  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BR, Cell 21 --> Cell 21\n",
      "State: [ 4 21  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: L, Cell 21 --> Cell 21\n",
      "State: [ 5 21  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: L, Cell 21 --> Cell 21\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: BR, Cell 13 --> Cell 19\n",
      "State: [ 1 19  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FR, Cell 19 --> Cell 15\n",
      "State: [ 2 15  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: F, Cell 15 --> Cell 10\n",
      "State: [ 3 10  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FL, Cell 10 --> Cell 4\n",
      "State: [4 4 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: L, Cell 4 --> Cell 3\n",
      "State: [5 3 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: L, Cell 4 --> Cell 3\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: B, Cell 13 --> Cell 18\n",
      "State: [ 1 18  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FL, Cell 18 --> Cell 12\n",
      "State: [ 2 12  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: B, Cell 12 --> Cell 17\n",
      "State: [ 3 17  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BR, Cell 17 --> Cell 23\n",
      "State: [ 4 23  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: H, Cell 23 --> Cell 23\n",
      "State: [ 5 23  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: H, Cell 23 --> Cell 23\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: B, Cell 13 --> Cell 18\n",
      "State: [ 1 18  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BL, Cell 18 --> Cell 22\n",
      "State: [ 2 22  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BL, Cell 22 --> Cell 22\n",
      "State: [ 3 22  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: L, Cell 22 --> Cell 21\n",
      "State: [ 4 21  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FL, Cell 21 --> Cell 21\n",
      "State: [ 5 21  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: FL, Cell 21 --> Cell 21\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: F, Cell 13 --> Cell 8\n",
      "State: [1 8 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: BR, Cell 8 --> Cell 14\n",
      "State: [ 2 14  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BR, Cell 14 --> Cell 20\n",
      "State: [ 3 20  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: R, Cell 20 --> Cell 20\n",
      "State: [ 4 20  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: R, Cell 20 --> Cell 20\n",
      "State: [ 5 20  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: R, Cell 20 --> Cell 20\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: BL, Cell 13 --> Cell 17\n",
      "State: [ 1 17  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FL, Cell 17 --> Cell 11\n",
      "State: [ 2 11  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FR, Cell 11 --> Cell 7\n",
      "State: [3 7 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: F, Cell 7 --> Cell 2\n",
      "State: [4 2 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: H, Cell 2 --> Cell 2\n",
      "State: [5 2 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: H, Cell 2 --> Cell 2\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: BR, Cell 13 --> Cell 19\n",
      "State: [ 1 19  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BR, Cell 19 --> Cell 25\n",
      "State: [ 2 25  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BR, Cell 25 --> Cell 25\n",
      "State: [ 3 25  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: B, Cell 25 --> Cell 25\n",
      "State: [ 4 25  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: B, Cell 25 --> Cell 25\n",
      "State: [ 5 25  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: B, Cell 25 --> Cell 25\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: BR, Cell 13 --> Cell 19\n",
      "State: [ 1 19  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: L, Cell 19 --> Cell 18\n",
      "State: [ 2 18  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: B, Cell 18 --> Cell 23\n",
      "State: [ 3 23  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: L, Cell 23 --> Cell 22\n",
      "State: [ 4 22  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: B, Cell 22 --> Cell 22\n",
      "State: [ 5 22  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: B, Cell 22 --> Cell 22\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: BL, Cell 13 --> Cell 17\n",
      "State: [ 1 17  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: F, Cell 17 --> Cell 12\n",
      "State: [ 2 12  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BL, Cell 12 --> Cell 16\n",
      "State: [ 3 16  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: H, Cell 16 --> Cell 16\n",
      "State: [ 4 16  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BL, Cell 16 --> Cell 16\n",
      "State: [ 5 16  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: BL, Cell 16 --> Cell 16\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: R, Cell 13 --> Cell 14\n",
      "State: [ 1 14  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: R, Cell 14 --> Cell 15\n",
      "State: [ 2 15  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FR, Cell 15 --> Cell 15\n",
      "State: [ 3 15  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: L, Cell 15 --> Cell 14\n",
      "State: [ 4 14  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: H, Cell 14 --> Cell 14\n",
      "State: [ 5 14  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: H, Cell 14 --> Cell 14\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: BR, Cell 13 --> Cell 19\n",
      "State: [ 1 19  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: B, Cell 19 --> Cell 24\n",
      "State: [ 2 24  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BR, Cell 24 --> Cell 24\n",
      "State: [ 3 24  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: R, Cell 24 --> Cell 25\n",
      "State: [ 4 25  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: R, Cell 25 --> Cell 25\n",
      "State: [ 5 25  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: R, Cell 25 --> Cell 25\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: F, Cell 13 --> Cell 8\n",
      "State: [1 8 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: FR, Cell 8 --> Cell 4\n",
      "State: [2 4 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: F, Cell 4 --> Cell 4\n",
      "State: [3 4 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: B, Cell 4 --> Cell 9\n",
      "State: [4 9 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: BL, Cell 9 --> Cell 13\n",
      "State: [ 5 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: BL, Cell 9 --> Cell 13\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: FR, Cell 13 --> Cell 9\n",
      "State: [1 9 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: R, Cell 9 --> Cell 10\n",
      "State: [ 2 10  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: F, Cell 10 --> Cell 5\n",
      "State: [3 5 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: FR, Cell 5 --> Cell 5\n",
      "State: [4 5 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: BR, Cell 5 --> Cell 5\n",
      "State: [5 5 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: BR, Cell 5 --> Cell 5\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: BL, Cell 13 --> Cell 17\n",
      "State: [ 1 17  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BR, Cell 17 --> Cell 23\n",
      "State: [ 2 23  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FR, Cell 23 --> Cell 19\n",
      "State: [ 3 19  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: B, Cell 19 --> Cell 24\n",
      "State: [ 4 24  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BR, Cell 24 --> Cell 24\n",
      "State: [ 5 24  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: BR, Cell 24 --> Cell 24\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: BL, Cell 13 --> Cell 17\n",
      "State: [ 1 17  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FL, Cell 17 --> Cell 11\n",
      "State: [ 2 11  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FL, Cell 11 --> Cell 11\n",
      "State: [ 3 11  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: H, Cell 11 --> Cell 11\n",
      "State: [ 4 11  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FR, Cell 11 --> Cell 7\n",
      "State: [5 7 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: FR, Cell 11 --> Cell 7\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: BR, Cell 13 --> Cell 19\n",
      "State: [ 1 19  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FR, Cell 19 --> Cell 15\n",
      "State: [ 2 15  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FR, Cell 15 --> Cell 15\n",
      "State: [ 3 15  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FL, Cell 15 --> Cell 9\n",
      "State: [4 9 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: L, Cell 9 --> Cell 8\n",
      "State: [5 8 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: L, Cell 9 --> Cell 8\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: BR, Cell 13 --> Cell 19\n",
      "State: [ 1 19  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BL, Cell 19 --> Cell 23\n",
      "State: [ 2 23  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: B, Cell 23 --> Cell 23\n",
      "State: [ 3 23  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FR, Cell 23 --> Cell 19\n",
      "State: [ 4 19  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: B, Cell 19 --> Cell 24\n",
      "State: [ 5 24  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: B, Cell 19 --> Cell 24\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: FR, Cell 13 --> Cell 9\n",
      "State: [1 9 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: BR, Cell 9 --> Cell 15\n",
      "State: [ 2 15  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: B, Cell 15 --> Cell 20\n",
      "State: [ 3 20  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: L, Cell 20 --> Cell 19\n",
      "State: [ 4 19  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: R, Cell 19 --> Cell 20\n",
      "State: [ 5 20  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: R, Cell 19 --> Cell 20\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: B, Cell 13 --> Cell 18\n",
      "State: [ 1 18  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FR, Cell 18 --> Cell 14\n",
      "State: [ 2 14  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: F, Cell 14 --> Cell 9\n",
      "State: [3 9 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: BL, Cell 9 --> Cell 13\n",
      "State: [ 4 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: F, Cell 13 --> Cell 8\n",
      "State: [5 8 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: F, Cell 13 --> Cell 8\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: BR, Cell 13 --> Cell 19\n",
      "State: [ 1 19  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FR, Cell 19 --> Cell 15\n",
      "State: [ 2 15  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: R, Cell 15 --> Cell 15\n",
      "State: [ 3 15  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BL, Cell 15 --> Cell 19\n",
      "State: [ 4 19  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: R, Cell 19 --> Cell 20\n",
      "State: [ 5 20  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: R, Cell 19 --> Cell 20\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: BL, Cell 13 --> Cell 17\n",
      "State: [ 1 17  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: R, Cell 17 --> Cell 18\n",
      "State: [ 2 18  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: H, Cell 18 --> Cell 18\n",
      "State: [ 3 18  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BL, Cell 18 --> Cell 22\n",
      "State: [ 4 22  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: R, Cell 22 --> Cell 23\n",
      "State: [ 5 23  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: R, Cell 22 --> Cell 23\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: F, Cell 13 --> Cell 8\n",
      "State: [1 8 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: R, Cell 8 --> Cell 9\n",
      "State: [2 9 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: BR, Cell 9 --> Cell 15\n",
      "State: [ 3 15  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FR, Cell 15 --> Cell 15\n",
      "State: [ 4 15  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FL, Cell 15 --> Cell 9\n",
      "State: [5 9 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: FL, Cell 15 --> Cell 9\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: FR, Cell 13 --> Cell 9\n",
      "State: [1 9 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: R, Cell 9 --> Cell 10\n",
      "State: [ 2 10  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: F, Cell 10 --> Cell 5\n",
      "State: [3 5 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: H, Cell 5 --> Cell 5\n",
      "State: [4 5 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: FL, Cell 5 --> Cell 5\n",
      "State: [5 5 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: FL, Cell 5 --> Cell 5\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: FL, Cell 13 --> Cell 7\n",
      "State: [1 7 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: L, Cell 7 --> Cell 6\n",
      "State: [2 6 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: L, Cell 6 --> Cell 6\n",
      "State: [3 6 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: BL, Cell 6 --> Cell 6\n",
      "State: [4 6 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: BR, Cell 6 --> Cell 12\n",
      "State: [ 5 12  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: BR, Cell 6 --> Cell 12\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: H, Cell 13 --> Cell 13\n",
      "State: [ 1 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: H, Cell 13 --> Cell 13\n",
      "State: [ 2 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BL, Cell 13 --> Cell 17\n",
      "State: [ 3 17  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: R, Cell 17 --> Cell 18\n",
      "State: [ 4 18  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FR, Cell 18 --> Cell 14\n",
      "State: [ 5 14  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: FR, Cell 18 --> Cell 14\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: R, Cell 13 --> Cell 14\n",
      "State: [ 1 14  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BL, Cell 14 --> Cell 18\n",
      "State: [ 2 18  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BL, Cell 18 --> Cell 22\n",
      "State: [ 3 22  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: R, Cell 22 --> Cell 23\n",
      "State: [ 4 23  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: H, Cell 23 --> Cell 23\n",
      "State: [ 5 23  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: H, Cell 23 --> Cell 23\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: BR, Cell 13 --> Cell 19\n",
      "State: [ 1 19  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: F, Cell 19 --> Cell 14\n",
      "State: [ 2 14  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FR, Cell 14 --> Cell 10\n",
      "State: [ 3 10  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FR, Cell 10 --> Cell 10\n",
      "State: [ 4 10  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BR, Cell 10 --> Cell 10\n",
      "State: [ 5 10  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: BR, Cell 10 --> Cell 10\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: BL, Cell 13 --> Cell 17\n",
      "State: [ 1 17  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FR, Cell 17 --> Cell 13\n",
      "State: [ 2 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BL, Cell 13 --> Cell 17\n",
      "State: [ 3 17  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: B, Cell 17 --> Cell 22\n",
      "State: [ 4 22  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: L, Cell 22 --> Cell 21\n",
      "State: [ 5 21  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: L, Cell 22 --> Cell 21\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: B, Cell 13 --> Cell 18\n",
      "State: [ 1 18  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: H, Cell 18 --> Cell 18\n",
      "State: [ 2 18  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BL, Cell 18 --> Cell 22\n",
      "State: [ 3 22  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BR, Cell 22 --> Cell 22\n",
      "State: [ 4 22  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FR, Cell 22 --> Cell 18\n",
      "State: [ 5 18  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: FR, Cell 22 --> Cell 18\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: FL, Cell 13 --> Cell 7\n",
      "State: [1 7 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: FL, Cell 7 --> Cell 1\n",
      "State: [2 1 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: L, Cell 1 --> Cell 1\n",
      "State: [3 1 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: FL, Cell 1 --> Cell 1\n",
      "State: [4 1 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: L, Cell 1 --> Cell 1\n",
      "State: [5 1 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: L, Cell 1 --> Cell 1\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: H, Cell 13 --> Cell 13\n",
      "State: [ 1 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: R, Cell 13 --> Cell 14\n",
      "State: [ 2 14  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: B, Cell 14 --> Cell 19\n",
      "State: [ 3 19  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FR, Cell 19 --> Cell 15\n",
      "State: [ 4 15  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BR, Cell 15 --> Cell 15\n",
      "State: [ 5 15  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: BR, Cell 15 --> Cell 15\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: FR, Cell 13 --> Cell 9\n",
      "State: [1 9 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: BR, Cell 9 --> Cell 15\n",
      "State: [ 2 15  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BR, Cell 15 --> Cell 15\n",
      "State: [ 3 15  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BL, Cell 15 --> Cell 19\n",
      "State: [ 4 19  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BL, Cell 19 --> Cell 23\n",
      "State: [ 5 23  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: BL, Cell 19 --> Cell 23\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: F, Cell 13 --> Cell 8\n",
      "State: [1 8 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: R, Cell 8 --> Cell 9\n",
      "State: [2 9 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: FR, Cell 9 --> Cell 5\n",
      "State: [3 5 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: L, Cell 5 --> Cell 4\n",
      "State: [4 4 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: L, Cell 4 --> Cell 3\n",
      "State: [5 3 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: L, Cell 4 --> Cell 3\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: L, Cell 13 --> Cell 12\n",
      "State: [ 1 12  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: B, Cell 12 --> Cell 17\n",
      "State: [ 2 17  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: H, Cell 17 --> Cell 17\n",
      "State: [ 3 17  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: R, Cell 17 --> Cell 18\n",
      "State: [ 4 18  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: R, Cell 18 --> Cell 19\n",
      "State: [ 5 19  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: R, Cell 18 --> Cell 19\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: BR, Cell 13 --> Cell 19\n",
      "State: [ 1 19  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BR, Cell 19 --> Cell 25\n",
      "State: [ 2 25  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FR, Cell 25 --> Cell 25\n",
      "State: [ 3 25  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FL, Cell 25 --> Cell 19\n",
      "State: [ 4 19  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: H, Cell 19 --> Cell 19\n",
      "State: [ 5 19  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: H, Cell 19 --> Cell 19\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: BR, Cell 13 --> Cell 19\n",
      "State: [ 1 19  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: H, Cell 19 --> Cell 19\n",
      "State: [ 2 19  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: B, Cell 19 --> Cell 24\n",
      "State: [ 3 24  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: L, Cell 24 --> Cell 23\n",
      "State: [ 4 23  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BR, Cell 23 --> Cell 23\n",
      "State: [ 5 23  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: BR, Cell 23 --> Cell 23\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: BR, Cell 13 --> Cell 19\n",
      "State: [ 1 19  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FL, Cell 19 --> Cell 13\n",
      "State: [ 2 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BR, Cell 13 --> Cell 19\n",
      "State: [ 3 19  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: L, Cell 19 --> Cell 18\n",
      "State: [ 4 18  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: L, Cell 18 --> Cell 17\n",
      "State: [ 5 17  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: L, Cell 18 --> Cell 17\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: FL, Cell 13 --> Cell 7\n",
      "State: [1 7 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: B, Cell 7 --> Cell 12\n",
      "State: [ 2 12  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: B, Cell 12 --> Cell 17\n",
      "State: [ 3 17  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: F, Cell 17 --> Cell 12\n",
      "State: [ 4 12  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BR, Cell 12 --> Cell 18\n",
      "State: [ 5 18  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: BR, Cell 12 --> Cell 18\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: F, Cell 13 --> Cell 8\n",
      "State: [1 8 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: H, Cell 8 --> Cell 8\n",
      "State: [2 8 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: FL, Cell 8 --> Cell 2\n",
      "State: [3 2 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: F, Cell 2 --> Cell 2\n",
      "State: [4 2 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: FL, Cell 2 --> Cell 2\n",
      "State: [5 2 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: FL, Cell 2 --> Cell 2\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: FR, Cell 13 --> Cell 9\n",
      "State: [1 9 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: BR, Cell 9 --> Cell 15\n",
      "State: [ 2 15  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BL, Cell 15 --> Cell 19\n",
      "State: [ 3 19  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: B, Cell 19 --> Cell 24\n",
      "State: [ 4 24  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FR, Cell 24 --> Cell 20\n",
      "State: [ 5 20  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: FR, Cell 24 --> Cell 20\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: BL, Cell 13 --> Cell 17\n",
      "State: [ 1 17  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: F, Cell 17 --> Cell 12\n",
      "State: [ 2 12  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: H, Cell 12 --> Cell 12\n",
      "State: [ 3 12  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FL, Cell 12 --> Cell 6\n",
      "State: [4 6 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: BL, Cell 6 --> Cell 6\n",
      "State: [5 6 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: BL, Cell 6 --> Cell 6\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: BR, Cell 13 --> Cell 19\n",
      "State: [ 1 19  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BR, Cell 19 --> Cell 25\n",
      "State: [ 2 25  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: B, Cell 25 --> Cell 25\n",
      "State: [ 3 25  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BR, Cell 25 --> Cell 25\n",
      "State: [ 4 25  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FL, Cell 25 --> Cell 19\n",
      "State: [ 5 19  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: FL, Cell 25 --> Cell 19\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: FL, Cell 13 --> Cell 7\n",
      "State: [1 7 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: L, Cell 7 --> Cell 6\n",
      "State: [2 6 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: B, Cell 6 --> Cell 11\n",
      "State: [ 3 11  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FL, Cell 11 --> Cell 11\n",
      "State: [ 4 11  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: H, Cell 11 --> Cell 11\n",
      "State: [ 5 11  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: H, Cell 11 --> Cell 11\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: B, Cell 13 --> Cell 18\n",
      "State: [ 1 18  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: B, Cell 18 --> Cell 23\n",
      "State: [ 2 23  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FL, Cell 23 --> Cell 17\n",
      "State: [ 3 17  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: R, Cell 17 --> Cell 18\n",
      "State: [ 4 18  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BL, Cell 18 --> Cell 22\n",
      "State: [ 5 22  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: BL, Cell 18 --> Cell 22\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: L, Cell 13 --> Cell 12\n",
      "State: [ 1 12  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BR, Cell 12 --> Cell 18\n",
      "State: [ 2 18  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: F, Cell 18 --> Cell 13\n",
      "State: [ 3 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: L, Cell 13 --> Cell 12\n",
      "State: [ 4 12  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: L, Cell 12 --> Cell 11\n",
      "State: [ 5 11  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: L, Cell 12 --> Cell 11\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: FL, Cell 13 --> Cell 7\n",
      "State: [1 7 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: L, Cell 7 --> Cell 6\n",
      "State: [2 6 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: R, Cell 6 --> Cell 7\n",
      "State: [3 7 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: L, Cell 7 --> Cell 6\n",
      "State: [4 6 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: BR, Cell 6 --> Cell 12\n",
      "State: [ 5 12  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: BR, Cell 6 --> Cell 12\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: FR, Cell 13 --> Cell 9\n",
      "State: [1 9 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: L, Cell 9 --> Cell 8\n",
      "State: [2 8 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: R, Cell 8 --> Cell 9\n",
      "State: [3 9 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: BL, Cell 9 --> Cell 13\n",
      "State: [ 4 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BR, Cell 13 --> Cell 19\n",
      "State: [ 5 19  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: BR, Cell 13 --> Cell 19\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: FL, Cell 13 --> Cell 7\n",
      "State: [1 7 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: FL, Cell 7 --> Cell 1\n",
      "State: [2 1 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: FR, Cell 1 --> Cell 1\n",
      "State: [3 1 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: H, Cell 1 --> Cell 1\n",
      "State: [4 1 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: FR, Cell 1 --> Cell 1\n",
      "State: [5 1 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: FR, Cell 1 --> Cell 1\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: FL, Cell 13 --> Cell 7\n",
      "State: [1 7 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: H, Cell 7 --> Cell 7\n",
      "State: [2 7 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: B, Cell 7 --> Cell 12\n",
      "State: [ 3 12  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BR, Cell 12 --> Cell 18\n",
      "State: [ 4 18  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: H, Cell 18 --> Cell 18\n",
      "State: [ 5 18  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: H, Cell 18 --> Cell 18\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: R, Cell 13 --> Cell 14\n",
      "State: [ 1 14  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: R, Cell 14 --> Cell 15\n",
      "State: [ 2 15  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BL, Cell 15 --> Cell 19\n",
      "State: [ 3 19  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: R, Cell 19 --> Cell 20\n",
      "State: [ 4 20  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: R, Cell 20 --> Cell 20\n",
      "State: [ 5 20  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: R, Cell 20 --> Cell 20\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: FR, Cell 13 --> Cell 9\n",
      "State: [1 9 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: L, Cell 9 --> Cell 8\n",
      "State: [2 8 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: BL, Cell 8 --> Cell 12\n",
      "State: [ 3 12  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: R, Cell 12 --> Cell 13\n",
      "State: [ 4 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: R, Cell 13 --> Cell 14\n",
      "State: [ 5 14  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: R, Cell 13 --> Cell 14\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: F, Cell 13 --> Cell 8\n",
      "State: [1 8 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: F, Cell 8 --> Cell 3\n",
      "State: [2 3 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: BL, Cell 3 --> Cell 7\n",
      "State: [3 7 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: L, Cell 7 --> Cell 6\n",
      "State: [4 6 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: FL, Cell 6 --> Cell 6\n",
      "State: [5 6 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: FL, Cell 6 --> Cell 6\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: FR, Cell 13 --> Cell 9\n",
      "State: [1 9 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: BR, Cell 9 --> Cell 15\n",
      "State: [ 2 15  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: L, Cell 15 --> Cell 14\n",
      "State: [ 3 14  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FL, Cell 14 --> Cell 8\n",
      "State: [4 8 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: L, Cell 8 --> Cell 7\n",
      "State: [5 7 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: L, Cell 8 --> Cell 7\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: H, Cell 13 --> Cell 13\n",
      "State: [ 1 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: F, Cell 13 --> Cell 8\n",
      "State: [2 8 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: F, Cell 8 --> Cell 3\n",
      "State: [3 3 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: H, Cell 3 --> Cell 3\n",
      "State: [4 3 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: FR, Cell 3 --> Cell 3\n",
      "State: [5 3 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: FR, Cell 3 --> Cell 3\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: BR, Cell 13 --> Cell 19\n",
      "State: [ 1 19  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: L, Cell 19 --> Cell 18\n",
      "State: [ 2 18  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BL, Cell 18 --> Cell 22\n",
      "State: [ 3 22  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FR, Cell 22 --> Cell 18\n",
      "State: [ 4 18  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: H, Cell 18 --> Cell 18\n",
      "State: [ 5 18  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: H, Cell 18 --> Cell 18\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: FL, Cell 13 --> Cell 7\n",
      "State: [1 7 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: BR, Cell 7 --> Cell 13\n",
      "State: [ 2 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: R, Cell 13 --> Cell 14\n",
      "State: [ 3 14  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: B, Cell 14 --> Cell 19\n",
      "State: [ 4 19  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: R, Cell 19 --> Cell 20\n",
      "State: [ 5 20  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: R, Cell 19 --> Cell 20\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: R, Cell 13 --> Cell 14\n",
      "State: [ 1 14  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: R, Cell 14 --> Cell 15\n",
      "State: [ 2 15  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FL, Cell 15 --> Cell 9\n",
      "State: [3 9 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: B, Cell 9 --> Cell 14\n",
      "State: [ 4 14  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: H, Cell 14 --> Cell 14\n",
      "State: [ 5 14  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: H, Cell 14 --> Cell 14\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: B, Cell 13 --> Cell 18\n",
      "State: [ 1 18  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BL, Cell 18 --> Cell 22\n",
      "State: [ 2 22  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FL, Cell 22 --> Cell 16\n",
      "State: [ 3 16  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: R, Cell 16 --> Cell 17\n",
      "State: [ 4 17  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BR, Cell 17 --> Cell 23\n",
      "State: [ 5 23  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: BR, Cell 17 --> Cell 23\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: FL, Cell 13 --> Cell 7\n",
      "State: [1 7 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: L, Cell 7 --> Cell 6\n",
      "State: [2 6 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: B, Cell 6 --> Cell 11\n",
      "State: [ 3 11  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: B, Cell 11 --> Cell 16\n",
      "State: [ 4 16  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: F, Cell 16 --> Cell 11\n",
      "State: [ 5 11  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: F, Cell 16 --> Cell 11\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: B, Cell 13 --> Cell 18\n",
      "State: [ 1 18  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: R, Cell 18 --> Cell 19\n",
      "State: [ 2 19  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: L, Cell 19 --> Cell 18\n",
      "State: [ 3 18  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BR, Cell 18 --> Cell 24\n",
      "State: [ 4 24  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: R, Cell 24 --> Cell 25\n",
      "State: [ 5 25  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: R, Cell 24 --> Cell 25\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: L, Cell 13 --> Cell 12\n",
      "State: [ 1 12  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: L, Cell 12 --> Cell 11\n",
      "State: [ 2 11  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: B, Cell 11 --> Cell 16\n",
      "State: [ 3 16  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FR, Cell 16 --> Cell 12\n",
      "State: [ 4 12  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BL, Cell 12 --> Cell 16\n",
      "State: [ 5 16  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: BL, Cell 12 --> Cell 16\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: FL, Cell 13 --> Cell 7\n",
      "State: [1 7 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: FR, Cell 7 --> Cell 3\n",
      "State: [2 3 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: B, Cell 3 --> Cell 8\n",
      "State: [3 8 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: F, Cell 8 --> Cell 3\n",
      "State: [4 3 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: B, Cell 3 --> Cell 8\n",
      "State: [5 8 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: B, Cell 3 --> Cell 8\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: L, Cell 13 --> Cell 12\n",
      "State: [ 1 12  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: H, Cell 12 --> Cell 12\n",
      "State: [ 2 12  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: B, Cell 12 --> Cell 17\n",
      "State: [ 3 17  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BR, Cell 17 --> Cell 23\n",
      "State: [ 4 23  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: F, Cell 23 --> Cell 18\n",
      "State: [ 5 18  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: F, Cell 23 --> Cell 18\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: L, Cell 13 --> Cell 12\n",
      "State: [ 1 12  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BL, Cell 12 --> Cell 16\n",
      "State: [ 2 16  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BL, Cell 16 --> Cell 16\n",
      "State: [ 3 16  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: L, Cell 16 --> Cell 16\n",
      "State: [ 4 16  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FR, Cell 16 --> Cell 12\n",
      "State: [ 5 12  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: FR, Cell 16 --> Cell 12\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: BL, Cell 13 --> Cell 17\n",
      "State: [ 1 17  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: R, Cell 17 --> Cell 18\n",
      "State: [ 2 18  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BR, Cell 18 --> Cell 24\n",
      "State: [ 3 24  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BL, Cell 24 --> Cell 24\n",
      "State: [ 4 24  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: F, Cell 24 --> Cell 19\n",
      "State: [ 5 19  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: F, Cell 24 --> Cell 19\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: R, Cell 13 --> Cell 14\n",
      "State: [ 1 14  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: R, Cell 14 --> Cell 15\n",
      "State: [ 2 15  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FR, Cell 15 --> Cell 15\n",
      "State: [ 3 15  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FR, Cell 15 --> Cell 15\n",
      "State: [ 4 15  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: F, Cell 15 --> Cell 10\n",
      "State: [ 5 10  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: F, Cell 15 --> Cell 10\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: B, Cell 13 --> Cell 18\n",
      "State: [ 1 18  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FR, Cell 18 --> Cell 14\n",
      "State: [ 2 14  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: L, Cell 14 --> Cell 13\n",
      "State: [ 3 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: L, Cell 13 --> Cell 12\n",
      "State: [ 4 12  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: B, Cell 12 --> Cell 17\n",
      "State: [ 5 17  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: B, Cell 12 --> Cell 17\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: B, Cell 13 --> Cell 18\n",
      "State: [ 1 18  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FL, Cell 18 --> Cell 12\n",
      "State: [ 2 12  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: B, Cell 12 --> Cell 17\n",
      "State: [ 3 17  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FR, Cell 17 --> Cell 13\n",
      "State: [ 4 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: H, Cell 13 --> Cell 13\n",
      "State: [ 5 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: H, Cell 13 --> Cell 13\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: B, Cell 13 --> Cell 18\n",
      "State: [ 1 18  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BL, Cell 18 --> Cell 22\n",
      "State: [ 2 22  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BL, Cell 22 --> Cell 22\n",
      "State: [ 3 22  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: R, Cell 22 --> Cell 23\n",
      "State: [ 4 23  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: R, Cell 23 --> Cell 24\n",
      "State: [ 5 24  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: R, Cell 23 --> Cell 24\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: F, Cell 13 --> Cell 8\n",
      "State: [1 8 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: L, Cell 8 --> Cell 7\n",
      "State: [2 7 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: L, Cell 7 --> Cell 6\n",
      "State: [3 6 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: F, Cell 6 --> Cell 1\n",
      "State: [4 1 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: R, Cell 1 --> Cell 2\n",
      "State: [5 2 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: R, Cell 1 --> Cell 2\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: B, Cell 13 --> Cell 18\n",
      "State: [ 1 18  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: H, Cell 18 --> Cell 18\n",
      "State: [ 2 18  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FL, Cell 18 --> Cell 12\n",
      "State: [ 3 12  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BR, Cell 12 --> Cell 18\n",
      "State: [ 4 18  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: L, Cell 18 --> Cell 17\n",
      "State: [ 5 17  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: L, Cell 18 --> Cell 17\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: F, Cell 13 --> Cell 8\n",
      "State: [1 8 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: F, Cell 8 --> Cell 3\n",
      "State: [2 3 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: BR, Cell 3 --> Cell 9\n",
      "State: [3 9 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: R, Cell 9 --> Cell 10\n",
      "State: [ 4 10  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: B, Cell 10 --> Cell 15\n",
      "State: [ 5 15  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: B, Cell 10 --> Cell 15\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: L, Cell 13 --> Cell 12\n",
      "State: [ 1 12  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: B, Cell 12 --> Cell 17\n",
      "State: [ 2 17  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: B, Cell 17 --> Cell 22\n",
      "State: [ 3 22  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: L, Cell 22 --> Cell 21\n",
      "State: [ 4 21  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: B, Cell 21 --> Cell 21\n",
      "State: [ 5 21  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: B, Cell 21 --> Cell 21\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: BL, Cell 13 --> Cell 17\n",
      "State: [ 1 17  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FL, Cell 17 --> Cell 11\n",
      "State: [ 2 11  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FL, Cell 11 --> Cell 11\n",
      "State: [ 3 11  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BL, Cell 11 --> Cell 11\n",
      "State: [ 4 11  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: F, Cell 11 --> Cell 6\n",
      "State: [5 6 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: F, Cell 11 --> Cell 6\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: L, Cell 13 --> Cell 12\n",
      "State: [ 1 12  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BR, Cell 12 --> Cell 18\n",
      "State: [ 2 18  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BR, Cell 18 --> Cell 24\n",
      "State: [ 3 24  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BR, Cell 24 --> Cell 24\n",
      "State: [ 4 24  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FR, Cell 24 --> Cell 20\n",
      "State: [ 5 20  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: FR, Cell 24 --> Cell 20\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: L, Cell 13 --> Cell 12\n",
      "State: [ 1 12  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BL, Cell 12 --> Cell 16\n",
      "State: [ 2 16  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BR, Cell 16 --> Cell 22\n",
      "State: [ 3 22  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BR, Cell 22 --> Cell 22\n",
      "State: [ 4 22  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: F, Cell 22 --> Cell 17\n",
      "State: [ 5 17  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: F, Cell 22 --> Cell 17\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: H, Cell 13 --> Cell 13\n",
      "State: [ 1 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: F, Cell 13 --> Cell 8\n",
      "State: [2 8 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: L, Cell 8 --> Cell 7\n",
      "State: [3 7 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: FR, Cell 7 --> Cell 3\n",
      "State: [4 3 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: BR, Cell 3 --> Cell 9\n",
      "State: [5 9 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: BR, Cell 3 --> Cell 9\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: FL, Cell 13 --> Cell 7\n",
      "State: [1 7 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: FR, Cell 7 --> Cell 3\n",
      "State: [2 3 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: B, Cell 3 --> Cell 8\n",
      "State: [3 8 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: BL, Cell 8 --> Cell 12\n",
      "State: [ 4 12  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FL, Cell 12 --> Cell 6\n",
      "State: [5 6 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: FL, Cell 12 --> Cell 6\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: F, Cell 13 --> Cell 8\n",
      "State: [1 8 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: F, Cell 8 --> Cell 3\n",
      "State: [2 3 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: B, Cell 3 --> Cell 8\n",
      "State: [3 8 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: H, Cell 8 --> Cell 8\n",
      "State: [4 8 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: L, Cell 8 --> Cell 7\n",
      "State: [5 7 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: L, Cell 8 --> Cell 7\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: R, Cell 13 --> Cell 14\n",
      "State: [ 1 14  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FL, Cell 14 --> Cell 8\n",
      "State: [2 8 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: BR, Cell 8 --> Cell 14\n",
      "State: [ 3 14  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FL, Cell 14 --> Cell 8\n",
      "State: [4 8 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: L, Cell 8 --> Cell 7\n",
      "State: [5 7 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: L, Cell 8 --> Cell 7\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: R, Cell 13 --> Cell 14\n",
      "State: [ 1 14  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BL, Cell 14 --> Cell 18\n",
      "State: [ 2 18  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FR, Cell 18 --> Cell 14\n",
      "State: [ 3 14  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BR, Cell 14 --> Cell 20\n",
      "State: [ 4 20  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: H, Cell 20 --> Cell 20\n",
      "State: [ 5 20  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: H, Cell 20 --> Cell 20\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: B, Cell 13 --> Cell 18\n",
      "State: [ 1 18  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: L, Cell 18 --> Cell 17\n",
      "State: [ 2 17  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: F, Cell 17 --> Cell 12\n",
      "State: [ 3 12  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FR, Cell 12 --> Cell 8\n",
      "State: [4 8 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: B, Cell 8 --> Cell 13\n",
      "State: [ 5 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: B, Cell 8 --> Cell 13\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: R, Cell 13 --> Cell 14\n",
      "State: [ 1 14  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BR, Cell 14 --> Cell 20\n",
      "State: [ 2 20  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BR, Cell 20 --> Cell 20\n",
      "State: [ 3 20  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: H, Cell 20 --> Cell 20\n",
      "State: [ 4 20  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: L, Cell 20 --> Cell 19\n",
      "State: [ 5 19  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: L, Cell 20 --> Cell 19\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: FR, Cell 13 --> Cell 9\n",
      "State: [1 9 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: L, Cell 9 --> Cell 8\n",
      "State: [2 8 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: FR, Cell 8 --> Cell 4\n",
      "State: [3 4 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: L, Cell 4 --> Cell 3\n",
      "State: [4 3 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: B, Cell 3 --> Cell 8\n",
      "State: [5 8 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: B, Cell 3 --> Cell 8\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: BR, Cell 13 --> Cell 19\n",
      "State: [ 1 19  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FL, Cell 19 --> Cell 13\n",
      "State: [ 2 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FR, Cell 13 --> Cell 9\n",
      "State: [3 9 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: F, Cell 9 --> Cell 4\n",
      "State: [4 4 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: R, Cell 4 --> Cell 5\n",
      "State: [5 5 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: R, Cell 4 --> Cell 5\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: H, Cell 13 --> Cell 13\n",
      "State: [ 1 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: B, Cell 13 --> Cell 18\n",
      "State: [ 2 18  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: L, Cell 18 --> Cell 17\n",
      "State: [ 3 17  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: H, Cell 17 --> Cell 17\n",
      "State: [ 4 17  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BR, Cell 17 --> Cell 23\n",
      "State: [ 5 23  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: BR, Cell 17 --> Cell 23\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: FR, Cell 13 --> Cell 9\n",
      "State: [1 9 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: FL, Cell 9 --> Cell 3\n",
      "State: [2 3 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: R, Cell 3 --> Cell 4\n",
      "State: [3 4 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: FL, Cell 4 --> Cell 4\n",
      "State: [4 4 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: FL, Cell 4 --> Cell 4\n",
      "State: [5 4 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: FL, Cell 4 --> Cell 4\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: FL, Cell 13 --> Cell 7\n",
      "State: [1 7 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: R, Cell 7 --> Cell 8\n",
      "State: [2 8 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: B, Cell 8 --> Cell 13\n",
      "State: [ 3 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: F, Cell 13 --> Cell 8\n",
      "State: [4 8 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: H, Cell 8 --> Cell 8\n",
      "State: [5 8 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: H, Cell 8 --> Cell 8\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: BL, Cell 13 --> Cell 17\n",
      "State: [ 1 17  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BL, Cell 17 --> Cell 21\n",
      "State: [ 2 21  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FL, Cell 21 --> Cell 21\n",
      "State: [ 3 21  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: F, Cell 21 --> Cell 16\n",
      "State: [ 4 16  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FL, Cell 16 --> Cell 16\n",
      "State: [ 5 16  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: FL, Cell 16 --> Cell 16\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: FR, Cell 13 --> Cell 9\n",
      "State: [1 9 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: B, Cell 9 --> Cell 14\n",
      "State: [ 2 14  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FL, Cell 14 --> Cell 8\n",
      "State: [3 8 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: BR, Cell 8 --> Cell 14\n",
      "State: [ 4 14  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: B, Cell 14 --> Cell 19\n",
      "State: [ 5 19  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: B, Cell 14 --> Cell 19\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: BR, Cell 13 --> Cell 19\n",
      "State: [ 1 19  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: L, Cell 19 --> Cell 18\n",
      "State: [ 2 18  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BL, Cell 18 --> Cell 22\n",
      "State: [ 3 22  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BR, Cell 22 --> Cell 22\n",
      "State: [ 4 22  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FL, Cell 22 --> Cell 16\n",
      "State: [ 5 16  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: FL, Cell 22 --> Cell 16\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: R, Cell 13 --> Cell 14\n",
      "State: [ 1 14  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BR, Cell 14 --> Cell 20\n",
      "State: [ 2 20  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: R, Cell 20 --> Cell 20\n",
      "State: [ 3 20  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: F, Cell 20 --> Cell 15\n",
      "State: [ 4 15  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: B, Cell 15 --> Cell 20\n",
      "State: [ 5 20  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: B, Cell 15 --> Cell 20\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: FR, Cell 13 --> Cell 9\n",
      "State: [1 9 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: BL, Cell 9 --> Cell 13\n",
      "State: [ 2 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: H, Cell 13 --> Cell 13\n",
      "State: [ 3 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FL, Cell 13 --> Cell 7\n",
      "State: [4 7 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: BL, Cell 7 --> Cell 11\n",
      "State: [ 5 11  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: BL, Cell 7 --> Cell 11\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: FR, Cell 13 --> Cell 9\n",
      "State: [1 9 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: H, Cell 9 --> Cell 9\n",
      "State: [2 9 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: BR, Cell 9 --> Cell 15\n",
      "State: [ 3 15  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BR, Cell 15 --> Cell 15\n",
      "State: [ 4 15  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BR, Cell 15 --> Cell 15\n",
      "State: [ 5 15  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: BR, Cell 15 --> Cell 15\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: L, Cell 13 --> Cell 12\n",
      "State: [ 1 12  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: B, Cell 12 --> Cell 17\n",
      "State: [ 2 17  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: L, Cell 17 --> Cell 16\n",
      "State: [ 3 16  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: H, Cell 16 --> Cell 16\n",
      "State: [ 4 16  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: B, Cell 16 --> Cell 21\n",
      "State: [ 5 21  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: B, Cell 16 --> Cell 21\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: R, Cell 13 --> Cell 14\n",
      "State: [ 1 14  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: B, Cell 14 --> Cell 19\n",
      "State: [ 2 19  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FR, Cell 19 --> Cell 15\n",
      "State: [ 3 15  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FR, Cell 15 --> Cell 15\n",
      "State: [ 4 15  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: B, Cell 15 --> Cell 20\n",
      "State: [ 5 20  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: B, Cell 15 --> Cell 20\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: H, Cell 13 --> Cell 13\n",
      "State: [ 1 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FR, Cell 13 --> Cell 9\n",
      "State: [2 9 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: B, Cell 9 --> Cell 14\n",
      "State: [ 3 14  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FR, Cell 14 --> Cell 10\n",
      "State: [ 4 10  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: H, Cell 10 --> Cell 10\n",
      "State: [ 5 10  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: H, Cell 10 --> Cell 10\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: FR, Cell 13 --> Cell 9\n",
      "State: [1 9 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: R, Cell 9 --> Cell 10\n",
      "State: [ 2 10  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BL, Cell 10 --> Cell 14\n",
      "State: [ 3 14  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FL, Cell 14 --> Cell 8\n",
      "State: [4 8 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward: -1\n",
      "Action: H, Cell 8 --> Cell 8\n",
      "State: [5 8 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: H, Cell 8 --> Cell 8\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: FR, Cell 13 --> Cell 9\n",
      "State: [1 9 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: F, Cell 9 --> Cell 4\n",
      "State: [2 4 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: H, Cell 4 --> Cell 4\n",
      "State: [3 4 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: BR, Cell 4 --> Cell 10\n",
      "State: [ 4 10  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: F, Cell 10 --> Cell 5\n",
      "State: [5 5 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: F, Cell 10 --> Cell 5\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: BR, Cell 13 --> Cell 19\n",
      "State: [ 1 19  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: R, Cell 19 --> Cell 20\n",
      "State: [ 2 20  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: L, Cell 20 --> Cell 19\n",
      "State: [ 3 19  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: H, Cell 19 --> Cell 19\n",
      "State: [ 4 19  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: L, Cell 19 --> Cell 18\n",
      "State: [ 5 18  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: L, Cell 19 --> Cell 18\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: BR, Cell 13 --> Cell 19\n",
      "State: [ 1 19  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: L, Cell 19 --> Cell 18\n",
      "State: [ 2 18  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BL, Cell 18 --> Cell 22\n",
      "State: [ 3 22  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: R, Cell 22 --> Cell 23\n",
      "State: [ 4 23  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FR, Cell 23 --> Cell 19\n",
      "State: [ 5 19  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: FR, Cell 23 --> Cell 19\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: F, Cell 13 --> Cell 8\n",
      "State: [1 8 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: B, Cell 8 --> Cell 13\n",
      "State: [ 2 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FR, Cell 13 --> Cell 9\n",
      "State: [3 9 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: BL, Cell 9 --> Cell 13\n",
      "State: [ 4 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: F, Cell 13 --> Cell 8\n",
      "State: [5 8 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: F, Cell 13 --> Cell 8\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: BL, Cell 13 --> Cell 17\n",
      "State: [ 1 17  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: H, Cell 17 --> Cell 17\n",
      "State: [ 2 17  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: L, Cell 17 --> Cell 16\n",
      "State: [ 3 16  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FR, Cell 16 --> Cell 12\n",
      "State: [ 4 12  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FL, Cell 12 --> Cell 6\n",
      "State: [5 6 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: FL, Cell 12 --> Cell 6\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: F, Cell 13 --> Cell 8\n",
      "State: [1 8 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: BR, Cell 8 --> Cell 14\n",
      "State: [ 2 14  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: H, Cell 14 --> Cell 14\n",
      "State: [ 3 14  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: F, Cell 14 --> Cell 9\n",
      "State: [4 9 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: B, Cell 9 --> Cell 14\n",
      "State: [ 5 14  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: B, Cell 9 --> Cell 14\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: BR, Cell 13 --> Cell 19\n",
      "State: [ 1 19  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: L, Cell 19 --> Cell 18\n",
      "State: [ 2 18  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: F, Cell 18 --> Cell 13\n",
      "State: [ 3 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: L, Cell 13 --> Cell 12\n",
      "State: [ 4 12  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: F, Cell 12 --> Cell 7\n",
      "State: [5 7 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: F, Cell 12 --> Cell 7\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: BL, Cell 13 --> Cell 17\n",
      "State: [ 1 17  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BR, Cell 17 --> Cell 23\n",
      "State: [ 2 23  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: L, Cell 23 --> Cell 22\n",
      "State: [ 3 22  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: L, Cell 22 --> Cell 21\n",
      "State: [ 4 21  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FR, Cell 21 --> Cell 17\n",
      "State: [ 5 17  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: FR, Cell 21 --> Cell 17\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: BL, Cell 13 --> Cell 17\n",
      "State: [ 1 17  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: R, Cell 17 --> Cell 18\n",
      "State: [ 2 18  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: R, Cell 18 --> Cell 19\n",
      "State: [ 3 19  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: L, Cell 19 --> Cell 18\n",
      "State: [ 4 18  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: B, Cell 18 --> Cell 23\n",
      "State: [ 5 23  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: B, Cell 18 --> Cell 23\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: B, Cell 13 --> Cell 18\n",
      "State: [ 1 18  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: L, Cell 18 --> Cell 17\n",
      "State: [ 2 17  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: R, Cell 17 --> Cell 18\n",
      "State: [ 3 18  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FR, Cell 18 --> Cell 14\n",
      "State: [ 4 14  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: L, Cell 14 --> Cell 13\n",
      "State: [ 5 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: L, Cell 14 --> Cell 13\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: FR, Cell 13 --> Cell 9\n",
      "State: [1 9 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: BL, Cell 9 --> Cell 13\n",
      "State: [ 2 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FL, Cell 13 --> Cell 7\n",
      "State: [3 7 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: BR, Cell 7 --> Cell 13\n",
      "State: [ 4 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BR, Cell 13 --> Cell 19\n",
      "State: [ 5 19  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: BR, Cell 13 --> Cell 19\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: L, Cell 13 --> Cell 12\n",
      "State: [ 1 12  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: B, Cell 12 --> Cell 17\n",
      "State: [ 2 17  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: L, Cell 17 --> Cell 16\n",
      "State: [ 3 16  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BL, Cell 16 --> Cell 16\n",
      "State: [ 4 16  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BR, Cell 16 --> Cell 22\n",
      "State: [ 5 22  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: BR, Cell 16 --> Cell 22\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: H, Cell 13 --> Cell 13\n",
      "State: [ 1 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BL, Cell 13 --> Cell 17\n",
      "State: [ 2 17  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FR, Cell 17 --> Cell 13\n",
      "State: [ 3 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: H, Cell 13 --> Cell 13\n",
      "State: [ 4 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FR, Cell 13 --> Cell 9\n",
      "State: [5 9 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: FR, Cell 13 --> Cell 9\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: B, Cell 13 --> Cell 18\n",
      "State: [ 1 18  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: R, Cell 18 --> Cell 19\n",
      "State: [ 2 19  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: L, Cell 19 --> Cell 18\n",
      "State: [ 3 18  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: B, Cell 18 --> Cell 23\n",
      "State: [ 4 23  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: L, Cell 23 --> Cell 22\n",
      "State: [ 5 22  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: L, Cell 23 --> Cell 22\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: L, Cell 13 --> Cell 12\n",
      "State: [ 1 12  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BR, Cell 12 --> Cell 18\n",
      "State: [ 2 18  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: R, Cell 18 --> Cell 19\n",
      "State: [ 3 19  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: H, Cell 19 --> Cell 19\n",
      "State: [ 4 19  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: H, Cell 19 --> Cell 19\n",
      "State: [ 5 19  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: H, Cell 19 --> Cell 19\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: B, Cell 13 --> Cell 18\n",
      "State: [ 1 18  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FL, Cell 18 --> Cell 12\n",
      "State: [ 2 12  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: B, Cell 12 --> Cell 17\n",
      "State: [ 3 17  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: H, Cell 17 --> Cell 17\n",
      "State: [ 4 17  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FL, Cell 17 --> Cell 11\n",
      "State: [ 5 11  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: FL, Cell 17 --> Cell 11\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: H, Cell 13 --> Cell 13\n",
      "State: [ 1 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BR, Cell 13 --> Cell 19\n",
      "State: [ 2 19  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FR, Cell 19 --> Cell 15\n",
      "State: [ 3 15  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FR, Cell 15 --> Cell 15\n",
      "State: [ 4 15  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FR, Cell 15 --> Cell 15\n",
      "State: [ 5 15  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: FR, Cell 15 --> Cell 15\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: FL, Cell 13 --> Cell 7\n",
      "State: [1 7 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: R, Cell 7 --> Cell 8\n",
      "State: [2 8 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: FL, Cell 8 --> Cell 2\n",
      "State: [3 2 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: BR, Cell 2 --> Cell 8\n",
      "State: [4 8 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: BL, Cell 8 --> Cell 12\n",
      "State: [ 5 12  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: BL, Cell 8 --> Cell 12\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: B, Cell 13 --> Cell 18\n",
      "State: [ 1 18  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BL, Cell 18 --> Cell 22\n",
      "State: [ 2 22  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: L, Cell 22 --> Cell 21\n",
      "State: [ 3 21  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: F, Cell 21 --> Cell 16\n",
      "State: [ 4 16  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: L, Cell 16 --> Cell 16\n",
      "State: [ 5 16  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: L, Cell 16 --> Cell 16\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: L, Cell 13 --> Cell 12\n",
      "State: [ 1 12  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FL, Cell 12 --> Cell 6\n",
      "State: [2 6 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: L, Cell 6 --> Cell 6\n",
      "State: [3 6 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: BR, Cell 6 --> Cell 12\n",
      "State: [ 4 12  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: H, Cell 12 --> Cell 12\n",
      "State: [ 5 12  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: H, Cell 12 --> Cell 12\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: FR, Cell 13 --> Cell 9\n",
      "State: [1 9 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: FL, Cell 9 --> Cell 3\n",
      "State: [2 3 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: FR, Cell 3 --> Cell 3\n",
      "State: [3 3 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: FR, Cell 3 --> Cell 3\n",
      "State: [4 3 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: BR, Cell 3 --> Cell 9\n",
      "State: [5 9 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: BR, Cell 3 --> Cell 9\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: FR, Cell 13 --> Cell 9\n",
      "State: [1 9 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: H, Cell 9 --> Cell 9\n",
      "State: [2 9 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: B, Cell 9 --> Cell 14\n",
      "State: [ 3 14  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: B, Cell 14 --> Cell 19\n",
      "State: [ 4 19  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BL, Cell 19 --> Cell 23\n",
      "State: [ 5 23  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: BL, Cell 19 --> Cell 23\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: L, Cell 13 --> Cell 12\n",
      "State: [ 1 12  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FL, Cell 12 --> Cell 6\n",
      "State: [2 6 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: L, Cell 6 --> Cell 6\n",
      "State: [3 6 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: FL, Cell 6 --> Cell 6\n",
      "State: [4 6 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: H, Cell 6 --> Cell 6\n",
      "State: [5 6 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: H, Cell 6 --> Cell 6\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: L, Cell 13 --> Cell 12\n",
      "State: [ 1 12  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FR, Cell 12 --> Cell 8\n",
      "State: [2 8 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: BL, Cell 8 --> Cell 12\n",
      "State: [ 3 12  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BL, Cell 12 --> Cell 16\n",
      "State: [ 4 16  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: B, Cell 16 --> Cell 21\n",
      "State: [ 5 21  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: B, Cell 16 --> Cell 21\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: BL, Cell 13 --> Cell 17\n",
      "State: [ 1 17  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: B, Cell 17 --> Cell 22\n",
      "State: [ 2 22  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: H, Cell 22 --> Cell 22\n",
      "State: [ 3 22  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: F, Cell 22 --> Cell 17\n",
      "State: [ 4 17  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BR, Cell 17 --> Cell 23\n",
      "State: [ 5 23  0  0  0  0  0  0  0  0  0  0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: BR, Cell 17 --> Cell 23\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: BR, Cell 13 --> Cell 19\n",
      "State: [ 1 19  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FL, Cell 19 --> Cell 13\n",
      "State: [ 2 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: B, Cell 13 --> Cell 18\n",
      "State: [ 3 18  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FR, Cell 18 --> Cell 14\n",
      "State: [ 4 14  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FR, Cell 14 --> Cell 10\n",
      "State: [ 5 10  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: FR, Cell 14 --> Cell 10\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: L, Cell 13 --> Cell 12\n",
      "State: [ 1 12  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: H, Cell 12 --> Cell 12\n",
      "State: [ 2 12  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FR, Cell 12 --> Cell 8\n",
      "State: [3 8 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: L, Cell 8 --> Cell 7\n",
      "State: [4 7 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: H, Cell 7 --> Cell 7\n",
      "State: [5 7 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: H, Cell 7 --> Cell 7\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: B, Cell 13 --> Cell 18\n",
      "State: [ 1 18  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: L, Cell 18 --> Cell 17\n",
      "State: [ 2 17  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: L, Cell 17 --> Cell 16\n",
      "State: [ 3 16  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: R, Cell 16 --> Cell 17\n",
      "State: [ 4 17  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: H, Cell 17 --> Cell 17\n",
      "State: [ 5 17  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: H, Cell 17 --> Cell 17\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: FR, Cell 13 --> Cell 9\n",
      "State: [1 9 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: F, Cell 9 --> Cell 4\n",
      "State: [2 4 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: FR, Cell 4 --> Cell 4\n",
      "State: [3 4 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: L, Cell 4 --> Cell 3\n",
      "State: [4 3 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: FR, Cell 3 --> Cell 3\n",
      "State: [5 3 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: FR, Cell 3 --> Cell 3\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: BR, Cell 13 --> Cell 19\n",
      "State: [ 1 19  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BR, Cell 19 --> Cell 25\n",
      "State: [ 2 25  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: H, Cell 25 --> Cell 25\n",
      "State: [ 3 25  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BR, Cell 25 --> Cell 25\n",
      "State: [ 4 25  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: F, Cell 25 --> Cell 20\n",
      "State: [ 5 20  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: F, Cell 25 --> Cell 20\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: BR, Cell 13 --> Cell 19\n",
      "State: [ 1 19  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FR, Cell 19 --> Cell 15\n",
      "State: [ 2 15  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: B, Cell 15 --> Cell 20\n",
      "State: [ 3 20  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BR, Cell 20 --> Cell 20\n",
      "State: [ 4 20  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FR, Cell 20 --> Cell 20\n",
      "State: [ 5 20  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: FR, Cell 20 --> Cell 20\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: H, Cell 13 --> Cell 13\n",
      "State: [ 1 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: L, Cell 13 --> Cell 12\n",
      "State: [ 2 12  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FL, Cell 12 --> Cell 6\n",
      "State: [3 6 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: R, Cell 6 --> Cell 7\n",
      "State: [4 7 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: H, Cell 7 --> Cell 7\n",
      "State: [5 7 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: H, Cell 7 --> Cell 7\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: B, Cell 13 --> Cell 18\n",
      "State: [ 1 18  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: L, Cell 18 --> Cell 17\n",
      "State: [ 2 17  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: B, Cell 17 --> Cell 22\n",
      "State: [ 3 22  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BR, Cell 22 --> Cell 22\n",
      "State: [ 4 22  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: L, Cell 22 --> Cell 21\n",
      "State: [ 5 21  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: L, Cell 22 --> Cell 21\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: B, Cell 13 --> Cell 18\n",
      "State: [ 1 18  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FL, Cell 18 --> Cell 12\n",
      "State: [ 2 12  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FR, Cell 12 --> Cell 8\n",
      "State: [3 8 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: B, Cell 8 --> Cell 13\n",
      "State: [ 4 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BL, Cell 13 --> Cell 17\n",
      "State: [ 5 17  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: BL, Cell 13 --> Cell 17\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: R, Cell 13 --> Cell 14\n",
      "State: [ 1 14  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: R, Cell 14 --> Cell 15\n",
      "State: [ 2 15  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FL, Cell 15 --> Cell 9\n",
      "State: [3 9 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: B, Cell 9 --> Cell 14\n",
      "State: [ 4 14  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: L, Cell 14 --> Cell 13\n",
      "State: [ 5 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: L, Cell 14 --> Cell 13\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: R, Cell 13 --> Cell 14\n",
      "State: [ 1 14  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: L, Cell 14 --> Cell 13\n",
      "State: [ 2 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FR, Cell 13 --> Cell 9\n",
      "State: [3 9 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: FR, Cell 9 --> Cell 5\n",
      "State: [4 5 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: R, Cell 5 --> Cell 5\n",
      "State: [5 5 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: R, Cell 5 --> Cell 5\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: FL, Cell 13 --> Cell 7\n",
      "State: [1 7 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: H, Cell 7 --> Cell 7\n",
      "State: [2 7 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: FL, Cell 7 --> Cell 1\n",
      "State: [3 1 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: F, Cell 1 --> Cell 1\n",
      "State: [4 1 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: R, Cell 1 --> Cell 2\n",
      "State: [5 2 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: R, Cell 1 --> Cell 2\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: L, Cell 13 --> Cell 12\n",
      "State: [ 1 12  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: F, Cell 12 --> Cell 7\n",
      "State: [2 7 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: R, Cell 7 --> Cell 8\n",
      "State: [3 8 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: L, Cell 8 --> Cell 7\n",
      "State: [4 7 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: B, Cell 7 --> Cell 12\n",
      "State: [ 5 12  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: B, Cell 7 --> Cell 12\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: F, Cell 13 --> Cell 8\n",
      "State: [1 8 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: R, Cell 8 --> Cell 9\n",
      "State: [2 9 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: R, Cell 9 --> Cell 10\n",
      "State: [ 3 10  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BR, Cell 10 --> Cell 10\n",
      "State: [ 4 10  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: R, Cell 10 --> Cell 10\n",
      "State: [ 5 10  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: R, Cell 10 --> Cell 10\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: B, Cell 13 --> Cell 18\n",
      "State: [ 1 18  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FL, Cell 18 --> Cell 12\n",
      "State: [ 2 12  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FR, Cell 12 --> Cell 8\n",
      "State: [3 8 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: B, Cell 8 --> Cell 13\n",
      "State: [ 4 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: H, Cell 13 --> Cell 13\n",
      "State: [ 5 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: H, Cell 13 --> Cell 13\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: H, Cell 13 --> Cell 13\n",
      "State: [ 1 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: B, Cell 13 --> Cell 18\n",
      "State: [ 2 18  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: R, Cell 18 --> Cell 19\n",
      "State: [ 3 19  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FL, Cell 19 --> Cell 13\n",
      "State: [ 4 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: B, Cell 13 --> Cell 18\n",
      "State: [ 5 18  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: B, Cell 13 --> Cell 18\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: F, Cell 13 --> Cell 8\n",
      "State: [1 8 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: FL, Cell 8 --> Cell 2\n",
      "State: [2 2 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: BR, Cell 2 --> Cell 8\n",
      "State: [3 8 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: BR, Cell 8 --> Cell 14\n",
      "State: [ 4 14  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BL, Cell 14 --> Cell 18\n",
      "State: [ 5 18  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: BL, Cell 14 --> Cell 18\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: FL, Cell 13 --> Cell 7\n",
      "State: [1 7 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: L, Cell 7 --> Cell 6\n",
      "State: [2 6 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: F, Cell 6 --> Cell 1\n",
      "State: [3 1 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: F, Cell 1 --> Cell 1\n",
      "State: [4 1 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: FL, Cell 1 --> Cell 1\n",
      "State: [5 1 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: FL, Cell 1 --> Cell 1\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: BL, Cell 13 --> Cell 17\n",
      "State: [ 1 17  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: H, Cell 17 --> Cell 17\n",
      "State: [ 2 17  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FR, Cell 17 --> Cell 13\n",
      "State: [ 3 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BR, Cell 13 --> Cell 19\n",
      "State: [ 4 19  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: F, Cell 19 --> Cell 14\n",
      "State: [ 5 14  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: F, Cell 19 --> Cell 14\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: FR, Cell 13 --> Cell 9\n",
      "State: [1 9 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: L, Cell 9 --> Cell 8\n",
      "State: [2 8 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: H, Cell 8 --> Cell 8\n",
      "State: [3 8 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: H, Cell 8 --> Cell 8\n",
      "State: [4 8 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: L, Cell 8 --> Cell 7\n",
      "State: [5 7 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: L, Cell 8 --> Cell 7\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: B, Cell 13 --> Cell 18\n",
      "State: [ 1 18  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: B, Cell 18 --> Cell 23\n",
      "State: [ 2 23  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: R, Cell 23 --> Cell 24\n",
      "State: [ 3 24  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: F, Cell 24 --> Cell 19\n",
      "State: [ 4 19  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: L, Cell 19 --> Cell 18\n",
      "State: [ 5 18  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: L, Cell 19 --> Cell 18\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: FR, Cell 13 --> Cell 9\n",
      "State: [1 9 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: FR, Cell 9 --> Cell 5\n",
      "State: [2 5 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: H, Cell 5 --> Cell 5\n",
      "State: [3 5 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: BL, Cell 5 --> Cell 9\n",
      "State: [4 9 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: FR, Cell 9 --> Cell 5\n",
      "State: [5 5 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: FR, Cell 9 --> Cell 5\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: B, Cell 13 --> Cell 18\n",
      "State: [ 1 18  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: L, Cell 18 --> Cell 17\n",
      "State: [ 2 17  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: F, Cell 17 --> Cell 12\n",
      "State: [ 3 12  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: B, Cell 12 --> Cell 17\n",
      "State: [ 4 17  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: B, Cell 17 --> Cell 22\n",
      "State: [ 5 22  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: B, Cell 17 --> Cell 22\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: B, Cell 13 --> Cell 18\n",
      "State: [ 1 18  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: L, Cell 18 --> Cell 17\n",
      "State: [ 2 17  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: R, Cell 17 --> Cell 18\n",
      "State: [ 3 18  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: L, Cell 18 --> Cell 17\n",
      "State: [ 4 17  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FR, Cell 17 --> Cell 13\n",
      "State: [ 5 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function Drone.__del__ at 0x7ff5cd77f160>\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_9486/3071416055.py\", line 183, in __del__\n",
      "  File \"/tmp/ipykernel_9486/3071416055.py\", line 180, in _land\n",
      "AssertionError: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: FR, Cell 17 --> Cell 13\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: R, Cell 13 --> Cell 14\n",
      "State: [ 1 14  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: L, Cell 14 --> Cell 13\n",
      "State: [ 2 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BL, Cell 13 --> Cell 17\n",
      "State: [ 3 17  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: H, Cell 17 --> Cell 17\n",
      "State: [ 4 17  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: F, Cell 17 --> Cell 12\n",
      "State: [ 5 12  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: F, Cell 17 --> Cell 12\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: FR, Cell 13 --> Cell 9\n",
      "State: [1 9 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: R, Cell 9 --> Cell 10\n",
      "State: [ 2 10  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: F, Cell 10 --> Cell 5\n",
      "State: [3 5 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: H, Cell 5 --> Cell 5\n",
      "State: [4 5 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: L, Cell 5 --> Cell 4\n",
      "State: [5 4 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: L, Cell 5 --> Cell 4\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: R, Cell 13 --> Cell 14\n",
      "State: [ 1 14  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FR, Cell 14 --> Cell 10\n",
      "State: [ 2 10  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BL, Cell 10 --> Cell 14\n",
      "State: [ 3 14  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: H, Cell 14 --> Cell 14\n",
      "State: [ 4 14  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: B, Cell 14 --> Cell 19\n",
      "State: [ 5 19  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: B, Cell 14 --> Cell 19\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: FR, Cell 13 --> Cell 9\n",
      "State: [1 9 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: B, Cell 9 --> Cell 14\n",
      "State: [ 2 14  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FL, Cell 14 --> Cell 8\n",
      "State: [3 8 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: B, Cell 8 --> Cell 13\n",
      "State: [ 4 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BL, Cell 13 --> Cell 17\n",
      "State: [ 5 17  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: BL, Cell 13 --> Cell 17\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: BL, Cell 13 --> Cell 17\n",
      "State: [ 1 17  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FR, Cell 17 --> Cell 13\n",
      "State: [ 2 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BR, Cell 13 --> Cell 19\n",
      "State: [ 3 19  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BL, Cell 19 --> Cell 23\n",
      "State: [ 4 23  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: H, Cell 23 --> Cell 23\n",
      "State: [ 5 23  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: H, Cell 23 --> Cell 23\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: F, Cell 13 --> Cell 8\n",
      "State: [1 8 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: H, Cell 8 --> Cell 8\n",
      "State: [2 8 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: H, Cell 8 --> Cell 8\n",
      "State: [3 8 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: FL, Cell 8 --> Cell 2\n",
      "State: [4 2 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: H, Cell 2 --> Cell 2\n",
      "State: [5 2 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: H, Cell 2 --> Cell 2\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: H, Cell 13 --> Cell 13\n",
      "State: [ 1 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BL, Cell 13 --> Cell 17\n",
      "State: [ 2 17  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: L, Cell 17 --> Cell 16\n",
      "State: [ 3 16  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FL, Cell 16 --> Cell 16\n",
      "State: [ 4 16  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: F, Cell 16 --> Cell 11\n",
      "State: [ 5 11  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: F, Cell 16 --> Cell 11\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: FL, Cell 13 --> Cell 7\n",
      "State: [1 7 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: L, Cell 7 --> Cell 6\n",
      "State: [2 6 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: R, Cell 6 --> Cell 7\n",
      "State: [3 7 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: B, Cell 7 --> Cell 12\n",
      "State: [ 4 12  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: R, Cell 12 --> Cell 13\n",
      "State: [ 5 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: R, Cell 12 --> Cell 13\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: H, Cell 13 --> Cell 13\n",
      "State: [ 1 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BR, Cell 13 --> Cell 19\n",
      "State: [ 2 19  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BR, Cell 19 --> Cell 25\n",
      "State: [ 3 25  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FL, Cell 25 --> Cell 19\n",
      "State: [ 4 19  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: B, Cell 19 --> Cell 24\n",
      "State: [ 5 24  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: B, Cell 19 --> Cell 24\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: L, Cell 13 --> Cell 12\n",
      "State: [ 1 12  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BL, Cell 12 --> Cell 16\n",
      "State: [ 2 16  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: R, Cell 16 --> Cell 17\n",
      "State: [ 3 17  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: L, Cell 17 --> Cell 16\n",
      "State: [ 4 16  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: F, Cell 16 --> Cell 11\n",
      "State: [ 5 11  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: F, Cell 16 --> Cell 11\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: B, Cell 13 --> Cell 18\n",
      "State: [ 1 18  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: F, Cell 18 --> Cell 13\n",
      "State: [ 2 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: L, Cell 13 --> Cell 12\n",
      "State: [ 3 12  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: B, Cell 12 --> Cell 17\n",
      "State: [ 4 17  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FR, Cell 17 --> Cell 13\n",
      "State: [ 5 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: FR, Cell 17 --> Cell 13\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: BR, Cell 13 --> Cell 19\n",
      "State: [ 1 19  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: R, Cell 19 --> Cell 20\n",
      "State: [ 2 20  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BR, Cell 20 --> Cell 20\n",
      "State: [ 3 20  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: F, Cell 20 --> Cell 15\n",
      "State: [ 4 15  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: R, Cell 15 --> Cell 15\n",
      "State: [ 5 15  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: R, Cell 15 --> Cell 15\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: F, Cell 13 --> Cell 8\n",
      "State: [1 8 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: FR, Cell 8 --> Cell 4\n",
      "State: [2 4 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: L, Cell 4 --> Cell 3\n",
      "State: [3 3 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: F, Cell 3 --> Cell 3\n",
      "State: [4 3 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: H, Cell 3 --> Cell 3\n",
      "State: [5 3 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: H, Cell 3 --> Cell 3\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: B, Cell 13 --> Cell 18\n",
      "State: [ 1 18  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BL, Cell 18 --> Cell 22\n",
      "State: [ 2 22  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BR, Cell 22 --> Cell 22\n",
      "State: [ 3 22  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: F, Cell 22 --> Cell 17\n",
      "State: [ 4 17  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BR, Cell 17 --> Cell 23\n",
      "State: [ 5 23  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: BR, Cell 17 --> Cell 23\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: FR, Cell 13 --> Cell 9\n",
      "State: [1 9 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: F, Cell 9 --> Cell 4\n",
      "State: [2 4 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: BR, Cell 4 --> Cell 10\n",
      "State: [ 3 10  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: B, Cell 10 --> Cell 15\n",
      "State: [ 4 15  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: BR, Cell 15 --> Cell 15\n",
      "State: [ 5 15  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: BR, Cell 15 --> Cell 15\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: BL, Cell 13 --> Cell 17\n",
      "State: [ 1 17  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: F, Cell 17 --> Cell 12\n",
      "State: [ 2 12  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FL, Cell 12 --> Cell 6\n",
      "State: [3 6 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: B, Cell 6 --> Cell 11\n",
      "State: [ 4 11  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FL, Cell 11 --> Cell 11\n",
      "State: [ 5 11  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: FL, Cell 11 --> Cell 11\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: H, Cell 13 --> Cell 13\n",
      "State: [ 1 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: F, Cell 13 --> Cell 8\n",
      "State: [2 8 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: F, Cell 8 --> Cell 3\n",
      "State: [3 3 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: B, Cell 3 --> Cell 8\n",
      "State: [4 8 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: FR, Cell 8 --> Cell 4\n",
      "State: [5 4 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: FR, Cell 8 --> Cell 4\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: L, Cell 13 --> Cell 12\n",
      "State: [ 1 12  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: R, Cell 12 --> Cell 13\n",
      "State: [ 2 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: B, Cell 13 --> Cell 18\n",
      "State: [ 3 18  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: F, Cell 18 --> Cell 13\n",
      "State: [ 4 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: F, Cell 13 --> Cell 8\n",
      "State: [5 8 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: F, Cell 13 --> Cell 8\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: BR, Cell 13 --> Cell 19\n",
      "State: [ 1 19  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: F, Cell 19 --> Cell 14\n",
      "State: [ 2 14  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: H, Cell 14 --> Cell 14\n",
      "State: [ 3 14  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: R, Cell 14 --> Cell 15\n",
      "State: [ 4 15  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: R, Cell 15 --> Cell 15\n",
      "State: [ 5 15  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: R, Cell 15 --> Cell 15\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: B, Cell 13 --> Cell 18\n",
      "State: [ 1 18  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: F, Cell 18 --> Cell 13\n",
      "State: [ 2 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: F, Cell 13 --> Cell 8\n",
      "State: [3 8 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: R, Cell 8 --> Cell 9\n",
      "State: [4 9 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "Action: L, Cell 9 --> Cell 8\n",
      "State: [5 8 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: L, Cell 9 --> Cell 8\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: H, Cell 13 --> Cell 13\n",
      "State: [ 1 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: R, Cell 13 --> Cell 14\n",
      "State: [ 2 14  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: L, Cell 14 --> Cell 13\n",
      "State: [ 3 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: L, Cell 13 --> Cell 12\n",
      "State: [ 4 12  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: F, Cell 12 --> Cell 7\n",
      "State: [5 7 0 0 0 0 0 0 0 0 0 0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: F, Cell 12 --> Cell 7\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n",
      "Action: BL, Cell 13 --> Cell 17\n",
      "State: [ 1 17  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: FL, Cell 17 --> Cell 11\n",
      "State: [ 2 11  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: B, Cell 11 --> Cell 16\n",
      "State: [ 3 16  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: L, Cell 16 --> Cell 16\n",
      "State: [ 4 16  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "Action: F, Cell 16 --> Cell 11\n",
      "State: [ 5 11  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: -1\n",
      "The episode has ended. Resetting environment...\n",
      "Action: F, Cell 16 --> Cell 11\n",
      "State: [ 0 13  0  0  0  0  0  0  0  0  0  0]\n",
      "Reward: 0\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    del env\n",
    "except Exception as e:\n",
    "    pass\n",
    "\n",
    "env = AnafiEnv(num_targets=10, max_timestep=5, is_training=True)\n",
    "observation = env.reset()\n",
    "# actions = [7, 4, 7, 3, 1, 2, 8, 4, 1, 5, 6]\n",
    "for i in range(1000):\n",
    "    action = env.action_space.sample()\n",
    "#     action = actions[i % len(actions)]\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    disp_info(action, observation, reward, done, info)\n",
    "\n",
    "    if done:\n",
    "        print(\"The episode has ended. Resetting environment...\")\n",
    "        observation = env.reset()\n",
    "        disp_info(action, observation, 0, done, info)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3a568a",
   "metadata": {},
   "source": [
    "# Train the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19743771",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveOnBestTrainingRewardCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback for saving a model (the check is done every ``check_freq`` steps)\n",
    "    based on the training reward (in practice, we recommend using ``EvalCallback``).\n",
    "\n",
    "    :param check_freq:\n",
    "    :param log_dir: Path to the folder where the model will be saved.\n",
    "      It must contains the file created by the ``Monitor`` wrapper.\n",
    "    :param verbose: Verbosity level.\n",
    "    \"\"\"\n",
    "    def __init__(self, check_freq: int, log_dir: str, verbose: int = 1):\n",
    "        super(SaveOnBestTrainingRewardCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.log_dir = log_dir\n",
    "        self.save_path = os.path.join(log_dir, 'best_model')\n",
    "        self.best_mean_reward = -np.inf\n",
    "\n",
    "    def _init_callback(self) -> None:\n",
    "        pass\n",
    "#         # Create folder if needed\n",
    "#         if self.save_path is not None:\n",
    "#             os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "\n",
    "            # Retrieve training reward\n",
    "            x, y = ts2xy(load_results(self.log_dir), 'timesteps')\n",
    "            if len(x) > 0:\n",
    "                # Mean training reward over the last 100 episodes\n",
    "                mean_reward = np.mean(y[-100:])\n",
    "                if self.verbose > 0:\n",
    "                    print(f\"Num timesteps: {self.num_timesteps}\")\n",
    "                    print(f\"Best mean reward: {self.best_mean_reward:.2f} - Last mean reward per episode: {mean_reward:.2f}\")\n",
    "\n",
    "                # New best model, you could save the agent here\n",
    "                if mean_reward > self.best_mean_reward:\n",
    "                    self.best_mean_reward = mean_reward\n",
    "                    # Example for saving best model\n",
    "                    if self.verbose > 0:\n",
    "                        print(f\"Saving new best model to {self.save_path}\")\n",
    "                    self.model.save(self.save_path)\n",
    "        \n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08ce3ab5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to logs/PPO_2_0\n",
      "Num timesteps: 15872\n",
      "Best mean reward: -inf - Last mean reward per episode: 5.42\n",
      "Saving new best model to logs/best_model\n",
      "Num timesteps: 16384\n",
      "Best mean reward: 5.42 - Last mean reward per episode: 5.22\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 11.4     |\n",
      "|    ep_rew_mean     | 5.22     |\n",
      "| time/              |          |\n",
      "|    fps             | 0        |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 2073     |\n",
      "|    total_timesteps | 16384    |\n",
      "---------------------------------\n",
      "Num timesteps: 16896\n",
      "Best mean reward: 5.42 - Last mean reward per episode: 5.92\n",
      "Saving new best model to logs/best_model\n",
      "Num timesteps: 17408\n",
      "Best mean reward: 5.92 - Last mean reward per episode: 6.30\n",
      "Saving new best model to logs/best_model\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 10.5        |\n",
      "|    ep_rew_mean          | 6.3         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 0           |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 4457        |\n",
      "|    total_timesteps      | 17408       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011718066 |\n",
      "|    clip_fraction        | 0.115       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.73       |\n",
      "|    explained_variance   | 0.257       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.6         |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0158     |\n",
      "|    value_loss           | 15.7        |\n",
      "-----------------------------------------\n",
      "Num timesteps: 17920\n",
      "Best mean reward: 6.30 - Last mean reward per episode: 5.82\n",
      "Num timesteps: 18432\n",
      "Best mean reward: 6.30 - Last mean reward per episode: 5.49\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 11.1        |\n",
      "|    ep_rew_mean          | 5.49        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 0           |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 6791        |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009512995 |\n",
      "|    clip_fraction        | 0.0856      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.67       |\n",
      "|    explained_variance   | 0.265       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.8         |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.012      |\n",
      "|    value_loss           | 17.2        |\n",
      "-----------------------------------------\n",
      "Num timesteps: 18944\n",
      "Best mean reward: 6.30 - Last mean reward per episode: 5.87\n",
      "Num timesteps: 19456\n",
      "Best mean reward: 6.30 - Last mean reward per episode: 5.64\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 11          |\n",
      "|    ep_rew_mean          | 5.64        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 0           |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 9165        |\n",
      "|    total_timesteps      | 19456       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008693878 |\n",
      "|    clip_fraction        | 0.0882      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.63       |\n",
      "|    explained_variance   | 0.288       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.01        |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0111     |\n",
      "|    value_loss           | 16.9        |\n",
      "-----------------------------------------\n",
      "Num timesteps: 19968\n",
      "Best mean reward: 6.30 - Last mean reward per episode: 5.37\n",
      "Num timesteps: 20480\n",
      "Best mean reward: 6.30 - Last mean reward per episode: 5.96\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 10.8        |\n",
      "|    ep_rew_mean          | 5.96        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 0           |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 11555       |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011997615 |\n",
      "|    clip_fraction        | 0.15        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.61       |\n",
      "|    explained_variance   | 0.266       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 11.2        |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.0199     |\n",
      "|    value_loss           | 16.8        |\n",
      "-----------------------------------------\n",
      "Num timesteps: 20992\n",
      "Best mean reward: 6.30 - Last mean reward per episode: 6.64\n",
      "Saving new best model to logs/best_model\n",
      "Num timesteps: 21504\n",
      "Best mean reward: 6.64 - Last mean reward per episode: 6.96\n",
      "Saving new best model to logs/best_model\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 10.2         |\n",
      "|    ep_rew_mean          | 6.96         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 0            |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 13786        |\n",
      "|    total_timesteps      | 21504        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073937727 |\n",
      "|    clip_fraction        | 0.0801       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.56        |\n",
      "|    explained_variance   | 0.27         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 11.2         |\n",
      "|    n_updates            | 200          |\n",
      "|    policy_gradient_loss | -0.0114      |\n",
      "|    value_loss           | 18.5         |\n",
      "------------------------------------------\n",
      "Num timesteps: 22016\n",
      "Best mean reward: 6.96 - Last mean reward per episode: 7.37\n",
      "Saving new best model to logs/best_model\n",
      "Num timesteps: 22528\n",
      "Best mean reward: 7.37 - Last mean reward per episode: 7.04\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 10.3        |\n",
      "|    ep_rew_mean          | 7.04        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 0           |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 15942       |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011353636 |\n",
      "|    clip_fraction        | 0.118       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.53       |\n",
      "|    explained_variance   | 0.3         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.72        |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.0112     |\n",
      "|    value_loss           | 16.6        |\n",
      "-----------------------------------------\n",
      "Num timesteps: 23040\n",
      "Best mean reward: 7.37 - Last mean reward per episode: 6.03\n",
      "Num timesteps: 23552\n",
      "Best mean reward: 7.37 - Last mean reward per episode: 6.08\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 10.8        |\n",
      "|    ep_rew_mean          | 6.08        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 0           |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 18242       |\n",
      "|    total_timesteps      | 23552       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009666811 |\n",
      "|    clip_fraction        | 0.125       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.5        |\n",
      "|    explained_variance   | 0.359       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.28        |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0136     |\n",
      "|    value_loss           | 15          |\n",
      "-----------------------------------------\n",
      "Num timesteps: 24064\n",
      "Best mean reward: 7.37 - Last mean reward per episode: 6.22\n",
      "Num timesteps: 24576\n",
      "Best mean reward: 7.37 - Last mean reward per episode: 8.03\n",
      "Saving new best model to logs/best_model\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9.7         |\n",
      "|    ep_rew_mean          | 8.03        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 0           |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 20656       |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009064315 |\n",
      "|    clip_fraction        | 0.0865      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.47       |\n",
      "|    explained_variance   | 0.324       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.03        |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.0103     |\n",
      "|    value_loss           | 16.9        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 25088\n",
      "Best mean reward: 8.03 - Last mean reward per episode: 8.27\n",
      "Saving new best model to logs/best_model\n",
      "Num timesteps: 25600\n",
      "Best mean reward: 8.27 - Last mean reward per episode: 8.78\n",
      "Saving new best model to logs/best_model\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 8.92         |\n",
      "|    ep_rew_mean          | 8.78         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 0            |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 23149        |\n",
      "|    total_timesteps      | 25600        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0088339625 |\n",
      "|    clip_fraction        | 0.0869       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.37         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.67         |\n",
      "|    n_updates            | 240          |\n",
      "|    policy_gradient_loss | -0.00972     |\n",
      "|    value_loss           | 16           |\n",
      "------------------------------------------\n",
      "Num timesteps: 26112\n",
      "Best mean reward: 8.78 - Last mean reward per episode: 8.32\n",
      "Num timesteps: 26624\n",
      "Best mean reward: 8.78 - Last mean reward per episode: 8.46\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9.04        |\n",
      "|    ep_rew_mean          | 8.46        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 0           |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 25573       |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009909709 |\n",
      "|    clip_fraction        | 0.111       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.36       |\n",
      "|    explained_variance   | 0.358       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.29        |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.018      |\n",
      "|    value_loss           | 16.9        |\n",
      "-----------------------------------------\n",
      "Num timesteps: 27136\n",
      "Best mean reward: 8.78 - Last mean reward per episode: 8.18\n",
      "Num timesteps: 27648\n",
      "Best mean reward: 8.78 - Last mean reward per episode: 7.78\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 9.59         |\n",
      "|    ep_rew_mean          | 7.78         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 0            |\n",
      "|    iterations           | 12           |\n",
      "|    time_elapsed         | 28104        |\n",
      "|    total_timesteps      | 27648        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069734585 |\n",
      "|    clip_fraction        | 0.071        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.32        |\n",
      "|    explained_variance   | 0.353        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.17         |\n",
      "|    n_updates            | 260          |\n",
      "|    policy_gradient_loss | -0.0105      |\n",
      "|    value_loss           | 16.5         |\n",
      "------------------------------------------\n",
      "Num timesteps: 28160\n",
      "Best mean reward: 8.78 - Last mean reward per episode: 8.80\n",
      "Saving new best model to logs/best_model\n",
      "Num timesteps: 28672\n",
      "Best mean reward: 8.80 - Last mean reward per episode: 9.70\n",
      "Saving new best model to logs/best_model\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 8.28        |\n",
      "|    ep_rew_mean          | 9.7         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 0           |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 30643       |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008765608 |\n",
      "|    clip_fraction        | 0.0938      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.3        |\n",
      "|    explained_variance   | 0.367       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.2         |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.0106     |\n",
      "|    value_loss           | 18.3        |\n",
      "-----------------------------------------\n",
      "Num timesteps: 29184\n",
      "Best mean reward: 9.70 - Last mean reward per episode: 9.70\n",
      "Saving new best model to logs/best_model\n",
      "Num timesteps: 29696\n",
      "Best mean reward: 9.70 - Last mean reward per episode: 9.90\n",
      "Saving new best model to logs/best_model\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 8.32       |\n",
      "|    ep_rew_mean          | 9.9        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 0          |\n",
      "|    iterations           | 14         |\n",
      "|    time_elapsed         | 33250      |\n",
      "|    total_timesteps      | 29696      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00938387 |\n",
      "|    clip_fraction        | 0.0999     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.21      |\n",
      "|    explained_variance   | 0.421      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 8.01       |\n",
      "|    n_updates            | 280        |\n",
      "|    policy_gradient_loss | -0.00909   |\n",
      "|    value_loss           | 15.1       |\n",
      "----------------------------------------\n",
      "Num timesteps: 30208\n",
      "Best mean reward: 9.90 - Last mean reward per episode: 10.37\n",
      "Saving new best model to logs/best_model\n",
      "Num timesteps: 30720\n",
      "Best mean reward: 10.37 - Last mean reward per episode: 10.77\n",
      "Saving new best model to logs/best_model\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 7.47        |\n",
      "|    ep_rew_mean          | 10.8        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 0           |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 36018       |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011649007 |\n",
      "|    clip_fraction        | 0.138       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.12       |\n",
      "|    explained_variance   | 0.545       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.01        |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.0184     |\n",
      "|    value_loss           | 12.6        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-08 06:34:35,631 \u001b[31m[ERROR] \u001b[0m\tulog - pomp - epoll_ctl(fd=89) err=9(Bad file descriptor)\u001b[0m\n",
      "2022-03-08 06:34:35,637 \u001b[31m[ERROR] \u001b[0m\tulog - pomp - epoll_ctl op=2 cb=0x7f8775037c18 userdata=0x7f85c5422b90\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    del env\n",
    "    del model\n",
    "except Exception as e:\n",
    "    pass\n",
    "\n",
    "# Create log dir\n",
    "run = 2\n",
    "log_dir = \"logs/\"\n",
    "monitor_file = os.path.join(log_dir, str(run))\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "env = AnafiEnv(num_targets=10, max_timestep=15, is_training=True)\n",
    "env = Monitor(env, monitor_file)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "\n",
    "callback = SaveOnBestTrainingRewardCallback(check_freq=512, log_dir=log_dir)\n",
    "\n",
    "# model = PPO(\"MlpPolicy\", env, n_steps=1024, verbose=1, tensorboard_log=log_dir)\n",
    "model = PPO.load(os.path.join(log_dir, str(run-1) + \"_run\"), env)\n",
    "model.learn(total_timesteps=15_000, callback=callback, tb_log_name=\"PPO_\" + str(run), reset_num_timesteps=False)\n",
    "model.save(os.path.join(log_dir, str(run) + \"_run\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b27d7fa",
   "metadata": {},
   "source": [
    "# Test the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fd568b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_target_pos():\n",
    "    Simulation.gen_targets_pos(10)\n",
    "    Simulation.reset_targets()\n",
    "    time.sleep(0.1)\n",
    "    Simulation.reset_targets(release=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "ea1be2f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "541 Server Error: Media Not Yet Indexed for url: http://10.202.0.1:80/api/v1/media/medias\n",
      "Media are not yet indexed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-09 15:16:31,648 \u001b[31m[ERROR] \u001b[0m\tulog - pomp - epoll_ctl(fd=121) err=9(Bad file descriptor)\u001b[0m\n",
      "2022-03-09 15:16:31,650 \u001b[31m[ERROR] \u001b[0m\tulog - pomp - epoll_ctl op=2 cb=0x7ff39afeebe0 userdata=0x7ff2a6a33f90\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    del env\n",
    "    del model\n",
    "except Exception as e:\n",
    "    pass\n",
    "\n",
    "env = AnafiEnv(num_targets=10, max_timestep=15)\n",
    "model = PPO.load(\"logs/4_run\", env, verbose=1)\n",
    "\n",
    "# header = [\"timestep\", \"action\"]\n",
    "\n",
    "# episode = 9\n",
    "# timestep = 0\n",
    "# file = open('run_' + str(episode+1) + '.csv', 'w', encoding='UTF8', newline='')\n",
    "# writer = csv.writer(file)\n",
    "# writer.writerow(header)\n",
    "\n",
    "obs = env.reset()\n",
    "reset_target_pos()\n",
    "\n",
    "for i in range(1000):\n",
    "    action, _state = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "#     timestep += 1\n",
    "    \n",
    "#     writer.writerow([str(timestep), info[\"action\"]])\n",
    "\n",
    "    if done:\n",
    "        obs = env.reset()\n",
    "        reset_target_pos()\n",
    "        \n",
    "#         if episode >= 10:\n",
    "#             file.close()\n",
    "#             break\n",
    "#         episode += 1\n",
    "#         timestep = 0\n",
    "\n",
    "#         file = open('run_' + str(episode+1) + '.csv', 'w', encoding='UTF8', newline='')\n",
    "#         writer = csv.writer(file)\n",
    "#         writer.writerow(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacef254",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
